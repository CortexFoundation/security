{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "window.load_remote_theme = true\n",
       "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
       "\n",
       "window.load_local_theme = function(){\n",
       "    var hostname = document.location.hostname\n",
       "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
       "}\n",
       "\n",
       "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
       "\n",
       "$.getScript(url)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "window.load_remote_theme = true\n",
    "var theme_js = \"https://odhk.github.io/hyrule_theme/custom.js\";\n",
    "\n",
    "window.load_local_theme = function(){\n",
    "    var hostname = document.location.hostname\n",
    "    return ((hostname == \"localhost\" || hostname == '127.0.0.1') && !load_remote_theme)\n",
    "}\n",
    "\n",
    "var url = load_local_theme() ? document.location.origin + \"/files/theme/custom.js\" : theme_js\n",
    "\n",
    "$.getScript(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.read_pickle('top_feature1000.pkl')\n",
    "\n",
    "sub_sample = pd.read_csv('3rd_security_submit_sample.csv')\n",
    "\n",
    "label = pd.read_pickle('label.pkl')\n",
    "label = label[0].values\n",
    "\n",
    "X['id'] = X.iloc[:, 0].values\n",
    "\n",
    "X.drop('file_id', axis = 1, inplace = True)\n",
    "\n",
    "train_FE = X.iloc[:len(label),:]\n",
    "test_FE = X.iloc[len(label):,:]\n",
    "\n",
    "train_FE['label'] = label\n",
    "\n",
    "test_FE.drop('id', axis = 1, inplace = True)\n",
    "\n",
    "x0 = train_FE[train_FE['label'] == 0]\n",
    "\n",
    "x1 = train_FE[train_FE['label'] != 0]\n",
    "\n",
    "x0.shape, x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = pd.read_csv('index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multiclass\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.05,\n",
    "          \"num_leaves\": 15,\n",
    "          # \"max_bin\": 128,\n",
    "          \"feature_fraction\": 0.85,\n",
    "#           \"min_child_samples\": 10,\n",
    "#           \"min_child_weight\": 150,\n",
    "#           \"min_split_gain\": 0,\n",
    "          \"subsample\": 0.85,\n",
    "          #'metric':'logloss',\n",
    "           'lambda_l1':0.02,\n",
    "           'lambda_l2':0.02,\n",
    "          'seed':666,\n",
    "          'num_class':6\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********第0次抽样***********\n",
      "负样本抽样数量：\n",
      "(53670, 1001)\n",
      "[20]\tcv_agg's multi_logloss: 0.537977 + 0.000852458\n",
      "[220]\tcv_agg's multi_logloss: 0.0582876 + 0.00358294\n",
      "最优结果0.05803473436927152\n",
      "总loss[0.02618883638668918, 0.03287141226624521, 0.051883784445321234, 0.02758720799855078, 0.05803473436927152]\n",
      "[10]\ttraining's multi_logloss: 0.941344\n",
      "[20]\ttraining's multi_logloss: 0.569703\n",
      "[30]\ttraining's multi_logloss: 0.364831\n",
      "[40]\ttraining's multi_logloss: 0.243211\n",
      "[50]\ttraining's multi_logloss: 0.168723\n",
      "[60]\ttraining's multi_logloss: 0.122148\n",
      "[70]\ttraining's multi_logloss: 0.0915753\n",
      "[80]\ttraining's multi_logloss: 0.0708085\n",
      "[90]\ttraining's multi_logloss: 0.0563478\n",
      "[100]\ttraining's multi_logloss: 0.0458895\n",
      "[110]\ttraining's multi_logloss: 0.0379351\n",
      "[120]\ttraining's multi_logloss: 0.0316158\n",
      "[130]\ttraining's multi_logloss: 0.0267317\n",
      "[140]\ttraining's multi_logloss: 0.022903\n",
      "[150]\ttraining's multi_logloss: 0.0198327\n",
      "[160]\ttraining's multi_logloss: 0.0173106\n",
      "[170]\ttraining's multi_logloss: 0.0152548\n",
      "[180]\ttraining's multi_logloss: 0.0134897\n",
      "[190]\ttraining's multi_logloss: 0.0120119\n",
      "[200]\ttraining's multi_logloss: 0.0107262\n",
      "[210]\ttraining's multi_logloss: 0.00963245\n",
      "[220]\ttraining's multi_logloss: 0.00867776\n",
      "***********第5次抽样***********\n",
      "负样本抽样数量：\n",
      "(17255, 1001)\n",
      "[20]\tcv_agg's multi_logloss: 0.577964 + 0.0034054\n",
      "[40]\tcv_agg's multi_logloss: 0.25647 + 0.00479275\n",
      "[60]\tcv_agg's multi_logloss: 0.140098 + 0.00547665\n",
      "[80]\tcv_agg's multi_logloss: 0.0953835 + 0.0060417\n",
      "[100]\tcv_agg's multi_logloss: 0.0761585 + 0.006536\n",
      "[120]\tcv_agg's multi_logloss: 0.0669179 + 0.00656676\n",
      "[140]\tcv_agg's multi_logloss: 0.0622518 + 0.00655403\n",
      "[160]\tcv_agg's multi_logloss: 0.0601107 + 0.00687239\n",
      "[180]\tcv_agg's multi_logloss: 0.0591974 + 0.00714538\n",
      "[200]\tcv_agg's multi_logloss: 0.0591113 + 0.0073393\n",
      "[220]\tcv_agg's multi_logloss: 0.0595026 + 0.00759759\n",
      "最优结果0.05900769788789789\n",
      "总loss[0.02618883638668918, 0.03287141226624521, 0.051883784445321234, 0.02758720799855078, 0.05803473436927152, 0.05900769788789789]\n",
      "[10]\ttraining's multi_logloss: 0.941348\n",
      "[20]\ttraining's multi_logloss: 0.56965\n",
      "[30]\ttraining's multi_logloss: 0.364721\n",
      "[40]\ttraining's multi_logloss: 0.243537\n",
      "[50]\ttraining's multi_logloss: 0.168769\n",
      "[60]\ttraining's multi_logloss: 0.122255\n",
      "[70]\ttraining's multi_logloss: 0.0917902\n",
      "[80]\ttraining's multi_logloss: 0.0712024\n",
      "[90]\ttraining's multi_logloss: 0.0567728\n",
      "[100]\ttraining's multi_logloss: 0.0461415\n",
      "[110]\ttraining's multi_logloss: 0.0380163\n",
      "[120]\ttraining's multi_logloss: 0.0317262\n",
      "[130]\ttraining's multi_logloss: 0.0267483\n",
      "[140]\ttraining's multi_logloss: 0.0228305\n",
      "[150]\ttraining's multi_logloss: 0.0196661\n",
      "[160]\ttraining's multi_logloss: 0.0171203\n",
      "[170]\ttraining's multi_logloss: 0.01499\n",
      "[180]\ttraining's multi_logloss: 0.0131845\n",
      "[190]\ttraining's multi_logloss: 0.011668\n",
      "[200]\ttraining's multi_logloss: 0.0103758\n",
      "[210]\ttraining's multi_logloss: 0.00925876\n",
      "***********第6次抽样***********\n",
      "负样本抽样数量：\n",
      "(31370, 1001)\n",
      "[20]\tcv_agg's multi_logloss: 0.554882 + 0.00135378\n",
      "[40]\tcv_agg's multi_logloss: 0.232133 + 0.00177366\n",
      "[60]\tcv_agg's multi_logloss: 0.116157 + 0.00153865\n",
      "[80]\tcv_agg's multi_logloss: 0.072223 + 0.00182013\n",
      "[100]\tcv_agg's multi_logloss: 0.0542739 + 0.00205935\n",
      "[120]\tcv_agg's multi_logloss: 0.046797 + 0.00218916\n",
      "[140]\tcv_agg's multi_logloss: 0.0428023 + 0.00233048\n",
      "[160]\tcv_agg's multi_logloss: 0.040802 + 0.00236413\n",
      "[180]\tcv_agg's multi_logloss: 0.0398618 + 0.00235198\n",
      "[200]\tcv_agg's multi_logloss: 0.0396254 + 0.00240498\n",
      "[220]\tcv_agg's multi_logloss: 0.039693 + 0.00251338\n",
      "最优结果0.03959616482732104\n",
      "总loss[0.02618883638668918, 0.03287141226624521, 0.051883784445321234, 0.02758720799855078, 0.05803473436927152, 0.05900769788789789, 0.03959616482732104]\n",
      "[10]\ttraining's multi_logloss: 0.924692\n",
      "[20]\ttraining's multi_logloss: 0.550197\n",
      "[30]\ttraining's multi_logloss: 0.344587\n",
      "[40]\ttraining's multi_logloss: 0.223833\n",
      "[50]\ttraining's multi_logloss: 0.150334\n",
      "[60]\ttraining's multi_logloss: 0.10527\n",
      "[70]\ttraining's multi_logloss: 0.0767806\n",
      "[80]\ttraining's multi_logloss: 0.0577639\n",
      "[90]\ttraining's multi_logloss: 0.0447727\n",
      "[100]\ttraining's multi_logloss: 0.0358223\n",
      "[110]\ttraining's multi_logloss: 0.029331\n",
      "[120]\ttraining's multi_logloss: 0.0244203\n",
      "[130]\ttraining's multi_logloss: 0.020495\n",
      "[140]\ttraining's multi_logloss: 0.0174384\n",
      "[150]\ttraining's multi_logloss: 0.0150196\n",
      "[160]\ttraining's multi_logloss: 0.0130577\n",
      "[170]\ttraining's multi_logloss: 0.0114755\n",
      "[180]\ttraining's multi_logloss: 0.010132\n",
      "[190]\ttraining's multi_logloss: 0.00901356\n",
      "[200]\ttraining's multi_logloss: 0.00805198\n",
      "[210]\ttraining's multi_logloss: 0.00721497\n",
      "[220]\ttraining's multi_logloss: 0.0064908\n",
      "***********第7次抽样***********\n",
      "负样本抽样数量：\n",
      "(38966, 1001)\n",
      "[20]\tcv_agg's multi_logloss: 0.547642 + 0.00190724\n",
      "[40]\tcv_agg's multi_logloss: 0.224172 + 0.00309088\n",
      "[60]\tcv_agg's multi_logloss: 0.108312 + 0.00334294\n",
      "[80]\tcv_agg's multi_logloss: 0.0649275 + 0.00350907\n",
      "[100]\tcv_agg's multi_logloss: 0.0476905 + 0.00367113\n",
      "[120]\tcv_agg's multi_logloss: 0.0403209 + 0.00376404\n",
      "[220]\tcv_agg's multi_logloss: 0.0334074 + 0.00397332\n",
      "[240]\tcv_agg's multi_logloss: 0.0335964 + 0.00415432\n",
      "最优结果0.0333815154926436\n",
      "总loss[0.02618883638668918, 0.03287141226624521, 0.051883784445321234, 0.02758720799855078, 0.05803473436927152, 0.05900769788789789, 0.03959616482732104, 0.0333815154926436]\n",
      "[10]\ttraining's multi_logloss: 0.920049\n",
      "[20]\ttraining's multi_logloss: 0.543683\n",
      "[30]\ttraining's multi_logloss: 0.3379\n",
      "[40]\ttraining's multi_logloss: 0.217558\n",
      "[50]\ttraining's multi_logloss: 0.144394\n",
      "[60]\ttraining's multi_logloss: 0.0994145\n",
      "[70]\ttraining's multi_logloss: 0.0714061\n",
      "[80]\ttraining's multi_logloss: 0.0530487\n",
      "[90]\ttraining's multi_logloss: 0.0406646\n",
      "[100]\ttraining's multi_logloss: 0.0320898\n",
      "[110]\ttraining's multi_logloss: 0.0260422\n",
      "[120]\ttraining's multi_logloss: 0.0215551\n",
      "[130]\ttraining's multi_logloss: 0.0180701\n",
      "[140]\ttraining's multi_logloss: 0.0153385\n",
      "[150]\ttraining's multi_logloss: 0.0131572\n",
      "[160]\ttraining's multi_logloss: 0.0114217\n",
      "[170]\ttraining's multi_logloss: 0.0100238\n",
      "[180]\ttraining's multi_logloss: 0.0088597\n",
      "[190]\ttraining's multi_logloss: 0.0078788\n",
      "[200]\ttraining's multi_logloss: 0.00704547\n",
      "[210]\ttraining's multi_logloss: 0.00633883\n",
      "[220]\ttraining's multi_logloss: 0.00572095\n",
      "[230]\ttraining's multi_logloss: 0.00518113\n",
      "***********第8次抽样***********\n",
      "负样本抽样数量：\n",
      "(23750, 1001)\n",
      "[20]\tcv_agg's multi_logloss: 0.565491 + 0.00229728\n",
      "[40]\tcv_agg's multi_logloss: 0.243986 + 0.00240395\n",
      "[60]\tcv_agg's multi_logloss: 0.127433 + 0.00229581\n",
      "[80]\tcv_agg's multi_logloss: 0.0823854 + 0.0023291\n",
      "[100]\tcv_agg's multi_logloss: 0.0639424 + 0.00289966\n",
      "[120]\tcv_agg's multi_logloss: 0.0555487 + 0.0033952\n",
      "[140]\tcv_agg's multi_logloss: 0.0514311 + 0.00386901\n",
      "[160]\tcv_agg's multi_logloss: 0.0494116 + 0.0042665\n",
      "[180]\tcv_agg's multi_logloss: 0.0485505 + 0.00470559\n",
      "[200]\tcv_agg's multi_logloss: 0.0483814 + 0.00500078\n",
      "[220]\tcv_agg's multi_logloss: 0.0485858 + 0.0052836\n",
      "最优结果0.048381377932017654\n",
      "总loss[0.02618883638668918, 0.03287141226624521, 0.051883784445321234, 0.02758720799855078, 0.05803473436927152, 0.05900769788789789, 0.03959616482732104, 0.0333815154926436, 0.048381377932017654]\n",
      "[10]\ttraining's multi_logloss: 0.932398\n",
      "[20]\ttraining's multi_logloss: 0.559061\n",
      "[30]\ttraining's multi_logloss: 0.354507\n",
      "[40]\ttraining's multi_logloss: 0.233598\n",
      "[50]\ttraining's multi_logloss: 0.159348\n",
      "[60]\ttraining's multi_logloss: 0.11323\n",
      "[70]\ttraining's multi_logloss: 0.0837261\n",
      "[80]\ttraining's multi_logloss: 0.0637495\n",
      "[90]\ttraining's multi_logloss: 0.050103\n",
      "[100]\ttraining's multi_logloss: 0.0404167\n",
      "[110]\ttraining's multi_logloss: 0.0332403\n",
      "[120]\ttraining's multi_logloss: 0.0277376\n",
      "[130]\ttraining's multi_logloss: 0.0233571\n",
      "[140]\ttraining's multi_logloss: 0.0199421\n",
      "[150]\ttraining's multi_logloss: 0.0172094\n",
      "[160]\ttraining's multi_logloss: 0.0149804\n",
      "[170]\ttraining's multi_logloss: 0.0131607\n",
      "[180]\ttraining's multi_logloss: 0.0116163\n",
      "[190]\ttraining's multi_logloss: 0.0103197\n",
      "[200]\ttraining's multi_logloss: 0.00920316\n",
      "[210]\ttraining's multi_logloss: 0.0082539\n",
      "[220]\ttraining's multi_logloss: 0.00742822\n",
      "***********第9次抽样***********\n",
      "负样本抽样数量：\n",
      "(24766, 1001)\n",
      "[20]\tcv_agg's multi_logloss: 0.563722 + 0.00316503\n",
      "[40]\tcv_agg's multi_logloss: 0.241634 + 0.00432327\n",
      "[60]\tcv_agg's multi_logloss: 0.125228 + 0.00489288\n",
      "[80]\tcv_agg's multi_logloss: 0.0805956 + 0.00522729\n",
      "[100]\tcv_agg's multi_logloss: 0.0622026 + 0.00517271\n",
      "[120]\tcv_agg's multi_logloss: 0.053793 + 0.00519933\n",
      "[140]\tcv_agg's multi_logloss: 0.0494308 + 0.0054256\n",
      "[160]\tcv_agg's multi_logloss: 0.0471452 + 0.00567556\n",
      "[180]\tcv_agg's multi_logloss: 0.046086 + 0.00601195\n",
      "[200]\tcv_agg's multi_logloss: 0.0456609 + 0.00645205\n",
      "[220]\tcv_agg's multi_logloss: 0.0456914 + 0.00675339\n",
      "最优结果0.04562884115416434\n",
      "总loss[0.02618883638668918, 0.03287141226624521, 0.051883784445321234, 0.02758720799855078, 0.05803473436927152, 0.05900769788789789, 0.03959616482732104, 0.0333815154926436, 0.048381377932017654, 0.04562884115416434]\n",
      "[10]\ttraining's multi_logloss: 0.931062\n",
      "[20]\ttraining's multi_logloss: 0.557608\n",
      "[30]\ttraining's multi_logloss: 0.352568\n",
      "[40]\ttraining's multi_logloss: 0.231582\n",
      "[50]\ttraining's multi_logloss: 0.157607\n",
      "[60]\ttraining's multi_logloss: 0.112016\n",
      "[70]\ttraining's multi_logloss: 0.0825229\n",
      "[80]\ttraining's multi_logloss: 0.0627804\n",
      "[90]\ttraining's multi_logloss: 0.0493039\n",
      "[100]\ttraining's multi_logloss: 0.0398291\n",
      "[110]\ttraining's multi_logloss: 0.0327263\n",
      "[120]\ttraining's multi_logloss: 0.0272624\n",
      "[130]\ttraining's multi_logloss: 0.0228709\n",
      "[140]\ttraining's multi_logloss: 0.0194805\n",
      "[150]\ttraining's multi_logloss: 0.0167733\n",
      "[160]\ttraining's multi_logloss: 0.0145745\n",
      "[170]\ttraining's multi_logloss: 0.0127684\n",
      "[180]\ttraining's multi_logloss: 0.0112559\n",
      "[190]\ttraining's multi_logloss: 0.00996398\n",
      "[200]\ttraining's multi_logloss: 0.0088685\n",
      "[210]\ttraining's multi_logloss: 0.00794235\n",
      "[220]\ttraining's multi_logloss: 0.00712588\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "pred = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    print('***********第%s次抽样***********'%i)\n",
    "    x0_part = x0.copy()\n",
    "    x0_part.index = range(x0_part.shape[0])\n",
    "    x1.index = range(x1.shape[0])\n",
    "    permutation = np.random.permutation(x0_part.shape[0])\n",
    "    \n",
    "    ratio = int(x0.shape[0]*random.uniform(1.5,5)/10)\n",
    "\n",
    "    select = x0_part.iloc[permutation[:ratio], :]\n",
    "    \n",
    "    print('负样本抽样数量：')\n",
    "    print(select.shape)\n",
    "    \n",
    "    data_new = pd.concat([x1, select], axis = 0)\n",
    "    \n",
    "    permutation = np.random.permutation(data_new.shape[0])\n",
    "\n",
    "    data_new = data_new.iloc[permutation, :]\n",
    "    \n",
    "    label_new = data_new['label'].values\n",
    "    \n",
    "    data_new = data_new.drop(['label', 'id'], axis = 1)\n",
    "    \n",
    "    dtrain = lgb.Dataset(data_new, label=label_new)\n",
    "\n",
    "    hist = lgb.cv(params, dtrain, num_boost_round = 1000, verbose_eval=20, early_stopping_rounds=30, stratified=False)\n",
    "    \n",
    "    print('最优结果%s'%hist['multi_logloss-mean'][-1])\n",
    "    loss_list.append(hist['multi_logloss-mean'][-1])\n",
    "    print('总loss%s'%loss_list)\n",
    "    model = lgb.train(params=params, train_set=dtrain, num_boost_round=int(1.1*len(hist['multi_logloss-mean'])), verbose_eval=10,\\\n",
    "                      valid_sets=dtrain)\n",
    "    \n",
    "    pred_temp = pd.DataFrame(model.predict(test_FE))\n",
    "    \n",
    "    pred = pd.concat([pred, pred_temp], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999581</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.999388</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.998179</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.001325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998742</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.998390</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.997043</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.001174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999794</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.999755</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.999548</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999773</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.999636</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.999588</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.999832</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999383</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.999491</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.999197</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.999578</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.999354</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.999919</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.999906</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.998270</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.997231</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.004492</td>\n",
       "      <td>0.994173</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.004860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.996067</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.995041</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.002290</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.992204</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.001558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999769</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.999573</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999921</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.999908</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.999837</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.999930</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.999904</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.999801</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.999742</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.999460</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.999590</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.999621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.999585</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.999560</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.999125</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.999729</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.999606</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.999752</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.999664</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.999923</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.999904</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.999466</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.999259</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.999144</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.999834</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.999810</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.999630</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.999828</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.999771</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.999721</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.014228</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.025771</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.956102</td>\n",
       "      <td>0.015472</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.983703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.999858</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.999794</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.999631</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.999517</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.998898</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.999155</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.999794</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.999777</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.999608</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.998247</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.998326</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.997527</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.001725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.999656</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.999484</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.999423</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.999760</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53063</th>\n",
       "      <td>0.999830</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.999785</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.999543</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53064</th>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.999638</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.999476</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53065</th>\n",
       "      <td>0.996506</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.003293</td>\n",
       "      <td>0.997090</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.006086</td>\n",
       "      <td>0.995939</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.003810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53066</th>\n",
       "      <td>0.999560</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.999598</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53067</th>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.998793</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.998967</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.998892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53068</th>\n",
       "      <td>0.999859</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.999850</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53069</th>\n",
       "      <td>0.999705</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.999584</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.999393</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53070</th>\n",
       "      <td>0.999946</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.999902</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53071</th>\n",
       "      <td>0.999839</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.999801</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.999706</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53072</th>\n",
       "      <td>0.999927</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.999906</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.999839</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53073</th>\n",
       "      <td>0.999754</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.999806</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.999569</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53074</th>\n",
       "      <td>0.999830</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.999826</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53075</th>\n",
       "      <td>0.999790</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.999637</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.999495</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53076</th>\n",
       "      <td>0.999789</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.999789</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.999745</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53077</th>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.999906</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53078</th>\n",
       "      <td>0.999926</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53079</th>\n",
       "      <td>0.999912</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.999906</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.999826</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53080</th>\n",
       "      <td>0.999515</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.999216</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.999115</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53081</th>\n",
       "      <td>0.998556</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.998764</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>0.998458</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53082</th>\n",
       "      <td>0.999830</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.999569</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.999432</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53083</th>\n",
       "      <td>0.999734</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.999719</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.999636</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53084</th>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.999942</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.999895</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53085</th>\n",
       "      <td>0.999856</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.999821</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.999697</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53086</th>\n",
       "      <td>0.999880</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.999886</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.999747</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53087</th>\n",
       "      <td>0.999219</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.998473</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.997843</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.001182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53088</th>\n",
       "      <td>0.989065</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.984659</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.004970</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015349</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.005571</td>\n",
       "      <td>0.970240</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.011405</td>\n",
       "      <td>0.011495</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.005912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53089</th>\n",
       "      <td>0.998718</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.998228</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53090</th>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.999918</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53091</th>\n",
       "      <td>0.999688</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.999498</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.999057</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53092</th>\n",
       "      <td>0.999945</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.999932</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.999944</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53093 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         0  \\\n",
       "0      0.999581  0.000019  0.000090  0.000037  0.000014  0.000258  0.999388   \n",
       "1      0.998742  0.000117  0.000580  0.000050  0.000021  0.000489  0.998390   \n",
       "2      0.999794  0.000018  0.000042  0.000056  0.000010  0.000079  0.999755   \n",
       "3      0.999773  0.000011  0.000045  0.000022  0.000010  0.000139  0.999636   \n",
       "4      0.999911  0.000005  0.000028  0.000008  0.000005  0.000042  0.999915   \n",
       "5      0.999383  0.000015  0.000509  0.000021  0.000014  0.000059  0.999491   \n",
       "6      0.999578  0.000017  0.000219  0.000027  0.000014  0.000144  0.999400   \n",
       "7      0.999919  0.000008  0.000017  0.000016  0.000006  0.000034  0.999906   \n",
       "8      0.998270  0.000065  0.000255  0.000115  0.000037  0.001257  0.997231   \n",
       "9      0.996067  0.000374  0.001832  0.000512  0.000148  0.001067  0.995041   \n",
       "10     0.999769  0.000010  0.000038  0.000011  0.000009  0.000163  0.999720   \n",
       "11     0.999921  0.000005  0.000022  0.000008  0.000005  0.000038  0.999908   \n",
       "12     0.999930  0.000005  0.000025  0.000007  0.000005  0.000028  0.999904   \n",
       "13     0.999801  0.000016  0.000076  0.000016  0.000009  0.000083  0.999742   \n",
       "14     0.000343  0.000049  0.000076  0.000055  0.000018  0.999460  0.000320   \n",
       "15     0.999585  0.000023  0.000029  0.000025  0.000012  0.000326  0.999560   \n",
       "16     0.999729  0.000020  0.000092  0.000035  0.000011  0.000112  0.999656   \n",
       "17     0.999752  0.000012  0.000169  0.000025  0.000011  0.000032  0.999664   \n",
       "18     0.999923  0.000007  0.000020  0.000014  0.000005  0.000031  0.999904   \n",
       "19     0.999466  0.000026  0.000244  0.000042  0.000023  0.000200  0.999259   \n",
       "20     0.999834  0.000011  0.000061  0.000029  0.000013  0.000052  0.999810   \n",
       "21     0.999828  0.000029  0.000086  0.000016  0.000008  0.000033  0.999771   \n",
       "22     0.014228  0.000158  0.000456  0.000106  0.000052  0.985000  0.025771   \n",
       "23     0.999858  0.000021  0.000056  0.000024  0.000011  0.000030  0.999794   \n",
       "24     0.999517  0.000017  0.000058  0.000030  0.000015  0.000362  0.998898   \n",
       "25     0.999794  0.000031  0.000081  0.000030  0.000009  0.000055  0.999777   \n",
       "26     0.998247  0.000072  0.000187  0.000057  0.000025  0.001413  0.998326   \n",
       "27     0.999951  0.000004  0.000012  0.000006  0.000004  0.000023  0.999943   \n",
       "28     0.999656  0.000015  0.000251  0.000030  0.000013  0.000035  0.999484   \n",
       "29     0.999760  0.000023  0.000043  0.000043  0.000018  0.000114  0.999738   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "53063  0.999830  0.000017  0.000044  0.000023  0.000017  0.000068  0.999785   \n",
       "53064  0.999682  0.000016  0.000096  0.000024  0.000015  0.000167  0.999638   \n",
       "53065  0.996506  0.000027  0.000085  0.000068  0.000022  0.003293  0.997090   \n",
       "53066  0.999560  0.000030  0.000213  0.000040  0.000026  0.000131  0.999598   \n",
       "53067  0.000575  0.000049  0.000249  0.000281  0.000054  0.998793  0.000506   \n",
       "53068  0.999859  0.000015  0.000037  0.000018  0.000016  0.000056  0.999850   \n",
       "53069  0.999705  0.000015  0.000162  0.000028  0.000011  0.000078  0.999584   \n",
       "53070  0.999946  0.000005  0.000024  0.000007  0.000004  0.000014  0.999943   \n",
       "53071  0.999839  0.000014  0.000065  0.000018  0.000010  0.000055  0.999801   \n",
       "53072  0.999927  0.000005  0.000022  0.000007  0.000005  0.000034  0.999906   \n",
       "53073  0.999754  0.000013  0.000035  0.000026  0.000012  0.000159  0.999806   \n",
       "53074  0.999830  0.000011  0.000027  0.000032  0.000012  0.000088  0.999826   \n",
       "53075  0.999790  0.000010  0.000115  0.000037  0.000009  0.000039  0.999637   \n",
       "53076  0.999789  0.000015  0.000065  0.000017  0.000008  0.000106  0.999789   \n",
       "53077  0.999915  0.000006  0.000024  0.000008  0.000005  0.000043  0.999906   \n",
       "53078  0.999926  0.000006  0.000025  0.000008  0.000007  0.000028  0.999916   \n",
       "53079  0.999912  0.000006  0.000024  0.000008  0.000005  0.000045  0.999906   \n",
       "53080  0.999515  0.000015  0.000326  0.000024  0.000014  0.000105  0.999216   \n",
       "53081  0.998556  0.000090  0.000120  0.000040  0.000020  0.001174  0.998764   \n",
       "53082  0.999830  0.000012  0.000050  0.000022  0.000010  0.000076  0.999569   \n",
       "53083  0.999734  0.000011  0.000026  0.000023  0.000131  0.000075  0.999719   \n",
       "53084  0.999948  0.000004  0.000014  0.000006  0.000004  0.000023  0.999942   \n",
       "53085  0.999856  0.000013  0.000042  0.000015  0.000010  0.000065  0.999821   \n",
       "53086  0.999880  0.000007  0.000030  0.000013  0.000007  0.000062  0.999886   \n",
       "53087  0.999219  0.000034  0.000228  0.000200  0.000034  0.000284  0.998473   \n",
       "53088  0.989065  0.000207  0.003816  0.004518  0.000236  0.002159  0.984659   \n",
       "53089  0.998718  0.000141  0.000327  0.000307  0.000060  0.000447  0.998900   \n",
       "53090  0.999932  0.000005  0.000016  0.000006  0.000004  0.000037  0.999918   \n",
       "53091  0.999688  0.000077  0.000105  0.000038  0.000015  0.000078  0.999498   \n",
       "53092  0.999945  0.000004  0.000019  0.000006  0.000004  0.000022  0.999932   \n",
       "\n",
       "              1         2         3    ...            2         3         4  \\\n",
       "0      0.000020  0.000065  0.000051    ...     0.000220  0.000095  0.000037   \n",
       "1      0.000081  0.000728  0.000091    ...     0.002001  0.000223  0.000068   \n",
       "2      0.000019  0.000043  0.000082    ...     0.000076  0.000181  0.000024   \n",
       "3      0.000013  0.000048  0.000029    ...     0.000085  0.000067  0.000022   \n",
       "4      0.000006  0.000029  0.000008    ...     0.000055  0.000020  0.000011   \n",
       "5      0.000015  0.000405  0.000021    ...     0.000721  0.000039  0.000027   \n",
       "6      0.000021  0.000351  0.000035    ...     0.000292  0.000054  0.000031   \n",
       "7      0.000007  0.000027  0.000017    ...     0.000041  0.000032  0.000013   \n",
       "8      0.000093  0.000299  0.000169    ...     0.000671  0.000280  0.000125   \n",
       "9      0.000401  0.002290  0.000700    ...     0.007230  0.001693  0.000569   \n",
       "10     0.000012  0.000047  0.000013    ...     0.000096  0.000030  0.000021   \n",
       "11     0.000006  0.000023  0.000009    ...     0.000050  0.000019  0.000011   \n",
       "12     0.000006  0.000035  0.000009    ...     0.000047  0.000016  0.000010   \n",
       "13     0.000018  0.000089  0.000017    ...     0.000215  0.000046  0.000028   \n",
       "14     0.000037  0.000056  0.000057    ...     0.000076  0.000072  0.000026   \n",
       "15     0.000049  0.000043  0.000022    ...     0.000079  0.000058  0.000031   \n",
       "16     0.000016  0.000059  0.000050    ...     0.000104  0.000057  0.000025   \n",
       "17     0.000015  0.000240  0.000030    ...     0.000421  0.000058  0.000025   \n",
       "18     0.000008  0.000028  0.000012    ...     0.000082  0.000028  0.000013   \n",
       "19     0.000028  0.000379  0.000050    ...     0.000488  0.000093  0.000052   \n",
       "20     0.000014  0.000057  0.000029    ...     0.000146  0.000057  0.000023   \n",
       "21     0.000052  0.000100  0.000019    ...     0.000205  0.000038  0.000021   \n",
       "22     0.000222  0.000615  0.000156    ...     0.001260  0.000331  0.000184   \n",
       "23     0.000021  0.000103  0.000024    ...     0.000138  0.000057  0.000025   \n",
       "24     0.000018  0.000065  0.000028    ...     0.000101  0.000059  0.000035   \n",
       "25     0.000047  0.000082  0.000027    ...     0.000187  0.000057  0.000019   \n",
       "26     0.000085  0.000292  0.000072    ...     0.000270  0.000117  0.000048   \n",
       "27     0.000005  0.000016  0.000007    ...     0.000030  0.000014  0.000010   \n",
       "28     0.000017  0.000411  0.000028    ...     0.000349  0.000063  0.000026   \n",
       "29     0.000024  0.000046  0.000056    ...     0.000171  0.000129  0.000050   \n",
       "...         ...       ...       ...    ...          ...       ...       ...   \n",
       "53063  0.000022  0.000043  0.000028    ...     0.000103  0.000061  0.000068   \n",
       "53064  0.000017  0.000099  0.000026    ...     0.000138  0.000048  0.000031   \n",
       "53065  0.000025  0.000147  0.000066    ...     0.000228  0.000137  0.000037   \n",
       "53066  0.000030  0.000192  0.000039    ...     0.000433  0.000081  0.000056   \n",
       "53067  0.000049  0.000301  0.000303    ...     0.000306  0.000236  0.000040   \n",
       "53068  0.000016  0.000037  0.000019    ...     0.000079  0.000042  0.000038   \n",
       "53069  0.000019  0.000272  0.000032    ...     0.000350  0.000060  0.000024   \n",
       "53070  0.000006  0.000022  0.000007    ...     0.000048  0.000016  0.000010   \n",
       "53071  0.000018  0.000090  0.000019    ...     0.000089  0.000034  0.000019   \n",
       "53072  0.000006  0.000026  0.000008    ...     0.000047  0.000016  0.000011   \n",
       "53073  0.000011  0.000027  0.000030    ...     0.000060  0.000060  0.000026   \n",
       "53074  0.000010  0.000035  0.000030    ...     0.000064  0.000048  0.000028   \n",
       "53075  0.000015  0.000219  0.000064    ...     0.000336  0.000112  0.000031   \n",
       "53076  0.000015  0.000072  0.000019    ...     0.000086  0.000042  0.000021   \n",
       "53077  0.000006  0.000026  0.000008    ...     0.000048  0.000017  0.000011   \n",
       "53078  0.000006  0.000032  0.000009    ...     0.000046  0.000015  0.000011   \n",
       "53079  0.000006  0.000025  0.000008    ...     0.000050  0.000017  0.000011   \n",
       "53080  0.000020  0.000569  0.000029    ...     0.000459  0.000050  0.000030   \n",
       "53081  0.000062  0.000210  0.000042    ...     0.000242  0.000081  0.000040   \n",
       "53082  0.000017  0.000112  0.000072    ...     0.000105  0.000076  0.000034   \n",
       "53083  0.000013  0.000057  0.000025    ...     0.000086  0.000061  0.000090   \n",
       "53084  0.000005  0.000015  0.000007    ...     0.000027  0.000014  0.000010   \n",
       "53085  0.000013  0.000059  0.000016    ...     0.000081  0.000029  0.000021   \n",
       "53086  0.000006  0.000034  0.000011    ...     0.000156  0.000044  0.000023   \n",
       "53087  0.000061  0.000290  0.000462    ...     0.000392  0.000726  0.000083   \n",
       "53088  0.000202  0.004970  0.007436    ...     0.015349  0.010630  0.000820   \n",
       "53089  0.000119  0.000293  0.000291    ...     0.001578  0.000937  0.000268   \n",
       "53090  0.000005  0.000023  0.000007    ...     0.000039  0.000014  0.000011   \n",
       "53091  0.000088  0.000264  0.000042    ...     0.000336  0.000078  0.000028   \n",
       "53092  0.000005  0.000026  0.000007    ...     0.000037  0.000015  0.000011   \n",
       "\n",
       "              5         0         1         2         3         4         5  \n",
       "0      0.000832  0.998179  0.000060  0.000258  0.000137  0.000042  0.001325  \n",
       "1      0.001601  0.997043  0.000205  0.001350  0.000180  0.000048  0.001174  \n",
       "2      0.000121  0.999548  0.000028  0.000084  0.000179  0.000023  0.000138  \n",
       "3      0.000489  0.999588  0.000024  0.000078  0.000047  0.000020  0.000242  \n",
       "4      0.000087  0.999832  0.000011  0.000061  0.000017  0.000010  0.000069  \n",
       "5      0.000073  0.999197  0.000031  0.000616  0.000039  0.000029  0.000087  \n",
       "6      0.000218  0.999354  0.000052  0.000271  0.000050  0.000027  0.000246  \n",
       "7      0.000049  0.999889  0.000012  0.000026  0.000023  0.000009  0.000040  \n",
       "8      0.004492  0.994173  0.000116  0.000500  0.000261  0.000091  0.004860  \n",
       "9      0.002651  0.992204  0.000334  0.004588  0.000983  0.000333  0.001558  \n",
       "10     0.000387  0.999573  0.000019  0.000096  0.000022  0.000015  0.000275  \n",
       "11     0.000080  0.999837  0.000011  0.000060  0.000014  0.000010  0.000068  \n",
       "12     0.000050  0.999851  0.000011  0.000058  0.000014  0.000010  0.000056  \n",
       "13     0.000232  0.999667  0.000045  0.000105  0.000030  0.000017  0.000136  \n",
       "14     0.999590  0.000169  0.000038  0.000070  0.000080  0.000023  0.999621  \n",
       "15     0.001101  0.999125  0.000068  0.000079  0.000054  0.000031  0.000644  \n",
       "16     0.000263  0.999606  0.000019  0.000111  0.000065  0.000017  0.000182  \n",
       "17     0.000065  0.999525  0.000029  0.000282  0.000062  0.000025  0.000078  \n",
       "18     0.000068  0.999866  0.000014  0.000039  0.000024  0.000010  0.000047  \n",
       "19     0.000392  0.999144  0.000047  0.000292  0.000066  0.000042  0.000409  \n",
       "20     0.000138  0.999630  0.000022  0.000133  0.000052  0.000026  0.000137  \n",
       "21     0.000079  0.999721  0.000047  0.000116  0.000035  0.000017  0.000064  \n",
       "22     0.956102  0.015472  0.000104  0.000553  0.000112  0.000056  0.983703  \n",
       "23     0.000068  0.999631  0.000039  0.000176  0.000054  0.000024  0.000077  \n",
       "24     0.001513  0.999155  0.000028  0.000078  0.000040  0.000025  0.000674  \n",
       "25     0.000077  0.999608  0.000062  0.000136  0.000080  0.000021  0.000092  \n",
       "26     0.001787  0.997527  0.000205  0.000370  0.000121  0.000051  0.001725  \n",
       "27     0.000043  0.999897  0.000009  0.000032  0.000013  0.000009  0.000040  \n",
       "28     0.000066  0.999423  0.000030  0.000393  0.000054  0.000024  0.000076  \n",
       "29     0.000287  0.999597  0.000041  0.000081  0.000072  0.000033  0.000176  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "53063  0.000167  0.999543  0.000046  0.000072  0.000051  0.000058  0.000230  \n",
       "53064  0.000282  0.999476  0.000031  0.000165  0.000042  0.000026  0.000260  \n",
       "53065  0.006086  0.995939  0.000032  0.000101  0.000093  0.000026  0.003810  \n",
       "53066  0.000267  0.998900  0.000088  0.000607  0.000095  0.000065  0.000244  \n",
       "53067  0.998967  0.000318  0.000077  0.000370  0.000299  0.000044  0.998892  \n",
       "53068  0.000114  0.999677  0.000035  0.000096  0.000042  0.000035  0.000115  \n",
       "53069  0.000135  0.999393  0.000039  0.000305  0.000067  0.000028  0.000168  \n",
       "53070  0.000031  0.999902  0.000010  0.000037  0.000015  0.000008  0.000027  \n",
       "53071  0.000085  0.999706  0.000032  0.000103  0.000039  0.000020  0.000100  \n",
       "53072  0.000057  0.999839  0.000011  0.000059  0.000014  0.000010  0.000067  \n",
       "53073  0.000215  0.999569  0.000023  0.000062  0.000055  0.000022  0.000268  \n",
       "53074  0.000116  0.999738  0.000017  0.000050  0.000052  0.000027  0.000116  \n",
       "53075  0.000112  0.999495  0.000024  0.000305  0.000089  0.000022  0.000066  \n",
       "53076  0.000073  0.999745  0.000025  0.000104  0.000036  0.000018  0.000072  \n",
       "53077  0.000081  0.999831  0.000011  0.000060  0.000014  0.000010  0.000074  \n",
       "53078  0.000044  0.999853  0.000012  0.000060  0.000017  0.000011  0.000048  \n",
       "53079  0.000089  0.999826  0.000011  0.000060  0.000014  0.000010  0.000078  \n",
       "53080  0.000188  0.999115  0.000025  0.000619  0.000038  0.000023  0.000180  \n",
       "53081  0.001847  0.998458  0.000103  0.000194  0.000066  0.000032  0.001147  \n",
       "53082  0.000133  0.999432  0.000030  0.000168  0.000082  0.000032  0.000255  \n",
       "53083  0.000185  0.999636  0.000026  0.000071  0.000048  0.000078  0.000142  \n",
       "53084  0.000052  0.999895  0.000010  0.000030  0.000013  0.000009  0.000043  \n",
       "53085  0.000102  0.999697  0.000024  0.000106  0.000033  0.000021  0.000119  \n",
       "53086  0.000176  0.999747  0.000015  0.000099  0.000028  0.000013  0.000098  \n",
       "53087  0.000818  0.997843  0.000105  0.000535  0.000263  0.000072  0.001182  \n",
       "53088  0.005571  0.970240  0.000399  0.011405  0.011495  0.000551  0.005912  \n",
       "53089  0.001120  0.998228  0.000211  0.000414  0.000444  0.000092  0.000612  \n",
       "53090  0.000078  0.999883  0.000010  0.000034  0.000012  0.000009  0.000052  \n",
       "53091  0.000170  0.999057  0.000153  0.000545  0.000062  0.000022  0.000160  \n",
       "53092  0.000046  0.999944  0.000005  0.000025  0.000006  0.000005  0.000016  \n",
       "\n",
       "[53093 rows x 60 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame(sub_sample.file_id)\n",
    "\n",
    "p['prob0'] = pred[0].mean(axis = 1)\n",
    "p['prob1'] = pred[1].mean(axis = 1)\n",
    "p['prob2'] = pred[2].mean(axis = 1)\n",
    "p['prob3'] = pred[3].mean(axis = 1)\n",
    "p['prob4'] = pred[4].mean(axis = 1)\n",
    "p['prob5'] = pred[5].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p.to_csv('/home/libo/Security/sub/8.22_10_sampling.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame(sub_sample.file_id)\n",
    "\n",
    "p['prob0'] = pred.iloc[:,-6:][0]\n",
    "p['prob1'] = pred.iloc[:,-6:][1]\n",
    "p['prob2'] = pred.iloc[:,-6:][2]\n",
    "p['prob3'] = pred.iloc[:,-6:][3]\n",
    "p['prob4'] = pred.iloc[:,-6:][4]\n",
    "p['prob5'] = pred.iloc[:,-6:][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p.to_csv('/home/libo/Security/sub/采样8.22.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998179</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.001325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.997043</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.001174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999548</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999588</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999832</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999197</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.999354</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.994173</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.004860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.992204</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.001558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999573</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999837</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.999621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.999125</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.999606</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000282</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.999144</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.999630</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.999721</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.015472</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.983703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.999631</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.999155</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.999608</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.997527</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.001725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.999897</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.999423</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.999597</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53063</th>\n",
       "      <td>0.999543</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53064</th>\n",
       "      <td>0.999476</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53065</th>\n",
       "      <td>0.995939</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.003810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53066</th>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53067</th>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.998892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53068</th>\n",
       "      <td>0.999677</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53069</th>\n",
       "      <td>0.999393</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53070</th>\n",
       "      <td>0.999902</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53071</th>\n",
       "      <td>0.999706</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53072</th>\n",
       "      <td>0.999839</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53073</th>\n",
       "      <td>0.999569</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53074</th>\n",
       "      <td>0.999738</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53075</th>\n",
       "      <td>0.999495</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53076</th>\n",
       "      <td>0.999745</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53077</th>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53078</th>\n",
       "      <td>0.999853</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53079</th>\n",
       "      <td>0.999826</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53080</th>\n",
       "      <td>0.999115</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53081</th>\n",
       "      <td>0.998458</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53082</th>\n",
       "      <td>0.999432</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53083</th>\n",
       "      <td>0.999636</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53084</th>\n",
       "      <td>0.999895</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53085</th>\n",
       "      <td>0.999697</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53086</th>\n",
       "      <td>0.999747</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53087</th>\n",
       "      <td>0.997843</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.001182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53088</th>\n",
       "      <td>0.970240</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.011405</td>\n",
       "      <td>0.011495</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.005912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53089</th>\n",
       "      <td>0.998228</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53090</th>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53091</th>\n",
       "      <td>0.999057</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53092</th>\n",
       "      <td>0.999944</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53093 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5\n",
       "0      0.998179  0.000060  0.000258  0.000137  0.000042  0.001325\n",
       "1      0.997043  0.000205  0.001350  0.000180  0.000048  0.001174\n",
       "2      0.999548  0.000028  0.000084  0.000179  0.000023  0.000138\n",
       "3      0.999588  0.000024  0.000078  0.000047  0.000020  0.000242\n",
       "4      0.999832  0.000011  0.000061  0.000017  0.000010  0.000069\n",
       "5      0.999197  0.000031  0.000616  0.000039  0.000029  0.000087\n",
       "6      0.999354  0.000052  0.000271  0.000050  0.000027  0.000246\n",
       "7      0.999889  0.000012  0.000026  0.000023  0.000009  0.000040\n",
       "8      0.994173  0.000116  0.000500  0.000261  0.000091  0.004860\n",
       "9      0.992204  0.000334  0.004588  0.000983  0.000333  0.001558\n",
       "10     0.999573  0.000019  0.000096  0.000022  0.000015  0.000275\n",
       "11     0.999837  0.000011  0.000060  0.000014  0.000010  0.000068\n",
       "12     0.999851  0.000011  0.000058  0.000014  0.000010  0.000056\n",
       "13     0.999667  0.000045  0.000105  0.000030  0.000017  0.000136\n",
       "14     0.000169  0.000038  0.000070  0.000080  0.000023  0.999621\n",
       "15     0.999125  0.000068  0.000079  0.000054  0.000031  0.000644\n",
       "16     0.999606  0.000019  0.000111  0.000065  0.000017  0.000182\n",
       "17     0.999525  0.000029  0.000282  0.000062  0.000025  0.000078\n",
       "18     0.999866  0.000014  0.000039  0.000024  0.000010  0.000047\n",
       "19     0.999144  0.000047  0.000292  0.000066  0.000042  0.000409\n",
       "20     0.999630  0.000022  0.000133  0.000052  0.000026  0.000137\n",
       "21     0.999721  0.000047  0.000116  0.000035  0.000017  0.000064\n",
       "22     0.015472  0.000104  0.000553  0.000112  0.000056  0.983703\n",
       "23     0.999631  0.000039  0.000176  0.000054  0.000024  0.000077\n",
       "24     0.999155  0.000028  0.000078  0.000040  0.000025  0.000674\n",
       "25     0.999608  0.000062  0.000136  0.000080  0.000021  0.000092\n",
       "26     0.997527  0.000205  0.000370  0.000121  0.000051  0.001725\n",
       "27     0.999897  0.000009  0.000032  0.000013  0.000009  0.000040\n",
       "28     0.999423  0.000030  0.000393  0.000054  0.000024  0.000076\n",
       "29     0.999597  0.000041  0.000081  0.000072  0.000033  0.000176\n",
       "...         ...       ...       ...       ...       ...       ...\n",
       "53063  0.999543  0.000046  0.000072  0.000051  0.000058  0.000230\n",
       "53064  0.999476  0.000031  0.000165  0.000042  0.000026  0.000260\n",
       "53065  0.995939  0.000032  0.000101  0.000093  0.000026  0.003810\n",
       "53066  0.998900  0.000088  0.000607  0.000095  0.000065  0.000244\n",
       "53067  0.000318  0.000077  0.000370  0.000299  0.000044  0.998892\n",
       "53068  0.999677  0.000035  0.000096  0.000042  0.000035  0.000115\n",
       "53069  0.999393  0.000039  0.000305  0.000067  0.000028  0.000168\n",
       "53070  0.999902  0.000010  0.000037  0.000015  0.000008  0.000027\n",
       "53071  0.999706  0.000032  0.000103  0.000039  0.000020  0.000100\n",
       "53072  0.999839  0.000011  0.000059  0.000014  0.000010  0.000067\n",
       "53073  0.999569  0.000023  0.000062  0.000055  0.000022  0.000268\n",
       "53074  0.999738  0.000017  0.000050  0.000052  0.000027  0.000116\n",
       "53075  0.999495  0.000024  0.000305  0.000089  0.000022  0.000066\n",
       "53076  0.999745  0.000025  0.000104  0.000036  0.000018  0.000072\n",
       "53077  0.999831  0.000011  0.000060  0.000014  0.000010  0.000074\n",
       "53078  0.999853  0.000012  0.000060  0.000017  0.000011  0.000048\n",
       "53079  0.999826  0.000011  0.000060  0.000014  0.000010  0.000078\n",
       "53080  0.999115  0.000025  0.000619  0.000038  0.000023  0.000180\n",
       "53081  0.998458  0.000103  0.000194  0.000066  0.000032  0.001147\n",
       "53082  0.999432  0.000030  0.000168  0.000082  0.000032  0.000255\n",
       "53083  0.999636  0.000026  0.000071  0.000048  0.000078  0.000142\n",
       "53084  0.999895  0.000010  0.000030  0.000013  0.000009  0.000043\n",
       "53085  0.999697  0.000024  0.000106  0.000033  0.000021  0.000119\n",
       "53086  0.999747  0.000015  0.000099  0.000028  0.000013  0.000098\n",
       "53087  0.997843  0.000105  0.000535  0.000263  0.000072  0.001182\n",
       "53088  0.970240  0.000399  0.011405  0.011495  0.000551  0.005912\n",
       "53089  0.998228  0.000211  0.000414  0.000444  0.000092  0.000612\n",
       "53090  0.999883  0.000010  0.000034  0.000012  0.000009  0.000052\n",
       "53091  0.999057  0.000153  0.000545  0.000062  0.000022  0.000160\n",
       "53092  0.999944  0.000005  0.000025  0.000006  0.000005  0.000016\n",
       "\n",
       "[53093 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.iloc[:,-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998995</td>\n",
       "      <td>0.997458</td>\n",
       "      <td>0.999229</td>\n",
       "      <td>0.995560</td>\n",
       "      <td>0.995657</td>\n",
       "      <td>0.998620</td>\n",
       "      <td>0.998792</td>\n",
       "      <td>0.997231</td>\n",
       "      <td>0.997433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997954</td>\n",
       "      <td>0.998910</td>\n",
       "      <td>0.996312</td>\n",
       "      <td>0.996543</td>\n",
       "      <td>0.998944</td>\n",
       "      <td>0.998966</td>\n",
       "      <td>0.998017</td>\n",
       "      <td>0.997774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997458</td>\n",
       "      <td>0.997954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997542</td>\n",
       "      <td>0.998286</td>\n",
       "      <td>0.998402</td>\n",
       "      <td>0.998537</td>\n",
       "      <td>0.998050</td>\n",
       "      <td>0.998647</td>\n",
       "      <td>0.998878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999229</td>\n",
       "      <td>0.998910</td>\n",
       "      <td>0.997542</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995949</td>\n",
       "      <td>0.995738</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>0.998673</td>\n",
       "      <td>0.997415</td>\n",
       "      <td>0.997602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.995560</td>\n",
       "      <td>0.996312</td>\n",
       "      <td>0.998286</td>\n",
       "      <td>0.995949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.997135</td>\n",
       "      <td>0.996792</td>\n",
       "      <td>0.998112</td>\n",
       "      <td>0.997894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.995657</td>\n",
       "      <td>0.996543</td>\n",
       "      <td>0.998402</td>\n",
       "      <td>0.995738</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997371</td>\n",
       "      <td>0.996697</td>\n",
       "      <td>0.998051</td>\n",
       "      <td>0.998293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.998620</td>\n",
       "      <td>0.998944</td>\n",
       "      <td>0.998537</td>\n",
       "      <td>0.998690</td>\n",
       "      <td>0.997135</td>\n",
       "      <td>0.997371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999025</td>\n",
       "      <td>0.998329</td>\n",
       "      <td>0.998540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.998792</td>\n",
       "      <td>0.998966</td>\n",
       "      <td>0.998050</td>\n",
       "      <td>0.998673</td>\n",
       "      <td>0.996792</td>\n",
       "      <td>0.996697</td>\n",
       "      <td>0.999025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997853</td>\n",
       "      <td>0.998068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.997231</td>\n",
       "      <td>0.998017</td>\n",
       "      <td>0.998647</td>\n",
       "      <td>0.997415</td>\n",
       "      <td>0.998112</td>\n",
       "      <td>0.998051</td>\n",
       "      <td>0.998329</td>\n",
       "      <td>0.997853</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.997433</td>\n",
       "      <td>0.997774</td>\n",
       "      <td>0.998878</td>\n",
       "      <td>0.997602</td>\n",
       "      <td>0.997894</td>\n",
       "      <td>0.998293</td>\n",
       "      <td>0.998540</td>\n",
       "      <td>0.998068</td>\n",
       "      <td>0.998517</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.000000  0.998995  0.997458  0.999229  0.995560  0.995657  0.998620   \n",
       "1  0.998995  1.000000  0.997954  0.998910  0.996312  0.996543  0.998944   \n",
       "2  0.997458  0.997954  1.000000  0.997542  0.998286  0.998402  0.998537   \n",
       "3  0.999229  0.998910  0.997542  1.000000  0.995949  0.995738  0.998690   \n",
       "4  0.995560  0.996312  0.998286  0.995949  1.000000  0.998316  0.997135   \n",
       "5  0.995657  0.996543  0.998402  0.995738  0.998316  1.000000  0.997371   \n",
       "6  0.998620  0.998944  0.998537  0.998690  0.997135  0.997371  1.000000   \n",
       "7  0.998792  0.998966  0.998050  0.998673  0.996792  0.996697  0.999025   \n",
       "8  0.997231  0.998017  0.998647  0.997415  0.998112  0.998051  0.998329   \n",
       "9  0.997433  0.997774  0.998878  0.997602  0.997894  0.998293  0.998540   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.998792  0.997231  0.997433  \n",
       "1  0.998966  0.998017  0.997774  \n",
       "2  0.998050  0.998647  0.998878  \n",
       "3  0.998673  0.997415  0.997602  \n",
       "4  0.996792  0.998112  0.997894  \n",
       "5  0.996697  0.998051  0.998293  \n",
       "6  0.999025  0.998329  0.998540  \n",
       "7  1.000000  0.997853  0.998068  \n",
       "8  0.997853  1.000000  0.998517  \n",
       "9  0.998068  0.998517  1.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.corrcoef(pred[0], rowvar = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_pickle('top_feature1000.pkl')\n",
    "X = X.iloc[:,:300]\n",
    "sub_sample = pd.read_csv('3rd_security_submit_sample.csv')\n",
    "\n",
    "label = pd.read_pickle('label.pkl')\n",
    "label = label[0].values\n",
    "\n",
    "X['id'] = X.iloc[:, 0].values\n",
    "\n",
    "X.drop('file_id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libo/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train_FE = X.iloc[:len(label),:]\n",
    "test_FE = X.iloc[len(label):,:]\n",
    "\n",
    "train_FE['label'] = label\n",
    "\n",
    "test_FE.drop('id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116624, 300), (53093, 299))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_FE.shape, test_FE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((111545, 300), (5079, 300))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = train_FE[train_FE['label'] == 0]\n",
    "\n",
    "x1 = train_FE[train_FE['label'] != 0]\n",
    "\n",
    "x0.shape, x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = pd.read_csv('index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multiclass\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.05,\n",
    "          \"num_leaves\": 5,\n",
    "          # \"max_bin\": 128,\n",
    "          \"feature_fraction\": 0.85,\n",
    "#           \"min_child_samples\": 10,\n",
    "#           \"min_child_weight\": 150,\n",
    "#           \"min_split_gain\": 0,\n",
    "          \"subsample\": 0.85,\n",
    "          #'metric':'logloss',\n",
    "#            'lambda_l1':0.02,\n",
    "#            'lambda_l2':0.02,\n",
    "          'seed':666,\n",
    "          'num_class':6\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54056"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(2)\n",
    "int(x0.shape[0]*random.uniform(1.5,5)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********第0次抽样***********\n",
      "负样本抽样数量：\n",
      "(24715, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.615233 + 0.00358708\n",
      "[40]\tcv_agg's multi_logloss: 0.292297 + 0.00414274\n",
      "[60]\tcv_agg's multi_logloss: 0.170287 + 0.00412122\n",
      "[80]\tcv_agg's multi_logloss: 0.118341 + 0.00405836\n",
      "[100]\tcv_agg's multi_logloss: 0.0933328 + 0.00398645\n",
      "[120]\tcv_agg's multi_logloss: 0.0793913 + 0.00393392\n",
      "[140]\tcv_agg's multi_logloss: 0.0707632 + 0.00410048\n",
      "[160]\tcv_agg's multi_logloss: 0.0650659 + 0.00422598\n",
      "[180]\tcv_agg's multi_logloss: 0.0610856 + 0.0042795\n",
      "[200]\tcv_agg's multi_logloss: 0.0581965 + 0.00441624\n",
      "[220]\tcv_agg's multi_logloss: 0.0560443 + 0.00442188\n",
      "[240]\tcv_agg's multi_logloss: 0.0543384 + 0.00452162\n",
      "[260]\tcv_agg's multi_logloss: 0.0529656 + 0.00463049\n",
      "[280]\tcv_agg's multi_logloss: 0.0519329 + 0.00470479\n",
      "[300]\tcv_agg's multi_logloss: 0.051075 + 0.00480555\n",
      "[320]\tcv_agg's multi_logloss: 0.0503055 + 0.00489633\n",
      "[340]\tcv_agg's multi_logloss: 0.0496798 + 0.00497883\n",
      "[360]\tcv_agg's multi_logloss: 0.0491524 + 0.00508604\n",
      "[380]\tcv_agg's multi_logloss: 0.0487771 + 0.00525152\n",
      "[400]\tcv_agg's multi_logloss: 0.0484036 + 0.00537644\n",
      "[420]\tcv_agg's multi_logloss: 0.0481443 + 0.00547\n",
      "[440]\tcv_agg's multi_logloss: 0.0479262 + 0.005577\n",
      "[460]\tcv_agg's multi_logloss: 0.0477786 + 0.00568488\n",
      "[480]\tcv_agg's multi_logloss: 0.0475761 + 0.00572587\n",
      "[500]\tcv_agg's multi_logloss: 0.0474562 + 0.00577975\n",
      "[520]\tcv_agg's multi_logloss: 0.0474397 + 0.00587428\n",
      "[540]\tcv_agg's multi_logloss: 0.0474078 + 0.00599617\n",
      "[560]\tcv_agg's multi_logloss: 0.0473608 + 0.00607863\n",
      "[580]\tcv_agg's multi_logloss: 0.0473912 + 0.00618386\n",
      "最优结果0.04735130628453371\n",
      "总loss[0.04735130628453371]\n",
      "[10]\ttraining's multi_logloss: 0.975927\n",
      "[20]\ttraining's multi_logloss: 0.612893\n",
      "[30]\ttraining's multi_logloss: 0.40954\n",
      "[40]\ttraining's multi_logloss: 0.288277\n",
      "[50]\ttraining's multi_logloss: 0.213571\n",
      "[60]\ttraining's multi_logloss: 0.165143\n",
      "[70]\ttraining's multi_logloss: 0.133695\n",
      "[80]\ttraining's multi_logloss: 0.112044\n",
      "[90]\ttraining's multi_logloss: 0.0965961\n",
      "[100]\ttraining's multi_logloss: 0.0853965\n",
      "[110]\ttraining's multi_logloss: 0.0768414\n",
      "[120]\ttraining's multi_logloss: 0.0701303\n",
      "[130]\ttraining's multi_logloss: 0.0646321\n",
      "[140]\ttraining's multi_logloss: 0.0599954\n",
      "[150]\ttraining's multi_logloss: 0.0559826\n",
      "[160]\ttraining's multi_logloss: 0.0526569\n",
      "[170]\ttraining's multi_logloss: 0.0497455\n",
      "[180]\ttraining's multi_logloss: 0.0471108\n",
      "[190]\ttraining's multi_logloss: 0.0447231\n",
      "[200]\ttraining's multi_logloss: 0.0426564\n",
      "[210]\ttraining's multi_logloss: 0.0408356\n",
      "[220]\ttraining's multi_logloss: 0.0391076\n",
      "[230]\ttraining's multi_logloss: 0.0375415\n",
      "[240]\ttraining's multi_logloss: 0.0361407\n",
      "[250]\ttraining's multi_logloss: 0.0348192\n",
      "[260]\ttraining's multi_logloss: 0.033507\n",
      "[270]\ttraining's multi_logloss: 0.0323866\n",
      "[280]\ttraining's multi_logloss: 0.0312996\n",
      "[290]\ttraining's multi_logloss: 0.0303324\n",
      "[300]\ttraining's multi_logloss: 0.0293864\n",
      "[310]\ttraining's multi_logloss: 0.0285246\n",
      "[320]\ttraining's multi_logloss: 0.0276981\n",
      "[330]\ttraining's multi_logloss: 0.0268886\n",
      "[340]\ttraining's multi_logloss: 0.0261313\n",
      "[350]\ttraining's multi_logloss: 0.0254176\n",
      "[360]\ttraining's multi_logloss: 0.0247168\n",
      "[370]\ttraining's multi_logloss: 0.0240828\n",
      "[380]\ttraining's multi_logloss: 0.0234765\n",
      "[390]\ttraining's multi_logloss: 0.0228742\n",
      "[400]\ttraining's multi_logloss: 0.0222824\n",
      "[410]\ttraining's multi_logloss: 0.0216928\n",
      "[420]\ttraining's multi_logloss: 0.0211598\n",
      "[430]\ttraining's multi_logloss: 0.0206458\n",
      "[440]\ttraining's multi_logloss: 0.0201042\n",
      "[450]\ttraining's multi_logloss: 0.0196156\n",
      "[460]\ttraining's multi_logloss: 0.0191489\n",
      "[470]\ttraining's multi_logloss: 0.0186988\n",
      "[480]\ttraining's multi_logloss: 0.0182479\n",
      "[490]\ttraining's multi_logloss: 0.0178233\n",
      "[500]\ttraining's multi_logloss: 0.0174256\n",
      "[510]\ttraining's multi_logloss: 0.0170061\n",
      "[520]\ttraining's multi_logloss: 0.0166311\n",
      "[530]\ttraining's multi_logloss: 0.0162698\n",
      "[540]\ttraining's multi_logloss: 0.0159076\n",
      "[550]\ttraining's multi_logloss: 0.0155719\n",
      "[560]\ttraining's multi_logloss: 0.0152333\n",
      "[570]\ttraining's multi_logloss: 0.0149101\n",
      "[580]\ttraining's multi_logloss: 0.0146091\n",
      "[590]\ttraining's multi_logloss: 0.0143036\n",
      "[600]\ttraining's multi_logloss: 0.0140077\n",
      "[610]\ttraining's multi_logloss: 0.0137202\n",
      "***********第1次抽样***********\n",
      "负样本抽样数量：\n",
      "(19829, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.629347 + 0.00243062\n",
      "[40]\tcv_agg's multi_logloss: 0.307612 + 0.00415971\n",
      "[60]\tcv_agg's multi_logloss: 0.184715 + 0.00520566\n",
      "[80]\tcv_agg's multi_logloss: 0.131177 + 0.0059615\n",
      "[100]\tcv_agg's multi_logloss: 0.1051 + 0.00614739\n",
      "[120]\tcv_agg's multi_logloss: 0.0904567 + 0.00624648\n",
      "[140]\tcv_agg's multi_logloss: 0.0813314 + 0.00617009\n",
      "[160]\tcv_agg's multi_logloss: 0.0752997 + 0.00605414\n",
      "[180]\tcv_agg's multi_logloss: 0.0710459 + 0.00575677\n",
      "[200]\tcv_agg's multi_logloss: 0.0677495 + 0.0054935\n",
      "[220]\tcv_agg's multi_logloss: 0.065184 + 0.00545423\n",
      "[240]\tcv_agg's multi_logloss: 0.063278 + 0.00553488\n",
      "[260]\tcv_agg's multi_logloss: 0.0616727 + 0.00561134\n",
      "[280]\tcv_agg's multi_logloss: 0.0604017 + 0.00564569\n",
      "[300]\tcv_agg's multi_logloss: 0.0593585 + 0.00568619\n",
      "[320]\tcv_agg's multi_logloss: 0.0585137 + 0.00575267\n",
      "[340]\tcv_agg's multi_logloss: 0.0577838 + 0.00590819\n",
      "[360]\tcv_agg's multi_logloss: 0.0571836 + 0.00600882\n",
      "[380]\tcv_agg's multi_logloss: 0.0566769 + 0.00617553\n",
      "[400]\tcv_agg's multi_logloss: 0.0561758 + 0.00622145\n",
      "[420]\tcv_agg's multi_logloss: 0.0558184 + 0.00634052\n",
      "[440]\tcv_agg's multi_logloss: 0.055592 + 0.00646676\n",
      "[460]\tcv_agg's multi_logloss: 0.055364 + 0.00659991\n",
      "[480]\tcv_agg's multi_logloss: 0.0552638 + 0.0067203\n",
      "[500]\tcv_agg's multi_logloss: 0.0551354 + 0.00683422\n",
      "[520]\tcv_agg's multi_logloss: 0.0550533 + 0.00696774\n",
      "[540]\tcv_agg's multi_logloss: 0.0550003 + 0.00703731\n",
      "[560]\tcv_agg's multi_logloss: 0.0549694 + 0.00718346\n",
      "[580]\tcv_agg's multi_logloss: 0.0549263 + 0.00728421\n",
      "[600]\tcv_agg's multi_logloss: 0.0548975 + 0.00740717\n",
      "[620]\tcv_agg's multi_logloss: 0.05496 + 0.00757977\n",
      "最优结果0.054884275472127694\n",
      "总loss[0.04735130628453371, 0.054884275472127694]\n",
      "[10]\ttraining's multi_logloss: 0.987042\n",
      "[20]\ttraining's multi_logloss: 0.626433\n",
      "[30]\ttraining's multi_logloss: 0.42475\n",
      "[40]\ttraining's multi_logloss: 0.30341\n",
      "[50]\ttraining's multi_logloss: 0.227927\n",
      "[60]\ttraining's multi_logloss: 0.178799\n",
      "[70]\ttraining's multi_logloss: 0.146309\n",
      "[80]\ttraining's multi_logloss: 0.12371\n",
      "[90]\ttraining's multi_logloss: 0.10766\n",
      "[100]\ttraining's multi_logloss: 0.0958017\n",
      "[110]\ttraining's multi_logloss: 0.0865951\n",
      "[120]\ttraining's multi_logloss: 0.0793099\n",
      "[130]\ttraining's multi_logloss: 0.0733916\n",
      "[140]\ttraining's multi_logloss: 0.0683074\n",
      "[150]\ttraining's multi_logloss: 0.0641114\n",
      "[160]\ttraining's multi_logloss: 0.0604707\n",
      "[170]\ttraining's multi_logloss: 0.0572668\n",
      "[180]\ttraining's multi_logloss: 0.0543273\n",
      "[190]\ttraining's multi_logloss: 0.0517498\n",
      "[200]\ttraining's multi_logloss: 0.0494396\n",
      "[210]\ttraining's multi_logloss: 0.0472964\n",
      "[220]\ttraining's multi_logloss: 0.0453897\n",
      "[230]\ttraining's multi_logloss: 0.0436335\n",
      "[240]\ttraining's multi_logloss: 0.0419771\n",
      "[250]\ttraining's multi_logloss: 0.0404896\n",
      "[260]\ttraining's multi_logloss: 0.0390974\n",
      "[270]\ttraining's multi_logloss: 0.0377584\n",
      "[280]\ttraining's multi_logloss: 0.0365258\n",
      "[290]\ttraining's multi_logloss: 0.0353903\n",
      "[300]\ttraining's multi_logloss: 0.0342791\n",
      "[310]\ttraining's multi_logloss: 0.0332037\n",
      "[320]\ttraining's multi_logloss: 0.0321685\n",
      "[330]\ttraining's multi_logloss: 0.0312272\n",
      "[340]\ttraining's multi_logloss: 0.0302951\n",
      "[350]\ttraining's multi_logloss: 0.0294233\n",
      "[360]\ttraining's multi_logloss: 0.0285826\n",
      "[370]\ttraining's multi_logloss: 0.027804\n",
      "[380]\ttraining's multi_logloss: 0.0270662\n",
      "[390]\ttraining's multi_logloss: 0.0263447\n",
      "[400]\ttraining's multi_logloss: 0.0256676\n",
      "[410]\ttraining's multi_logloss: 0.0250135\n",
      "[420]\ttraining's multi_logloss: 0.0243922\n",
      "[430]\ttraining's multi_logloss: 0.0237915\n",
      "[440]\ttraining's multi_logloss: 0.0231994\n",
      "[450]\ttraining's multi_logloss: 0.0226319\n",
      "[460]\ttraining's multi_logloss: 0.0221035\n",
      "[470]\ttraining's multi_logloss: 0.0215954\n",
      "[480]\ttraining's multi_logloss: 0.0210939\n",
      "[490]\ttraining's multi_logloss: 0.0206016\n",
      "[500]\ttraining's multi_logloss: 0.0201176\n",
      "[510]\ttraining's multi_logloss: 0.0196571\n",
      "[520]\ttraining's multi_logloss: 0.0192064\n",
      "[530]\ttraining's multi_logloss: 0.0187889\n",
      "[540]\ttraining's multi_logloss: 0.0183647\n",
      "[550]\ttraining's multi_logloss: 0.0179598\n",
      "[560]\ttraining's multi_logloss: 0.0175559\n",
      "[570]\ttraining's multi_logloss: 0.0171398\n",
      "[580]\ttraining's multi_logloss: 0.0167739\n",
      "[590]\ttraining's multi_logloss: 0.0164418\n",
      "[600]\ttraining's multi_logloss: 0.0160924\n",
      "[610]\ttraining's multi_logloss: 0.0157758\n",
      "[620]\ttraining's multi_logloss: 0.0154403\n",
      "[630]\ttraining's multi_logloss: 0.0151268\n",
      "[640]\ttraining's multi_logloss: 0.0148307\n",
      "[650]\ttraining's multi_logloss: 0.0145281\n",
      "***********第2次抽样***********\n",
      "负样本抽样数量：\n",
      "(35278, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.59422 + 0.00173126\n",
      "[40]\tcv_agg's multi_logloss: 0.270274 + 0.00325863\n",
      "[60]\tcv_agg's multi_logloss: 0.150042 + 0.00385711\n",
      "[80]\tcv_agg's multi_logloss: 0.100053 + 0.00403741\n",
      "[100]\tcv_agg's multi_logloss: 0.0768323 + 0.00425672\n",
      "[120]\tcv_agg's multi_logloss: 0.0644281 + 0.00441921\n",
      "[140]\tcv_agg's multi_logloss: 0.0569706 + 0.00437426\n",
      "[160]\tcv_agg's multi_logloss: 0.0521199 + 0.00419709\n",
      "[180]\tcv_agg's multi_logloss: 0.048767 + 0.00410987\n",
      "[200]\tcv_agg's multi_logloss: 0.0464434 + 0.00412793\n",
      "[220]\tcv_agg's multi_logloss: 0.0447293 + 0.0042198\n",
      "[240]\tcv_agg's multi_logloss: 0.0433343 + 0.00428492\n",
      "[260]\tcv_agg's multi_logloss: 0.0422288 + 0.00436794\n",
      "[280]\tcv_agg's multi_logloss: 0.0413269 + 0.00444761\n",
      "[300]\tcv_agg's multi_logloss: 0.0406409 + 0.00453699\n",
      "[320]\tcv_agg's multi_logloss: 0.0400515 + 0.00459088\n",
      "[340]\tcv_agg's multi_logloss: 0.039531 + 0.00467574\n",
      "[360]\tcv_agg's multi_logloss: 0.0391347 + 0.00475386\n",
      "[380]\tcv_agg's multi_logloss: 0.038832 + 0.00483507\n",
      "[400]\tcv_agg's multi_logloss: 0.0386088 + 0.004898\n",
      "[420]\tcv_agg's multi_logloss: 0.0384293 + 0.00493695\n",
      "[440]\tcv_agg's multi_logloss: 0.0382343 + 0.00499643\n",
      "[460]\tcv_agg's multi_logloss: 0.0380821 + 0.00503608\n",
      "[480]\tcv_agg's multi_logloss: 0.0380084 + 0.00505528\n",
      "[500]\tcv_agg's multi_logloss: 0.0379309 + 0.00512846\n",
      "[520]\tcv_agg's multi_logloss: 0.0378583 + 0.00519988\n",
      "[540]\tcv_agg's multi_logloss: 0.0378614 + 0.00525491\n",
      "[560]\tcv_agg's multi_logloss: 0.0378521 + 0.0052918\n",
      "[580]\tcv_agg's multi_logloss: 0.0378247 + 0.00535456\n",
      "[600]\tcv_agg's multi_logloss: 0.0378646 + 0.00541997\n",
      "最优结果0.03781991717105289\n",
      "总loss[0.04735130628453371, 0.054884275472127694, 0.03781991717105289]\n",
      "[10]\ttraining's multi_logloss: 0.959284\n",
      "[20]\ttraining's multi_logloss: 0.593036\n",
      "[30]\ttraining's multi_logloss: 0.388243\n",
      "[40]\ttraining's multi_logloss: 0.267933\n",
      "[50]\ttraining's multi_logloss: 0.193856\n",
      "[60]\ttraining's multi_logloss: 0.146741\n",
      "[70]\ttraining's multi_logloss: 0.116373\n",
      "[80]\ttraining's multi_logloss: 0.0957526\n",
      "[90]\ttraining's multi_logloss: 0.0813652\n",
      "[100]\ttraining's multi_logloss: 0.0711569\n",
      "[110]\ttraining's multi_logloss: 0.0635664\n",
      "[120]\ttraining's multi_logloss: 0.0576068\n",
      "[130]\ttraining's multi_logloss: 0.0528757\n",
      "[140]\ttraining's multi_logloss: 0.049048\n",
      "[150]\ttraining's multi_logloss: 0.0456401\n",
      "[160]\ttraining's multi_logloss: 0.042761\n",
      "[170]\ttraining's multi_logloss: 0.0404192\n",
      "[180]\ttraining's multi_logloss: 0.0382416\n",
      "[190]\ttraining's multi_logloss: 0.0363379\n",
      "[200]\ttraining's multi_logloss: 0.0346497\n",
      "[210]\ttraining's multi_logloss: 0.0331389\n",
      "[220]\ttraining's multi_logloss: 0.0317119\n",
      "[230]\ttraining's multi_logloss: 0.0304652\n",
      "[240]\ttraining's multi_logloss: 0.0293241\n",
      "[250]\ttraining's multi_logloss: 0.0282681\n",
      "[260]\ttraining's multi_logloss: 0.0272842\n",
      "[270]\ttraining's multi_logloss: 0.0263457\n",
      "[280]\ttraining's multi_logloss: 0.0254572\n",
      "[290]\ttraining's multi_logloss: 0.024644\n",
      "[300]\ttraining's multi_logloss: 0.0238995\n",
      "[310]\ttraining's multi_logloss: 0.0231738\n",
      "[320]\ttraining's multi_logloss: 0.0224733\n",
      "[330]\ttraining's multi_logloss: 0.0218154\n",
      "[340]\ttraining's multi_logloss: 0.0211611\n",
      "[350]\ttraining's multi_logloss: 0.0205806\n",
      "[360]\ttraining's multi_logloss: 0.0199989\n",
      "[370]\ttraining's multi_logloss: 0.0194709\n",
      "[380]\ttraining's multi_logloss: 0.0189689\n",
      "[390]\ttraining's multi_logloss: 0.0184812\n",
      "[400]\ttraining's multi_logloss: 0.0180053\n",
      "[410]\ttraining's multi_logloss: 0.017561\n",
      "[420]\ttraining's multi_logloss: 0.0171283\n",
      "[430]\ttraining's multi_logloss: 0.0167304\n",
      "[440]\ttraining's multi_logloss: 0.0163105\n",
      "[450]\ttraining's multi_logloss: 0.0159402\n",
      "[460]\ttraining's multi_logloss: 0.0155661\n",
      "[470]\ttraining's multi_logloss: 0.0152081\n",
      "[480]\ttraining's multi_logloss: 0.0148669\n",
      "[490]\ttraining's multi_logloss: 0.0145186\n",
      "[500]\ttraining's multi_logloss: 0.0142034\n",
      "[510]\ttraining's multi_logloss: 0.0138954\n",
      "[520]\ttraining's multi_logloss: 0.0135934\n",
      "[530]\ttraining's multi_logloss: 0.0132995\n",
      "[540]\ttraining's multi_logloss: 0.0130023\n",
      "[550]\ttraining's multi_logloss: 0.0127316\n",
      "[560]\ttraining's multi_logloss: 0.012466\n",
      "[570]\ttraining's multi_logloss: 0.0122061\n",
      "[580]\ttraining's multi_logloss: 0.0119457\n",
      "[590]\ttraining's multi_logloss: 0.0117069\n",
      "[600]\ttraining's multi_logloss: 0.0114757\n",
      "[610]\ttraining's multi_logloss: 0.0112488\n",
      "[620]\ttraining's multi_logloss: 0.0110291\n",
      "[630]\ttraining's multi_logloss: 0.0107985\n",
      "[640]\ttraining's multi_logloss: 0.0105795\n",
      "***********第3次抽样***********\n",
      "负样本抽样数量：\n",
      "(26733, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.610304 + 0.00189279\n",
      "[40]\tcv_agg's multi_logloss: 0.286601 + 0.00259339\n",
      "[400]\tcv_agg's multi_logloss: 0.0409588 + 0.00380677\n",
      "[420]\tcv_agg's multi_logloss: 0.0406914 + 0.00385567\n",
      "[440]\tcv_agg's multi_logloss: 0.0404345 + 0.00388108\n",
      "[460]\tcv_agg's multi_logloss: 0.0402835 + 0.00390418\n",
      "[480]\tcv_agg's multi_logloss: 0.0401306 + 0.00397778\n",
      "[500]\tcv_agg's multi_logloss: 0.0400241 + 0.0040346\n",
      "[520]\tcv_agg's multi_logloss: 0.0399545 + 0.00413536\n",
      "[540]\tcv_agg's multi_logloss: 0.0399122 + 0.00418759\n",
      "[560]\tcv_agg's multi_logloss: 0.0399045 + 0.00421817\n",
      "[580]\tcv_agg's multi_logloss: 0.0398868 + 0.00431449\n",
      "[600]\tcv_agg's multi_logloss: 0.0398868 + 0.00436949\n",
      "[620]\tcv_agg's multi_logloss: 0.0398964 + 0.00445461\n",
      "最优结果0.039876161644722685\n",
      "总loss[0.04735130628453371, 0.054884275472127694, 0.03781991717105289, 0.04323592371048647, 0.04230347054874013, 0.05716380758019011, 0.039876161644722685]\n",
      "[10]\ttraining's multi_logloss: 0.964785\n",
      "[20]\ttraining's multi_logloss: 0.599004\n",
      "[30]\ttraining's multi_logloss: 0.394671\n",
      "[40]\ttraining's multi_logloss: 0.274189\n",
      "[50]\ttraining's multi_logloss: 0.199965\n",
      "[60]\ttraining's multi_logloss: 0.152167\n",
      "[70]\ttraining's multi_logloss: 0.121067\n",
      "[80]\ttraining's multi_logloss: 0.100286\n",
      "[90]\ttraining's multi_logloss: 0.0857077\n",
      "[100]\ttraining's multi_logloss: 0.0751587\n",
      "[110]\ttraining's multi_logloss: 0.0672332\n",
      "[120]\ttraining's multi_logloss: 0.061033\n",
      "[130]\ttraining's multi_logloss: 0.0561099\n",
      "[140]\ttraining's multi_logloss: 0.0520464\n",
      "[150]\ttraining's multi_logloss: 0.0483911\n",
      "[160]\ttraining's multi_logloss: 0.0454661\n",
      "[170]\ttraining's multi_logloss: 0.042879\n",
      "[180]\ttraining's multi_logloss: 0.040556\n",
      "[190]\ttraining's multi_logloss: 0.0385313\n",
      "[200]\ttraining's multi_logloss: 0.0366676\n",
      "[210]\ttraining's multi_logloss: 0.035103\n",
      "[220]\ttraining's multi_logloss: 0.0336469\n",
      "[230]\ttraining's multi_logloss: 0.0323473\n",
      "[240]\ttraining's multi_logloss: 0.0311603\n",
      "[250]\ttraining's multi_logloss: 0.0300732\n",
      "[260]\ttraining's multi_logloss: 0.0289724\n",
      "[270]\ttraining's multi_logloss: 0.027979\n",
      "[280]\ttraining's multi_logloss: 0.0270411\n",
      "[290]\ttraining's multi_logloss: 0.0261244\n",
      "[300]\ttraining's multi_logloss: 0.0253174\n",
      "[310]\ttraining's multi_logloss: 0.0245496\n",
      "[320]\ttraining's multi_logloss: 0.0238104\n",
      "[330]\ttraining's multi_logloss: 0.0231256\n",
      "[340]\ttraining's multi_logloss: 0.0224359\n",
      "[350]\ttraining's multi_logloss: 0.0218072\n",
      "[360]\ttraining's multi_logloss: 0.0211895\n",
      "[370]\ttraining's multi_logloss: 0.0205705\n",
      "[380]\ttraining's multi_logloss: 0.0200385\n",
      "[390]\ttraining's multi_logloss: 0.0194983\n",
      "[400]\ttraining's multi_logloss: 0.018977\n",
      "[410]\ttraining's multi_logloss: 0.0184868\n",
      "[420]\ttraining's multi_logloss: 0.0180258\n",
      "[430]\ttraining's multi_logloss: 0.0175784\n",
      "[440]\ttraining's multi_logloss: 0.0171637\n",
      "[450]\ttraining's multi_logloss: 0.0167463\n",
      "[460]\ttraining's multi_logloss: 0.0163465\n",
      "[470]\ttraining's multi_logloss: 0.015963\n",
      "[480]\ttraining's multi_logloss: 0.015595\n",
      "[490]\ttraining's multi_logloss: 0.0152304\n",
      "[500]\ttraining's multi_logloss: 0.0148689\n",
      "[510]\ttraining's multi_logloss: 0.0145396\n",
      "[520]\ttraining's multi_logloss: 0.014222\n",
      "[530]\ttraining's multi_logloss: 0.0139081\n",
      "[540]\ttraining's multi_logloss: 0.0136041\n",
      "[550]\ttraining's multi_logloss: 0.0133157\n",
      "[560]\ttraining's multi_logloss: 0.0130117\n",
      "[570]\ttraining's multi_logloss: 0.0127389\n",
      "[580]\ttraining's multi_logloss: 0.0124767\n",
      "[590]\ttraining's multi_logloss: 0.0122218\n",
      "[600]\ttraining's multi_logloss: 0.0119673\n",
      "[610]\ttraining's multi_logloss: 0.0117139\n",
      "[620]\ttraining's multi_logloss: 0.0114785\n",
      "[630]\ttraining's multi_logloss: 0.0112505\n",
      "[640]\ttraining's multi_logloss: 0.011024\n",
      "[650]\ttraining's multi_logloss: 0.0108055\n",
      "***********第7次抽样***********\n",
      "负样本抽样数量：\n",
      "(54716, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.571997 + 0.000642044\n",
      "[40]\tcv_agg's multi_logloss: 0.247204 + 0.00109838\n",
      "[60]\tcv_agg's multi_logloss: 0.129116 + 0.00136114\n",
      "[80]\tcv_agg's multi_logloss: 0.0815145 + 0.00154054\n",
      "[100]\tcv_agg's multi_logloss: 0.060094 + 0.00162149\n",
      "[120]\tcv_agg's multi_logloss: 0.0491886 + 0.00177708\n",
      "[140]\tcv_agg's multi_logloss: 0.0426285 + 0.00161008\n",
      "[160]\tcv_agg's multi_logloss: 0.0384714 + 0.00156109\n",
      "[180]\tcv_agg's multi_logloss: 0.0358097 + 0.00155099\n",
      "[200]\tcv_agg's multi_logloss: 0.0338453 + 0.00157749\n",
      "[220]\tcv_agg's multi_logloss: 0.032377 + 0.00158737\n",
      "[240]\tcv_agg's multi_logloss: 0.0313317 + 0.00157142\n",
      "[260]\tcv_agg's multi_logloss: 0.0304578 + 0.00155164\n",
      "[280]\tcv_agg's multi_logloss: 0.0297573 + 0.00153531\n",
      "[300]\tcv_agg's multi_logloss: 0.0291612 + 0.00150711\n",
      "[320]\tcv_agg's multi_logloss: 0.028677 + 0.00149167\n",
      "[340]\tcv_agg's multi_logloss: 0.0283158 + 0.00148353\n",
      "[360]\tcv_agg's multi_logloss: 0.0279884 + 0.00149698\n",
      "[380]\tcv_agg's multi_logloss: 0.0277404 + 0.00148029\n",
      "[400]\tcv_agg's multi_logloss: 0.0275109 + 0.00146352\n",
      "[420]\tcv_agg's multi_logloss: 0.0273459 + 0.00149387\n",
      "[440]\tcv_agg's multi_logloss: 0.0271956 + 0.00150549\n",
      "[460]\tcv_agg's multi_logloss: 0.0270856 + 0.00151948\n",
      "[480]\tcv_agg's multi_logloss: 0.026977 + 0.0015429\n",
      "[500]\tcv_agg's multi_logloss: 0.026912 + 0.00156015\n",
      "[520]\tcv_agg's multi_logloss: 0.02681 + 0.00156283\n",
      "[540]\tcv_agg's multi_logloss: 0.0267689 + 0.00155974\n",
      "[560]\tcv_agg's multi_logloss: 0.0267335 + 0.00157347\n",
      "[580]\tcv_agg's multi_logloss: 0.0267196 + 0.00156641\n",
      "[600]\tcv_agg's multi_logloss: 0.0266997 + 0.00158107\n",
      "[620]\tcv_agg's multi_logloss: 0.0266887 + 0.00159961\n",
      "[640]\tcv_agg's multi_logloss: 0.0266956 + 0.00160928\n",
      "[660]\tcv_agg's multi_logloss: 0.0267047 + 0.00160252\n",
      "最优结果0.026682194021648432\n",
      "总loss[0.04735130628453371, 0.054884275472127694, 0.03781991717105289, 0.04323592371048647, 0.04230347054874013, 0.05716380758019011, 0.039876161644722685, 0.026682194021648432]\n",
      "[10]\ttraining's multi_logloss: 0.941514\n",
      "[20]\ttraining's multi_logloss: 0.570617\n",
      "[30]\ttraining's multi_logloss: 0.365135\n",
      "[40]\ttraining's multi_logloss: 0.245288\n",
      "[50]\ttraining's multi_logloss: 0.17249\n",
      "[60]\ttraining's multi_logloss: 0.126613\n",
      "[70]\ttraining's multi_logloss: 0.0974499\n",
      "[80]\ttraining's multi_logloss: 0.0781821\n",
      "[90]\ttraining's multi_logloss: 0.065199\n",
      "[100]\ttraining's multi_logloss: 0.0559693\n",
      "[110]\ttraining's multi_logloss: 0.0491771\n",
      "[120]\ttraining's multi_logloss: 0.04407\n",
      "[130]\ttraining's multi_logloss: 0.0400512\n",
      "[140]\ttraining's multi_logloss: 0.0368319\n",
      "[150]\ttraining's multi_logloss: 0.0341327\n",
      "[160]\ttraining's multi_logloss: 0.0318846\n",
      "[170]\ttraining's multi_logloss: 0.0299442\n",
      "[180]\ttraining's multi_logloss: 0.0281517\n",
      "[190]\ttraining's multi_logloss: 0.026713\n",
      "[200]\ttraining's multi_logloss: 0.0253908\n",
      "[210]\ttraining's multi_logloss: 0.0242148\n",
      "[220]\ttraining's multi_logloss: 0.0231518\n",
      "[230]\ttraining's multi_logloss: 0.0221782\n",
      "[240]\ttraining's multi_logloss: 0.0213445\n",
      "[250]\ttraining's multi_logloss: 0.0205649\n",
      "[260]\ttraining's multi_logloss: 0.0198569\n",
      "[270]\ttraining's multi_logloss: 0.0191834\n",
      "[280]\ttraining's multi_logloss: 0.0185478\n",
      "[290]\ttraining's multi_logloss: 0.0179283\n",
      "[300]\ttraining's multi_logloss: 0.0173565\n",
      "[310]\ttraining's multi_logloss: 0.0168181\n",
      "[320]\ttraining's multi_logloss: 0.0163207\n",
      "[330]\ttraining's multi_logloss: 0.0158201\n",
      "[340]\ttraining's multi_logloss: 0.0153758\n",
      "[350]\ttraining's multi_logloss: 0.0149302\n",
      "[360]\ttraining's multi_logloss: 0.0145276\n",
      "[370]\ttraining's multi_logloss: 0.0141394\n",
      "[380]\ttraining's multi_logloss: 0.0137641\n",
      "[390]\ttraining's multi_logloss: 0.0134012\n",
      "[400]\ttraining's multi_logloss: 0.0130664\n",
      "[410]\ttraining's multi_logloss: 0.0127355\n",
      "[420]\ttraining's multi_logloss: 0.0124231\n",
      "[430]\ttraining's multi_logloss: 0.0121329\n",
      "[440]\ttraining's multi_logloss: 0.0118385\n",
      "[450]\ttraining's multi_logloss: 0.0115425\n",
      "[460]\ttraining's multi_logloss: 0.0112603\n",
      "[470]\ttraining's multi_logloss: 0.0109999\n",
      "[480]\ttraining's multi_logloss: 0.010744\n",
      "[490]\ttraining's multi_logloss: 0.0105075\n",
      "[500]\ttraining's multi_logloss: 0.0102827\n",
      "[510]\ttraining's multi_logloss: 0.0100611\n",
      "[520]\ttraining's multi_logloss: 0.00984599\n",
      "[530]\ttraining's multi_logloss: 0.00963661\n",
      "[540]\ttraining's multi_logloss: 0.00943978\n",
      "[550]\ttraining's multi_logloss: 0.00924733\n",
      "[560]\ttraining's multi_logloss: 0.00906036\n",
      "[570]\ttraining's multi_logloss: 0.00887653\n",
      "[580]\ttraining's multi_logloss: 0.00870144\n",
      "[590]\ttraining's multi_logloss: 0.00851977\n",
      "[600]\ttraining's multi_logloss: 0.00835144\n",
      "[610]\ttraining's multi_logloss: 0.00818699\n",
      "[620]\ttraining's multi_logloss: 0.00802313\n",
      "[630]\ttraining's multi_logloss: 0.00786621\n",
      "[640]\ttraining's multi_logloss: 0.00771356\n",
      "[650]\ttraining's multi_logloss: 0.00757436\n",
      "[660]\ttraining's multi_logloss: 0.00743181\n",
      "[670]\ttraining's multi_logloss: 0.00729611\n",
      "[680]\ttraining's multi_logloss: 0.00715894\n",
      "[690]\ttraining's multi_logloss: 0.00703005\n",
      "***********第8次抽样***********\n",
      "负样本抽样数量：\n",
      "(44318, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.581835 + 0.00172849\n",
      "[40]\tcv_agg's multi_logloss: 0.25768 + 0.00240443\n",
      "[60]\tcv_agg's multi_logloss: 0.138373 + 0.00271813\n",
      "[80]\tcv_agg's multi_logloss: 0.0898017 + 0.00293984\n",
      "[100]\tcv_agg's multi_logloss: 0.0674794 + 0.0031447\n",
      "[120]\tcv_agg's multi_logloss: 0.0556936 + 0.00331809\n",
      "[140]\tcv_agg's multi_logloss: 0.0487812 + 0.0033232\n",
      "[160]\tcv_agg's multi_logloss: 0.044371 + 0.0034836\n",
      "[180]\tcv_agg's multi_logloss: 0.0413929 + 0.0035018\n",
      "[200]\tcv_agg's multi_logloss: 0.0392386 + 0.00360105\n",
      "[220]\tcv_agg's multi_logloss: 0.0376759 + 0.00365747\n",
      "[240]\tcv_agg's multi_logloss: 0.0364655 + 0.00378757\n",
      "[260]\tcv_agg's multi_logloss: 0.0355466 + 0.00387818\n",
      "[280]\tcv_agg's multi_logloss: 0.0348012 + 0.003991\n",
      "[300]\tcv_agg's multi_logloss: 0.0342169 + 0.00409254\n",
      "[320]\tcv_agg's multi_logloss: 0.0337287 + 0.00418831\n",
      "[340]\tcv_agg's multi_logloss: 0.0332991 + 0.00426195\n",
      "[360]\tcv_agg's multi_logloss: 0.0329282 + 0.00435503\n",
      "[380]\tcv_agg's multi_logloss: 0.032624 + 0.00442913\n",
      "[400]\tcv_agg's multi_logloss: 0.0323755 + 0.00449097\n",
      "[420]\tcv_agg's multi_logloss: 0.0321734 + 0.00455198\n",
      "[440]\tcv_agg's multi_logloss: 0.0320154 + 0.00465665\n",
      "[460]\tcv_agg's multi_logloss: 0.0318922 + 0.00475772\n",
      "[480]\tcv_agg's multi_logloss: 0.0317767 + 0.00486358\n",
      "[500]\tcv_agg's multi_logloss: 0.0316991 + 0.00493964\n",
      "[520]\tcv_agg's multi_logloss: 0.0316575 + 0.00503721\n",
      "[540]\tcv_agg's multi_logloss: 0.031617 + 0.00511169\n",
      "[560]\tcv_agg's multi_logloss: 0.0315883 + 0.00518496\n",
      "[580]\tcv_agg's multi_logloss: 0.0315773 + 0.0052497\n",
      "[600]\tcv_agg's multi_logloss: 0.0316066 + 0.00532668\n",
      "[620]\tcv_agg's multi_logloss: 0.0316015 + 0.00543298\n",
      "最优结果0.031569233860371755\n",
      "总loss[0.04735130628453371, 0.054884275472127694, 0.03781991717105289, 0.04323592371048647, 0.04230347054874013, 0.05716380758019011, 0.039876161644722685, 0.026682194021648432, 0.031569233860371755]\n",
      "[10]\ttraining's multi_logloss: 0.949453\n",
      "[20]\ttraining's multi_logloss: 0.580357\n",
      "[30]\ttraining's multi_logloss: 0.375407\n",
      "[40]\ttraining's multi_logloss: 0.255585\n",
      "[50]\ttraining's multi_logloss: 0.181893\n",
      "[60]\ttraining's multi_logloss: 0.135545\n",
      "[70]\ttraining's multi_logloss: 0.10583\n",
      "[80]\ttraining's multi_logloss: 0.0858812\n",
      "[90]\ttraining's multi_logloss: 0.0723833\n",
      "[100]\ttraining's multi_logloss: 0.0626322\n",
      "[110]\ttraining's multi_logloss: 0.0553769\n",
      "[120]\ttraining's multi_logloss: 0.0499465\n",
      "[130]\ttraining's multi_logloss: 0.0455814\n",
      "[140]\ttraining's multi_logloss: 0.0420607\n",
      "[150]\ttraining's multi_logloss: 0.0391026\n",
      "[160]\ttraining's multi_logloss: 0.0365255\n",
      "[170]\ttraining's multi_logloss: 0.0343554\n",
      "[180]\ttraining's multi_logloss: 0.0324367\n",
      "[190]\ttraining's multi_logloss: 0.0308013\n",
      "[200]\ttraining's multi_logloss: 0.0292665\n",
      "[210]\ttraining's multi_logloss: 0.0279271\n",
      "[220]\ttraining's multi_logloss: 0.0267555\n",
      "[230]\ttraining's multi_logloss: 0.0256589\n",
      "[240]\ttraining's multi_logloss: 0.0247005\n",
      "[250]\ttraining's multi_logloss: 0.0237376\n",
      "[260]\ttraining's multi_logloss: 0.0228654\n",
      "[270]\ttraining's multi_logloss: 0.0220938\n",
      "[280]\ttraining's multi_logloss: 0.0213379\n",
      "[290]\ttraining's multi_logloss: 0.0206516\n",
      "[300]\ttraining's multi_logloss: 0.019986\n",
      "[310]\ttraining's multi_logloss: 0.0193729\n",
      "[320]\ttraining's multi_logloss: 0.0187943\n",
      "[330]\ttraining's multi_logloss: 0.0182394\n",
      "[340]\ttraining's multi_logloss: 0.0177376\n",
      "[350]\ttraining's multi_logloss: 0.0172314\n",
      "[360]\ttraining's multi_logloss: 0.0167686\n",
      "[370]\ttraining's multi_logloss: 0.0163187\n",
      "[380]\ttraining's multi_logloss: 0.015903\n",
      "[390]\ttraining's multi_logloss: 0.015481\n",
      "[400]\ttraining's multi_logloss: 0.015061\n",
      "[410]\ttraining's multi_logloss: 0.0146799\n",
      "[420]\ttraining's multi_logloss: 0.0143199\n",
      "[430]\ttraining's multi_logloss: 0.0139696\n",
      "[440]\ttraining's multi_logloss: 0.0136251\n",
      "[450]\ttraining's multi_logloss: 0.0132834\n",
      "[460]\ttraining's multi_logloss: 0.0129827\n",
      "[470]\ttraining's multi_logloss: 0.0126941\n",
      "[480]\ttraining's multi_logloss: 0.0123983\n",
      "[490]\ttraining's multi_logloss: 0.0121237\n",
      "[500]\ttraining's multi_logloss: 0.0118489\n",
      "[510]\ttraining's multi_logloss: 0.0115949\n",
      "[520]\ttraining's multi_logloss: 0.0113488\n",
      "[530]\ttraining's multi_logloss: 0.0111067\n",
      "[540]\ttraining's multi_logloss: 0.0108647\n",
      "[550]\ttraining's multi_logloss: 0.01064\n",
      "[560]\ttraining's multi_logloss: 0.010414\n",
      "[570]\ttraining's multi_logloss: 0.0101972\n",
      "[580]\ttraining's multi_logloss: 0.00998732\n",
      "[590]\ttraining's multi_logloss: 0.00977569\n",
      "[600]\ttraining's multi_logloss: 0.0095849\n",
      "[610]\ttraining's multi_logloss: 0.00939137\n",
      "[620]\ttraining's multi_logloss: 0.00920166\n",
      "[630]\ttraining's multi_logloss: 0.00902649\n",
      "[640]\ttraining's multi_logloss: 0.0088525\n",
      "[650]\ttraining's multi_logloss: 0.00868659\n",
      "***********第9次抽样***********\n",
      "负样本抽样数量：\n",
      "(46689, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.579693 + 0.00237402\n",
      "[40]\tcv_agg's multi_logloss: 0.255199 + 0.00297627\n",
      "[60]\tcv_agg's multi_logloss: 0.136218 + 0.00310481\n",
      "[80]\tcv_agg's multi_logloss: 0.0878854 + 0.00322042\n",
      "[100]\tcv_agg's multi_logloss: 0.0657362 + 0.00303948\n",
      "[120]\tcv_agg's multi_logloss: 0.0542463 + 0.00282934\n",
      "[140]\tcv_agg's multi_logloss: 0.0474826 + 0.00261799\n",
      "[160]\tcv_agg's multi_logloss: 0.0431789 + 0.0025442\n",
      "[180]\tcv_agg's multi_logloss: 0.0402468 + 0.00245949\n",
      "[200]\tcv_agg's multi_logloss: 0.0381005 + 0.00239757\n",
      "[220]\tcv_agg's multi_logloss: 0.0364521 + 0.00241506\n",
      "[240]\tcv_agg's multi_logloss: 0.0352202 + 0.00239301\n",
      "[260]\tcv_agg's multi_logloss: 0.0342528 + 0.00231132\n",
      "[280]\tcv_agg's multi_logloss: 0.0333998 + 0.00224671\n",
      "[300]\tcv_agg's multi_logloss: 0.032739 + 0.00219728\n",
      "[320]\tcv_agg's multi_logloss: 0.0321753 + 0.00214218\n",
      "[340]\tcv_agg's multi_logloss: 0.0317258 + 0.00213104\n",
      "[360]\tcv_agg's multi_logloss: 0.031369 + 0.00209653\n",
      "[380]\tcv_agg's multi_logloss: 0.031046 + 0.00208766\n",
      "[400]\tcv_agg's multi_logloss: 0.0307969 + 0.00208714\n",
      "[420]\tcv_agg's multi_logloss: 0.0305559 + 0.0020484\n",
      "[440]\tcv_agg's multi_logloss: 0.0303745 + 0.00201634\n",
      "[460]\tcv_agg's multi_logloss: 0.0302281 + 0.00200428\n",
      "[480]\tcv_agg's multi_logloss: 0.0301074 + 0.00198946\n",
      "[500]\tcv_agg's multi_logloss: 0.0300207 + 0.00198299\n",
      "[520]\tcv_agg's multi_logloss: 0.0299485 + 0.00197993\n",
      "[540]\tcv_agg's multi_logloss: 0.0298704 + 0.0020103\n",
      "[560]\tcv_agg's multi_logloss: 0.0298172 + 0.00201356\n",
      "[580]\tcv_agg's multi_logloss: 0.0297716 + 0.002011\n",
      "[600]\tcv_agg's multi_logloss: 0.029782 + 0.00203788\n",
      "最优结果0.02977158357292824\n",
      "总loss[0.04735130628453371, 0.054884275472127694, 0.03781991717105289, 0.04323592371048647, 0.04230347054874013, 0.05716380758019011, 0.039876161644722685, 0.026682194021648432, 0.031569233860371755, 0.02977158357292824]\n",
      "[10]\ttraining's multi_logloss: 0.947495\n",
      "[20]\ttraining's multi_logloss: 0.578678\n",
      "[30]\ttraining's multi_logloss: 0.373118\n",
      "[40]\ttraining's multi_logloss: 0.253214\n",
      "[50]\ttraining's multi_logloss: 0.179817\n",
      "[60]\ttraining's multi_logloss: 0.133577\n",
      "[70]\ttraining's multi_logloss: 0.104094\n",
      "[80]\ttraining's multi_logloss: 0.0844109\n",
      "[90]\ttraining's multi_logloss: 0.0710648\n",
      "[100]\ttraining's multi_logloss: 0.0614123\n",
      "[110]\ttraining's multi_logloss: 0.0542615\n",
      "[120]\ttraining's multi_logloss: 0.0487937\n",
      "[130]\ttraining's multi_logloss: 0.0445481\n",
      "[140]\ttraining's multi_logloss: 0.0410987\n",
      "[150]\ttraining's multi_logloss: 0.0380768\n",
      "[160]\ttraining's multi_logloss: 0.0356119\n",
      "[170]\ttraining's multi_logloss: 0.033489\n",
      "[180]\ttraining's multi_logloss: 0.0316256\n",
      "[190]\ttraining's multi_logloss: 0.0300096\n",
      "[200]\ttraining's multi_logloss: 0.0285427\n",
      "[210]\ttraining's multi_logloss: 0.0272666\n",
      "[220]\ttraining's multi_logloss: 0.0261292\n",
      "[230]\ttraining's multi_logloss: 0.0250947\n",
      "[240]\ttraining's multi_logloss: 0.0241137\n",
      "[250]\ttraining's multi_logloss: 0.0232273\n",
      "[260]\ttraining's multi_logloss: 0.0224017\n",
      "[270]\ttraining's multi_logloss: 0.0216126\n",
      "[280]\ttraining's multi_logloss: 0.0209063\n",
      "[290]\ttraining's multi_logloss: 0.0202045\n",
      "[300]\ttraining's multi_logloss: 0.019562\n",
      "[310]\ttraining's multi_logloss: 0.0189549\n",
      "[320]\ttraining's multi_logloss: 0.0183984\n",
      "[330]\ttraining's multi_logloss: 0.0178572\n",
      "[340]\ttraining's multi_logloss: 0.0173477\n",
      "[350]\ttraining's multi_logloss: 0.0168773\n",
      "[360]\ttraining's multi_logloss: 0.0164062\n",
      "[370]\ttraining's multi_logloss: 0.0159677\n",
      "[380]\ttraining's multi_logloss: 0.015539\n",
      "[390]\ttraining's multi_logloss: 0.0151314\n",
      "[400]\ttraining's multi_logloss: 0.0147226\n",
      "[410]\ttraining's multi_logloss: 0.0143536\n",
      "[420]\ttraining's multi_logloss: 0.0140001\n",
      "[430]\ttraining's multi_logloss: 0.0136572\n",
      "[440]\ttraining's multi_logloss: 0.0133367\n",
      "[450]\ttraining's multi_logloss: 0.0130259\n",
      "[460]\ttraining's multi_logloss: 0.0127118\n",
      "[470]\ttraining's multi_logloss: 0.012408\n",
      "[480]\ttraining's multi_logloss: 0.0121225\n",
      "[490]\ttraining's multi_logloss: 0.0118405\n",
      "[500]\ttraining's multi_logloss: 0.0115771\n",
      "[510]\ttraining's multi_logloss: 0.0113177\n",
      "[520]\ttraining's multi_logloss: 0.0110733\n",
      "[530]\ttraining's multi_logloss: 0.0108236\n",
      "[540]\ttraining's multi_logloss: 0.0105881\n",
      "[550]\ttraining's multi_logloss: 0.0103632\n",
      "[560]\ttraining's multi_logloss: 0.0101476\n",
      "[570]\ttraining's multi_logloss: 0.0099447\n",
      "[580]\ttraining's multi_logloss: 0.00973894\n",
      "[590]\ttraining's multi_logloss: 0.0095372\n",
      "[600]\ttraining's multi_logloss: 0.00934525\n",
      "[610]\ttraining's multi_logloss: 0.00916033\n",
      "[620]\ttraining's multi_logloss: 0.00897732\n",
      "[630]\ttraining's multi_logloss: 0.00880681\n",
      "***********第10次抽样***********\n",
      "负样本抽样数量：\n",
      "(37847, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.590054 + 0.000928348\n",
      "[40]\tcv_agg's multi_logloss: 0.265822 + 0.0010762\n",
      "[60]\tcv_agg's multi_logloss: 0.14611 + 0.00115983\n",
      "[80]\tcv_agg's multi_logloss: 0.0966146 + 0.00120732\n",
      "[100]\tcv_agg's multi_logloss: 0.0734132 + 0.00127525\n",
      "[120]\tcv_agg's multi_logloss: 0.0611072 + 0.00152467\n",
      "[140]\tcv_agg's multi_logloss: 0.0537115 + 0.00180655\n",
      "[160]\tcv_agg's multi_logloss: 0.0489134 + 0.00199697\n",
      "[180]\tcv_agg's multi_logloss: 0.0457193 + 0.00222895\n",
      "[200]\tcv_agg's multi_logloss: 0.0434054 + 0.00241453\n",
      "[220]\tcv_agg's multi_logloss: 0.041609 + 0.00259683\n",
      "[240]\tcv_agg's multi_logloss: 0.04025 + 0.00275147\n",
      "[260]\tcv_agg's multi_logloss: 0.0391415 + 0.00281768\n",
      "[280]\tcv_agg's multi_logloss: 0.03827 + 0.0029337\n",
      "[300]\tcv_agg's multi_logloss: 0.0375958 + 0.00303576\n",
      "[320]\tcv_agg's multi_logloss: 0.0370623 + 0.00316941\n",
      "[340]\tcv_agg's multi_logloss: 0.036591 + 0.00328782\n",
      "[360]\tcv_agg's multi_logloss: 0.0361622 + 0.00345192\n",
      "[380]\tcv_agg's multi_logloss: 0.0358404 + 0.0035627\n",
      "[400]\tcv_agg's multi_logloss: 0.0355725 + 0.00372386\n",
      "[420]\tcv_agg's multi_logloss: 0.0353531 + 0.00381568\n",
      "[440]\tcv_agg's multi_logloss: 0.035149 + 0.00393118\n",
      "[460]\tcv_agg's multi_logloss: 0.0349879 + 0.0040221\n",
      "[480]\tcv_agg's multi_logloss: 0.0349117 + 0.00413898\n",
      "[500]\tcv_agg's multi_logloss: 0.0348278 + 0.00426389\n",
      "[520]\tcv_agg's multi_logloss: 0.0347533 + 0.00438965\n",
      "[540]\tcv_agg's multi_logloss: 0.0346941 + 0.00445823\n",
      "[560]\tcv_agg's multi_logloss: 0.0346564 + 0.00458502\n",
      "[580]\tcv_agg's multi_logloss: 0.0346606 + 0.0046997\n",
      "[600]\tcv_agg's multi_logloss: 0.0346684 + 0.00477051\n",
      "[620]\tcv_agg's multi_logloss: 0.0346906 + 0.00485453\n",
      "最优结果0.03464358245542504\n",
      "总loss[0.04735130628453371, 0.054884275472127694, 0.03781991717105289, 0.04323592371048647, 0.04230347054874013, 0.05716380758019011, 0.039876161644722685, 0.026682194021648432, 0.031569233860371755, 0.02977158357292824, 0.03464358245542504]\n",
      "[10]\ttraining's multi_logloss: 0.955947\n",
      "[20]\ttraining's multi_logloss: 0.588308\n",
      "[30]\ttraining's multi_logloss: 0.383525\n",
      "[40]\ttraining's multi_logloss: 0.263178\n",
      "[50]\ttraining's multi_logloss: 0.189275\n",
      "[60]\ttraining's multi_logloss: 0.142384\n",
      "[70]\ttraining's multi_logloss: 0.112132\n",
      "[80]\ttraining's multi_logloss: 0.0918698\n",
      "[90]\ttraining's multi_logloss: 0.0778561\n",
      "[100]\ttraining's multi_logloss: 0.067853\n",
      "[110]\ttraining's multi_logloss: 0.0602693\n",
      "[120]\ttraining's multi_logloss: 0.054543\n",
      "[130]\ttraining's multi_logloss: 0.0499309\n",
      "[140]\ttraining's multi_logloss: 0.0461382\n",
      "[150]\ttraining's multi_logloss: 0.0429213\n",
      "[160]\ttraining's multi_logloss: 0.0402154\n",
      "[170]\ttraining's multi_logloss: 0.0378964\n",
      "[180]\ttraining's multi_logloss: 0.035804\n",
      "[190]\ttraining's multi_logloss: 0.0339788\n",
      "[200]\ttraining's multi_logloss: 0.0323709\n",
      "[210]\ttraining's multi_logloss: 0.0309282\n",
      "[220]\ttraining's multi_logloss: 0.0296159\n",
      "[230]\ttraining's multi_logloss: 0.0284175\n",
      "[240]\ttraining's multi_logloss: 0.0273307\n",
      "[250]\ttraining's multi_logloss: 0.0263432\n",
      "[260]\ttraining's multi_logloss: 0.0254275\n",
      "[270]\ttraining's multi_logloss: 0.0245619\n",
      "[280]\ttraining's multi_logloss: 0.0237196\n",
      "[290]\ttraining's multi_logloss: 0.0229819\n",
      "[300]\ttraining's multi_logloss: 0.0222471\n",
      "[310]\ttraining's multi_logloss: 0.0215839\n",
      "[320]\ttraining's multi_logloss: 0.0209117\n",
      "[330]\ttraining's multi_logloss: 0.0202868\n",
      "[340]\ttraining's multi_logloss: 0.0196985\n",
      "[350]\ttraining's multi_logloss: 0.0191173\n",
      "[360]\ttraining's multi_logloss: 0.0185731\n",
      "[370]\ttraining's multi_logloss: 0.0180592\n",
      "[380]\ttraining's multi_logloss: 0.0175729\n",
      "[390]\ttraining's multi_logloss: 0.0171183\n",
      "[400]\ttraining's multi_logloss: 0.0166644\n",
      "[410]\ttraining's multi_logloss: 0.016244\n",
      "[420]\ttraining's multi_logloss: 0.0158449\n",
      "[430]\ttraining's multi_logloss: 0.0154656\n",
      "[440]\ttraining's multi_logloss: 0.015086\n",
      "[450]\ttraining's multi_logloss: 0.0147314\n",
      "[460]\ttraining's multi_logloss: 0.0143865\n",
      "[470]\ttraining's multi_logloss: 0.0140525\n",
      "[480]\ttraining's multi_logloss: 0.0137369\n",
      "[490]\ttraining's multi_logloss: 0.0134137\n",
      "[500]\ttraining's multi_logloss: 0.0131103\n",
      "[510]\ttraining's multi_logloss: 0.0128263\n",
      "[520]\ttraining's multi_logloss: 0.0125394\n",
      "[530]\ttraining's multi_logloss: 0.0122694\n",
      "[540]\ttraining's multi_logloss: 0.0119999\n",
      "[550]\ttraining's multi_logloss: 0.0117577\n",
      "[560]\ttraining's multi_logloss: 0.0115138\n",
      "[570]\ttraining's multi_logloss: 0.0112783\n",
      "[580]\ttraining's multi_logloss: 0.0110493\n",
      "[590]\ttraining's multi_logloss: 0.010817\n",
      "[600]\ttraining's multi_logloss: 0.010593\n",
      "[610]\ttraining's multi_logloss: 0.0103817\n",
      "[620]\ttraining's multi_logloss: 0.0101725\n",
      "[630]\ttraining's multi_logloss: 0.00996932\n",
      "[640]\ttraining's multi_logloss: 0.00978294\n",
      "[650]\ttraining's multi_logloss: 0.00959004\n",
      "***********第11次抽样***********\n",
      "负样本抽样数量：\n",
      "(20719, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.624931 + 0.00281562\n",
      "[40]\tcv_agg's multi_logloss: 0.302628 + 0.00494168\n",
      "[60]\tcv_agg's multi_logloss: 0.179775 + 0.00598192\n",
      "[80]\tcv_agg's multi_logloss: 0.126677 + 0.00649153\n",
      "[100]\tcv_agg's multi_logloss: 0.10098 + 0.00650951\n",
      "[120]\tcv_agg's multi_logloss: 0.0862857 + 0.00675642\n",
      "[140]\tcv_agg's multi_logloss: 0.0773382 + 0.00688599\n",
      "[160]\tcv_agg's multi_logloss: 0.0715831 + 0.0070124\n",
      "[180]\tcv_agg's multi_logloss: 0.0675233 + 0.0069546\n",
      "[200]\tcv_agg's multi_logloss: 0.0646664 + 0.00699579\n",
      "[220]\tcv_agg's multi_logloss: 0.0624935 + 0.00704855\n",
      "[240]\tcv_agg's multi_logloss: 0.0608298 + 0.0069755\n",
      "[260]\tcv_agg's multi_logloss: 0.0595473 + 0.00693898\n",
      "[280]\tcv_agg's multi_logloss: 0.0585371 + 0.00696962\n",
      "[300]\tcv_agg's multi_logloss: 0.0577187 + 0.00702456\n",
      "[320]\tcv_agg's multi_logloss: 0.0569878 + 0.00705351\n",
      "[340]\tcv_agg's multi_logloss: 0.0563324 + 0.00704822\n",
      "[360]\tcv_agg's multi_logloss: 0.05582 + 0.00703613\n",
      "[380]\tcv_agg's multi_logloss: 0.0553816 + 0.00702205\n",
      "[400]\tcv_agg's multi_logloss: 0.0550304 + 0.00705237\n",
      "[420]\tcv_agg's multi_logloss: 0.054705 + 0.00702617\n",
      "[440]\tcv_agg's multi_logloss: 0.0545038 + 0.00698624\n",
      "[460]\tcv_agg's multi_logloss: 0.0543596 + 0.00695579\n",
      "[480]\tcv_agg's multi_logloss: 0.0543119 + 0.0069642\n",
      "[500]\tcv_agg's multi_logloss: 0.0541992 + 0.00701985\n",
      "[520]\tcv_agg's multi_logloss: 0.054136 + 0.00708348\n",
      "[540]\tcv_agg's multi_logloss: 0.0541567 + 0.00706934\n",
      "最优结果0.054121566714876626\n",
      "总loss[0.04735130628453371, 0.054884275472127694, 0.03781991717105289, 0.04323592371048647, 0.04230347054874013, 0.05716380758019011, 0.039876161644722685, 0.026682194021648432, 0.031569233860371755, 0.02977158357292824, 0.03464358245542504, 0.054121566714876626]\n",
      "[10]\ttraining's multi_logloss: 0.984307\n",
      "[20]\ttraining's multi_logloss: 0.62259\n",
      "[30]\ttraining's multi_logloss: 0.42033\n",
      "[40]\ttraining's multi_logloss: 0.298826\n",
      "[50]\ttraining's multi_logloss: 0.223088\n",
      "[60]\ttraining's multi_logloss: 0.174115\n",
      "[70]\ttraining's multi_logloss: 0.141954\n",
      "[80]\ttraining's multi_logloss: 0.119672\n",
      "[90]\ttraining's multi_logloss: 0.103951\n",
      "[100]\ttraining's multi_logloss: 0.0924781\n",
      "[110]\ttraining's multi_logloss: 0.0834647\n",
      "[120]\ttraining's multi_logloss: 0.0763025\n",
      "[130]\ttraining's multi_logloss: 0.0705043\n",
      "[140]\ttraining's multi_logloss: 0.0656313\n",
      "[150]\ttraining's multi_logloss: 0.0613892\n",
      "[160]\ttraining's multi_logloss: 0.0578524\n",
      "[170]\ttraining's multi_logloss: 0.0547088\n",
      "[180]\ttraining's multi_logloss: 0.0519458\n",
      "[190]\ttraining's multi_logloss: 0.0494244\n",
      "[200]\ttraining's multi_logloss: 0.0471785\n",
      "[210]\ttraining's multi_logloss: 0.0451563\n",
      "[220]\ttraining's multi_logloss: 0.0433867\n",
      "[230]\ttraining's multi_logloss: 0.0417198\n",
      "[240]\ttraining's multi_logloss: 0.0401746\n",
      "[250]\ttraining's multi_logloss: 0.0387214\n",
      "[260]\ttraining's multi_logloss: 0.0373798\n",
      "[270]\ttraining's multi_logloss: 0.0361124\n",
      "[280]\ttraining's multi_logloss: 0.0349063\n",
      "[290]\ttraining's multi_logloss: 0.0337502\n",
      "[300]\ttraining's multi_logloss: 0.0327017\n",
      "[310]\ttraining's multi_logloss: 0.0317123\n",
      "[320]\ttraining's multi_logloss: 0.0307653\n",
      "[330]\ttraining's multi_logloss: 0.0298549\n",
      "[340]\ttraining's multi_logloss: 0.028987\n",
      "[350]\ttraining's multi_logloss: 0.0281606\n",
      "[360]\ttraining's multi_logloss: 0.0273681\n",
      "[370]\ttraining's multi_logloss: 0.0265861\n",
      "[380]\ttraining's multi_logloss: 0.0258516\n",
      "[390]\ttraining's multi_logloss: 0.0251729\n",
      "[400]\ttraining's multi_logloss: 0.0245049\n",
      "[410]\ttraining's multi_logloss: 0.0238739\n",
      "[420]\ttraining's multi_logloss: 0.0232633\n",
      "[430]\ttraining's multi_logloss: 0.0226727\n",
      "[440]\ttraining's multi_logloss: 0.0221174\n",
      "[450]\ttraining's multi_logloss: 0.021557\n",
      "[460]\ttraining's multi_logloss: 0.0210471\n",
      "[470]\ttraining's multi_logloss: 0.0205417\n",
      "[480]\ttraining's multi_logloss: 0.0200544\n",
      "[490]\ttraining's multi_logloss: 0.019582\n",
      "[500]\ttraining's multi_logloss: 0.0191386\n",
      "[510]\ttraining's multi_logloss: 0.0186913\n",
      "[520]\ttraining's multi_logloss: 0.0182648\n",
      "[530]\ttraining's multi_logloss: 0.0178284\n",
      "[540]\ttraining's multi_logloss: 0.0173992\n",
      "[550]\ttraining's multi_logloss: 0.0170068\n",
      "[560]\ttraining's multi_logloss: 0.0165726\n",
      "[570]\ttraining's multi_logloss: 0.0162123\n",
      "***********第12次抽样***********\n",
      "负样本抽样数量：\n",
      "(31320, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.601658 + 0.00157628\n",
      "[40]\tcv_agg's multi_logloss: 0.277817 + 0.00238632\n",
      "[60]\tcv_agg's multi_logloss: 0.157149 + 0.00265713\n",
      "[80]\tcv_agg's multi_logloss: 0.106502 + 0.00261958\n",
      "[100]\tcv_agg's multi_logloss: 0.08256 + 0.0026459\n",
      "[120]\tcv_agg's multi_logloss: 0.0695888 + 0.00268397\n",
      "[140]\tcv_agg's multi_logloss: 0.0614604 + 0.00247671\n",
      "[160]\tcv_agg's multi_logloss: 0.0562948 + 0.00245498\n",
      "[180]\tcv_agg's multi_logloss: 0.0526961 + 0.00254241\n",
      "[200]\tcv_agg's multi_logloss: 0.0500862 + 0.00249917\n",
      "[220]\tcv_agg's multi_logloss: 0.0481266 + 0.00248577\n",
      "[240]\tcv_agg's multi_logloss: 0.0465947 + 0.00251094\n",
      "[260]\tcv_agg's multi_logloss: 0.0453876 + 0.00254446\n",
      "[280]\tcv_agg's multi_logloss: 0.0443513 + 0.00260527\n",
      "[300]\tcv_agg's multi_logloss: 0.0435446 + 0.00262093\n",
      "[320]\tcv_agg's multi_logloss: 0.0428678 + 0.00262647\n",
      "[340]\tcv_agg's multi_logloss: 0.042268 + 0.00266774\n",
      "[360]\tcv_agg's multi_logloss: 0.0418149 + 0.0026469\n",
      "[380]\tcv_agg's multi_logloss: 0.041444 + 0.00267117\n",
      "[400]\tcv_agg's multi_logloss: 0.0411262 + 0.00267529\n",
      "[420]\tcv_agg's multi_logloss: 0.0408467 + 0.0026636\n",
      "[440]\tcv_agg's multi_logloss: 0.0406376 + 0.00267831\n",
      "[460]\tcv_agg's multi_logloss: 0.0404443 + 0.00272174\n",
      "[480]\tcv_agg's multi_logloss: 0.0403225 + 0.00276002\n",
      "[500]\tcv_agg's multi_logloss: 0.0401892 + 0.00277026\n",
      "[520]\tcv_agg's multi_logloss: 0.0400833 + 0.00280299\n",
      "[540]\tcv_agg's multi_logloss: 0.0400183 + 0.00283181\n",
      "[560]\tcv_agg's multi_logloss: 0.0400135 + 0.00286839\n",
      "[580]\tcv_agg's multi_logloss: 0.0399879 + 0.00287345\n",
      "[600]\tcv_agg's multi_logloss: 0.0399606 + 0.00288453\n",
      "[620]\tcv_agg's multi_logloss: 0.0399649 + 0.0029195\n",
      "[640]\tcv_agg's multi_logloss: 0.0399711 + 0.00291832\n",
      "[660]\tcv_agg's multi_logloss: 0.0400019 + 0.00294409\n",
      "最优结果0.03994100666800342\n",
      "总loss[0.04735130628453371, 0.054884275472127694, 0.03781991717105289, 0.04323592371048647, 0.04230347054874013, 0.05716380758019011, 0.039876161644722685, 0.026682194021648432, 0.031569233860371755, 0.02977158357292824, 0.03464358245542504, 0.054121566714876626, 0.03994100666800342]\n",
      "[10]\ttraining's multi_logloss: 0.965193\n",
      "[20]\ttraining's multi_logloss: 0.599835\n",
      "[30]\ttraining's multi_logloss: 0.395907\n",
      "[40]\ttraining's multi_logloss: 0.275248\n",
      "[50]\ttraining's multi_logloss: 0.201184\n",
      "[60]\ttraining's multi_logloss: 0.153537\n",
      "[70]\ttraining's multi_logloss: 0.122392\n",
      "[80]\ttraining's multi_logloss: 0.101479\n",
      "[90]\ttraining's multi_logloss: 0.0866646\n",
      "[100]\ttraining's multi_logloss: 0.0760681\n",
      "[110]\ttraining's multi_logloss: 0.0681604\n",
      "[120]\ttraining's multi_logloss: 0.0619015\n",
      "[130]\ttraining's multi_logloss: 0.0569451\n",
      "[140]\ttraining's multi_logloss: 0.0527321\n",
      "[150]\ttraining's multi_logloss: 0.0492167\n",
      "[160]\ttraining's multi_logloss: 0.0462396\n",
      "[170]\ttraining's multi_logloss: 0.0436464\n",
      "[180]\ttraining's multi_logloss: 0.0413639\n",
      "[190]\ttraining's multi_logloss: 0.0393353\n",
      "[200]\ttraining's multi_logloss: 0.0374684\n",
      "[210]\ttraining's multi_logloss: 0.0358289\n",
      "[220]\ttraining's multi_logloss: 0.0344001\n",
      "[230]\ttraining's multi_logloss: 0.0330498\n",
      "[240]\ttraining's multi_logloss: 0.0317849\n",
      "[250]\ttraining's multi_logloss: 0.030639\n",
      "[260]\ttraining's multi_logloss: 0.029529\n",
      "[270]\ttraining's multi_logloss: 0.028489\n",
      "[280]\ttraining's multi_logloss: 0.0275504\n",
      "[290]\ttraining's multi_logloss: 0.0266717\n",
      "[300]\ttraining's multi_logloss: 0.0258396\n",
      "[310]\ttraining's multi_logloss: 0.0250448\n",
      "[320]\ttraining's multi_logloss: 0.0242848\n",
      "[330]\ttraining's multi_logloss: 0.0235319\n",
      "[340]\ttraining's multi_logloss: 0.0228204\n",
      "[350]\ttraining's multi_logloss: 0.0221691\n",
      "[360]\ttraining's multi_logloss: 0.0215379\n",
      "[370]\ttraining's multi_logloss: 0.020946\n",
      "[380]\ttraining's multi_logloss: 0.0204002\n",
      "[390]\ttraining's multi_logloss: 0.0198635\n",
      "[400]\ttraining's multi_logloss: 0.0193436\n",
      "[410]\ttraining's multi_logloss: 0.0188548\n",
      "[420]\ttraining's multi_logloss: 0.018386\n",
      "[430]\ttraining's multi_logloss: 0.0179279\n",
      "[440]\ttraining's multi_logloss: 0.0174844\n",
      "[450]\ttraining's multi_logloss: 0.0170704\n",
      "[460]\ttraining's multi_logloss: 0.0166329\n",
      "[470]\ttraining's multi_logloss: 0.016243\n",
      "[480]\ttraining's multi_logloss: 0.0158559\n",
      "[490]\ttraining's multi_logloss: 0.0155065\n",
      "[500]\ttraining's multi_logloss: 0.0151592\n",
      "[510]\ttraining's multi_logloss: 0.0148272\n",
      "[520]\ttraining's multi_logloss: 0.0144853\n",
      "[530]\ttraining's multi_logloss: 0.0141727\n",
      "[540]\ttraining's multi_logloss: 0.01386\n",
      "[550]\ttraining's multi_logloss: 0.0135499\n",
      "[560]\ttraining's multi_logloss: 0.0132696\n",
      "[570]\ttraining's multi_logloss: 0.0129998\n",
      "[580]\ttraining's multi_logloss: 0.0127262\n",
      "[590]\ttraining's multi_logloss: 0.0124551\n",
      "[600]\ttraining's multi_logloss: 0.0122042\n",
      "[610]\ttraining's multi_logloss: 0.0119496\n",
      "[620]\ttraining's multi_logloss: 0.0117089\n",
      "[630]\ttraining's multi_logloss: 0.011465\n",
      "[640]\ttraining's multi_logloss: 0.0112353\n",
      "[650]\ttraining's multi_logloss: 0.0110184\n",
      "[660]\ttraining's multi_logloss: 0.0107693\n",
      "[670]\ttraining's multi_logloss: 0.0105545\n",
      "[680]\ttraining's multi_logloss: 0.010356\n",
      "[690]\ttraining's multi_logloss: 0.0101506\n",
      "***********第13次抽样***********\n",
      "负样本抽样数量：\n",
      "(19207, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.630292 + 0.00191072\n",
      "[40]\tcv_agg's multi_logloss: 0.30847 + 0.00325423\n",
      "[60]\tcv_agg's multi_logloss: 0.185027 + 0.00383701\n",
      "[80]\tcv_agg's multi_logloss: 0.131459 + 0.00414559\n",
      "[100]\tcv_agg's multi_logloss: 0.105452 + 0.00460689\n",
      "[120]\tcv_agg's multi_logloss: 0.0908735 + 0.00475795\n",
      "[140]\tcv_agg's multi_logloss: 0.0818224 + 0.005082\n",
      "[160]\tcv_agg's multi_logloss: 0.0757661 + 0.00514975\n",
      "[180]\tcv_agg's multi_logloss: 0.0714109 + 0.00540184\n",
      "[200]\tcv_agg's multi_logloss: 0.0682287 + 0.00565528\n",
      "[220]\tcv_agg's multi_logloss: 0.0659442 + 0.00583816\n",
      "[240]\tcv_agg's multi_logloss: 0.0641962 + 0.00595939\n",
      "[260]\tcv_agg's multi_logloss: 0.0628086 + 0.00606926\n",
      "[280]\tcv_agg's multi_logloss: 0.0616159 + 0.00616663\n",
      "[300]\tcv_agg's multi_logloss: 0.0606337 + 0.00623545\n",
      "[320]\tcv_agg's multi_logloss: 0.0598192 + 0.00629132\n",
      "[340]\tcv_agg's multi_logloss: 0.0592344 + 0.00639568\n",
      "[360]\tcv_agg's multi_logloss: 0.0587971 + 0.00646936\n",
      "[380]\tcv_agg's multi_logloss: 0.0583443 + 0.0065779\n",
      "[400]\tcv_agg's multi_logloss: 0.0580821 + 0.00664918\n",
      "[420]\tcv_agg's multi_logloss: 0.0577747 + 0.00671983\n",
      "[440]\tcv_agg's multi_logloss: 0.0575664 + 0.00678088\n",
      "[460]\tcv_agg's multi_logloss: 0.0574685 + 0.00681508\n",
      "[480]\tcv_agg's multi_logloss: 0.0573862 + 0.00687932\n",
      "[500]\tcv_agg's multi_logloss: 0.0573458 + 0.00696857\n",
      "[520]\tcv_agg's multi_logloss: 0.0573039 + 0.00703291\n",
      "[540]\tcv_agg's multi_logloss: 0.0573453 + 0.00709914\n",
      "最优结果0.05727697364366343\n",
      "总loss[0.04735130628453371, 0.054884275472127694, 0.03781991717105289, 0.04323592371048647, 0.04230347054874013, 0.05716380758019011, 0.039876161644722685, 0.026682194021648432, 0.031569233860371755, 0.02977158357292824, 0.03464358245542504, 0.054121566714876626, 0.03994100666800342, 0.05727697364366343]\n",
      "[10]\ttraining's multi_logloss: 0.98849\n",
      "[20]\ttraining's multi_logloss: 0.62788\n",
      "[30]\ttraining's multi_logloss: 0.425885\n",
      "[40]\ttraining's multi_logloss: 0.304347\n",
      "[50]\ttraining's multi_logloss: 0.22818\n",
      "[60]\ttraining's multi_logloss: 0.178968\n",
      "[70]\ttraining's multi_logloss: 0.146442\n",
      "[80]\ttraining's multi_logloss: 0.123637\n",
      "[90]\ttraining's multi_logloss: 0.107763\n",
      "[100]\ttraining's multi_logloss: 0.095923\n",
      "[110]\ttraining's multi_logloss: 0.0867599\n",
      "[120]\ttraining's multi_logloss: 0.0794085\n",
      "[130]\ttraining's multi_logloss: 0.0734028\n",
      "[140]\ttraining's multi_logloss: 0.06827\n",
      "[150]\ttraining's multi_logloss: 0.0638919\n",
      "[160]\ttraining's multi_logloss: 0.0602604\n",
      "[170]\ttraining's multi_logloss: 0.0569992\n",
      "[180]\ttraining's multi_logloss: 0.0540613\n",
      "[190]\ttraining's multi_logloss: 0.0515135\n",
      "[200]\ttraining's multi_logloss: 0.049193\n",
      "[210]\ttraining's multi_logloss: 0.0471185\n",
      "[220]\ttraining's multi_logloss: 0.0450954\n",
      "[230]\ttraining's multi_logloss: 0.0433099\n",
      "[240]\ttraining's multi_logloss: 0.0416323\n",
      "[250]\ttraining's multi_logloss: 0.0401305\n",
      "[260]\ttraining's multi_logloss: 0.0386766\n",
      "[270]\ttraining's multi_logloss: 0.0373835\n",
      "[280]\ttraining's multi_logloss: 0.036126\n",
      "[290]\ttraining's multi_logloss: 0.0349142\n",
      "[300]\ttraining's multi_logloss: 0.0338087\n",
      "[310]\ttraining's multi_logloss: 0.0327361\n",
      "[320]\ttraining's multi_logloss: 0.0317157\n",
      "[330]\ttraining's multi_logloss: 0.030781\n",
      "[340]\ttraining's multi_logloss: 0.0298731\n",
      "[350]\ttraining's multi_logloss: 0.0290041\n",
      "[360]\ttraining's multi_logloss: 0.0281661\n",
      "[370]\ttraining's multi_logloss: 0.0273781\n",
      "[380]\ttraining's multi_logloss: 0.0266171\n",
      "[390]\ttraining's multi_logloss: 0.0259028\n",
      "[400]\ttraining's multi_logloss: 0.0252315\n",
      "[410]\ttraining's multi_logloss: 0.0245783\n",
      "[420]\ttraining's multi_logloss: 0.0239542\n",
      "[430]\ttraining's multi_logloss: 0.023347\n",
      "[440]\ttraining's multi_logloss: 0.0227598\n",
      "[450]\ttraining's multi_logloss: 0.0221771\n",
      "[460]\ttraining's multi_logloss: 0.021626\n",
      "[470]\ttraining's multi_logloss: 0.0211018\n",
      "[480]\ttraining's multi_logloss: 0.0205811\n",
      "[490]\ttraining's multi_logloss: 0.0200635\n",
      "[500]\ttraining's multi_logloss: 0.0195963\n",
      "[510]\ttraining's multi_logloss: 0.0191503\n",
      "[520]\ttraining's multi_logloss: 0.0187075\n",
      "[530]\ttraining's multi_logloss: 0.0182706\n",
      "[540]\ttraining's multi_logloss: 0.0178615\n",
      "[550]\ttraining's multi_logloss: 0.0174559\n",
      "[560]\ttraining's multi_logloss: 0.0170672\n",
      "***********第14次抽样***********\n",
      "负样本抽样数量：\n",
      "(38887, 300)\n",
      "[20]\tcv_agg's multi_logloss: 0.589345 + 0.000647472\n",
      "[40]\tcv_agg's multi_logloss: 0.265157 + 0.00172213\n",
      "[60]\tcv_agg's multi_logloss: 0.145244 + 0.00226594\n",
      "[80]\tcv_agg's multi_logloss: 0.0958029 + 0.00242927\n",
      "[100]\tcv_agg's multi_logloss: 0.0726777 + 0.0025975\n",
      "[120]\tcv_agg's multi_logloss: 0.0604674 + 0.00261549\n",
      "[140]\tcv_agg's multi_logloss: 0.0531557 + 0.00256083\n",
      "[160]\tcv_agg's multi_logloss: 0.0485675 + 0.00264882\n",
      "[180]\tcv_agg's multi_logloss: 0.0454107 + 0.00257204\n",
      "[200]\tcv_agg's multi_logloss: 0.0431512 + 0.00250263\n",
      "[220]\tcv_agg's multi_logloss: 0.0413876 + 0.0024823\n",
      "[240]\tcv_agg's multi_logloss: 0.0400277 + 0.00253988\n",
      "[260]\tcv_agg's multi_logloss: 0.0389096 + 0.00249737\n",
      "[280]\tcv_agg's multi_logloss: 0.038033 + 0.00247751\n",
      "[300]\tcv_agg's multi_logloss: 0.0372888 + 0.00248209\n",
      "[320]\tcv_agg's multi_logloss: 0.0367099 + 0.00246127\n",
      "[340]\tcv_agg's multi_logloss: 0.0362221 + 0.00245357\n",
      "[360]\tcv_agg's multi_logloss: 0.0358308 + 0.00244077\n",
      "[380]\tcv_agg's multi_logloss: 0.0355035 + 0.00245622\n",
      "[400]\tcv_agg's multi_logloss: 0.03523 + 0.00245191\n",
      "[420]\tcv_agg's multi_logloss: 0.0349771 + 0.00246178\n",
      "[440]\tcv_agg's multi_logloss: 0.0348117 + 0.00246127\n",
      "[460]\tcv_agg's multi_logloss: 0.0346878 + 0.00247197\n",
      "[480]\tcv_agg's multi_logloss: 0.0346014 + 0.00246259\n",
      "[500]\tcv_agg's multi_logloss: 0.034501 + 0.00247238\n",
      "[520]\tcv_agg's multi_logloss: 0.0344407 + 0.00248604\n",
      "[540]\tcv_agg's multi_logloss: 0.0343979 + 0.00248899\n",
      "[560]\tcv_agg's multi_logloss: 0.0343485 + 0.00250505\n",
      "[580]\tcv_agg's multi_logloss: 0.0343084 + 0.00250855\n",
      "[600]\tcv_agg's multi_logloss: 0.0343263 + 0.00247368\n",
      "最优结果0.03430675756035102\n",
      "总loss[0.04735130628453371, 0.054884275472127694, 0.03781991717105289, 0.04323592371048647, 0.04230347054874013, 0.05716380758019011, 0.039876161644722685, 0.026682194021648432, 0.031569233860371755, 0.02977158357292824, 0.03464358245542504, 0.054121566714876626, 0.03994100666800342, 0.05727697364366343, 0.03430675756035102]\n",
      "[10]\ttraining's multi_logloss: 0.955556\n",
      "[20]\ttraining's multi_logloss: 0.587661\n",
      "[30]\ttraining's multi_logloss: 0.383062\n",
      "[40]\ttraining's multi_logloss: 0.262922\n",
      "[50]\ttraining's multi_logloss: 0.188817\n",
      "[60]\ttraining's multi_logloss: 0.142136\n",
      "[70]\ttraining's multi_logloss: 0.112113\n",
      "[80]\ttraining's multi_logloss: 0.0919134\n",
      "[90]\ttraining's multi_logloss: 0.0778372\n",
      "[100]\ttraining's multi_logloss: 0.0677502\n",
      "[110]\ttraining's multi_logloss: 0.0603065\n",
      "[120]\ttraining's multi_logloss: 0.0544832\n",
      "[130]\ttraining's multi_logloss: 0.0499054\n",
      "[140]\ttraining's multi_logloss: 0.0460914\n",
      "[150]\ttraining's multi_logloss: 0.0429138\n",
      "[160]\ttraining's multi_logloss: 0.0401159\n",
      "[170]\ttraining's multi_logloss: 0.0378235\n",
      "[180]\ttraining's multi_logloss: 0.0358454\n",
      "[190]\ttraining's multi_logloss: 0.0339905\n",
      "[200]\ttraining's multi_logloss: 0.0323835\n",
      "[210]\ttraining's multi_logloss: 0.0309827\n",
      "[220]\ttraining's multi_logloss: 0.0297232\n",
      "[230]\ttraining's multi_logloss: 0.0285473\n",
      "[240]\ttraining's multi_logloss: 0.027464\n",
      "[250]\ttraining's multi_logloss: 0.0264505\n",
      "[260]\ttraining's multi_logloss: 0.025536\n",
      "[270]\ttraining's multi_logloss: 0.0246628\n",
      "[280]\ttraining's multi_logloss: 0.0238571\n",
      "[290]\ttraining's multi_logloss: 0.0230912\n",
      "[300]\ttraining's multi_logloss: 0.0223966\n",
      "[310]\ttraining's multi_logloss: 0.0217154\n",
      "[320]\ttraining's multi_logloss: 0.0210349\n",
      "[330]\ttraining's multi_logloss: 0.0204083\n",
      "[340]\ttraining's multi_logloss: 0.0198143\n",
      "[350]\ttraining's multi_logloss: 0.0192507\n",
      "[360]\ttraining's multi_logloss: 0.0187246\n",
      "[370]\ttraining's multi_logloss: 0.0182141\n",
      "[380]\ttraining's multi_logloss: 0.0177323\n",
      "[390]\ttraining's multi_logloss: 0.0172696\n",
      "[400]\ttraining's multi_logloss: 0.0168231\n",
      "[410]\ttraining's multi_logloss: 0.0163955\n",
      "[420]\ttraining's multi_logloss: 0.0160008\n",
      "[430]\ttraining's multi_logloss: 0.0156093\n",
      "[440]\ttraining's multi_logloss: 0.0152254\n",
      "[450]\ttraining's multi_logloss: 0.0148715\n",
      "[460]\ttraining's multi_logloss: 0.0145233\n",
      "[470]\ttraining's multi_logloss: 0.0142048\n",
      "[480]\ttraining's multi_logloss: 0.0138824\n",
      "[490]\ttraining's multi_logloss: 0.0135674\n",
      "[500]\ttraining's multi_logloss: 0.0132669\n",
      "[510]\ttraining's multi_logloss: 0.0129859\n",
      "[520]\ttraining's multi_logloss: 0.0127021\n",
      "[530]\ttraining's multi_logloss: 0.0124169\n",
      "[540]\ttraining's multi_logloss: 0.0121501\n",
      "[550]\ttraining's multi_logloss: 0.0118955\n",
      "[560]\ttraining's multi_logloss: 0.0116481\n",
      "[570]\ttraining's multi_logloss: 0.0114111\n",
      "[580]\ttraining's multi_logloss: 0.0111768\n",
      "[590]\ttraining's multi_logloss: 0.0109524\n",
      "[600]\ttraining's multi_logloss: 0.0107473\n",
      "[610]\ttraining's multi_logloss: 0.010528\n",
      "[620]\ttraining's multi_logloss: 0.0103187\n",
      "[630]\ttraining's multi_logloss: 0.0101156\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "pred = pd.DataFrame()\n",
    "for i in range(15):\n",
    "    print('***********第%s次抽样***********'%i)\n",
    "    x0_part = x0.copy()\n",
    "    x0_part.index = range(x0_part.shape[0])\n",
    "    x1.index = range(x1.shape[0])\n",
    "    permutation = np.random.permutation(x0_part.shape[0])\n",
    "    \n",
    "    ratio = int(x0.shape[0]*random.uniform(1.5,5)/10)\n",
    "\n",
    "    select = x0_part.iloc[permutation[:ratio], :]\n",
    "    \n",
    "    print('负样本抽样数量：')\n",
    "    print(select.shape)\n",
    "    \n",
    "    data_new = pd.concat([x1, select], axis = 0)\n",
    "    \n",
    "    permutation = np.random.permutation(data_new.shape[0])\n",
    "\n",
    "    data_new = data_new.iloc[permutation, :]\n",
    "    \n",
    "    label_new = data_new['label'].values\n",
    "    \n",
    "    data_new = data_new.drop(['label', 'id'], axis = 1)\n",
    "    \n",
    "    dtrain = lgb.Dataset(data_new, label=label_new)\n",
    "\n",
    "    hist = lgb.cv(params, dtrain, num_boost_round = 1000, verbose_eval=20, early_stopping_rounds=30, stratified=False)\n",
    "    \n",
    "    print('最优结果%s'%hist['multi_logloss-mean'][-1])\n",
    "    loss_list.append(hist['multi_logloss-mean'][-1])\n",
    "    print('总loss%s'%loss_list)\n",
    "    model = lgb.train(params=params, train_set=dtrain, num_boost_round=int(1.1*len(hist['multi_logloss-mean'])), verbose_eval=10,\\\n",
    "                      valid_sets=dtrain)\n",
    "    \n",
    "    pred_temp = pd.DataFrame(model.predict(test_FE))\n",
    "    \n",
    "    pred = pd.concat([pred, pred_temp], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame(sub_sample.file_id)\n",
    "\n",
    "p['prob0'] = pred[0].mean(axis = 1)\n",
    "p['prob1'] = pred[1].mean(axis = 1)\n",
    "p['prob2'] = pred[2].mean(axis = 1)\n",
    "p['prob3'] = pred[3].mean(axis = 1)\n",
    "p['prob4'] = pred[4].mean(axis = 1)\n",
    "p['prob5'] = pred[5].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_id    26546.000000\n",
       "prob0          0.900665\n",
       "prob1          0.007622\n",
       "prob2          0.014091\n",
       "prob3          0.012161\n",
       "prob4          0.001237\n",
       "prob5          0.064224\n",
       "dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.to_csv('/home/libo/Security/sub/采样8.23.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = pd.read_csv('/home/libo/Security/sub/采样8.22.csv')\n",
    "x2 = pd.read_csv('/home/libo/Security/sub/采样8.23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.99771934, 0.99943215],\n",
       "       [0.99771934, 1.        , 0.99942719],\n",
       "       [0.99943215, 0.99942719, 1.        ]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef([x1['prob0'], x2['prob0'], pred['prob0']], rowvar=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = x1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred['prob0'] = 0.5*x1['prob0'] + 0.5*x2['prob0']\n",
    "pred['prob1'] = 0.5*x1['prob1'] + 0.5*x2['prob1']\n",
    "pred['prob2'] = 0.5*x1['prob2'] + 0.5*x2['prob2']\n",
    "pred['prob3'] = 0.5*x1['prob3'] + 0.5*x2['prob3']\n",
    "pred['prob4'] = 0.5*x1['prob4'] + 0.5*x2['prob4']\n",
    "pred['prob5'] = 0.5*x1['prob5'] + 0.5*x2['prob5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred.to_csv('/home/libo/Security/sub/采样融合.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stacking bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_sample = pd.read_csv('/home/libo/Security/3rd_security_submit_sample.csv')\n",
    "\n",
    "label = pd.read_pickle('/home/libo/Security/label.pkl')\n",
    "label = label[0].values\n",
    "\n",
    "feature = pd.read_pickle('/home/libo/Security/top_feature1000.pkl')\n",
    "\n",
    "feature['id'] = feature.iloc[:, 0].values\n",
    "\n",
    "feature.drop('file_id', axis = 1, inplace = True)\n",
    "\n",
    "train_FE = feature.iloc[:len(label),:]\n",
    "test_FE = feature.iloc[len(label):,:]\n",
    "\n",
    "train_FE.shape, test_FE.shape\n",
    "\n",
    "path = '/home/libo/Security/stack'\n",
    "os.chdir(path)\n",
    "\n",
    "list_ = os.listdir()\n",
    "train_list = [x for x in list_ if 'train' in x]\n",
    "test_list = [x for x in list_ if 'test' in x]\n",
    "\n",
    "len(train_list) == len(test_list)\n",
    "\n",
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116624, 6)\n",
      "(116624, 12)\n",
      "(116624, 18)\n",
      "(116624, 24)\n",
      "(116624, 30)\n",
      "(116624, 36)\n",
      "(116624, 42)\n",
      "(116624, 48)\n",
      "(116624, 54)\n",
      "(116624, 60)\n",
      "(116624, 66)\n",
      "(116624, 72)\n",
      "(116624, 78)\n",
      "(116624, 84)\n",
      "(116624, 90)\n",
      "(116624, 96)\n",
      "(116624, 102)\n",
      "(116624, 192)\n",
      "(116624, 198)\n",
      "(116624, 204)\n",
      "(116624, 210)\n",
      "(116624, 216)\n",
      "(116624, 222)\n",
      "(116624, 228)\n",
      "(116624, 234)\n",
      "(116624, 240)\n",
      "(116624, 246)\n",
      "(116624, 252)\n",
      "(116624, 258)\n"
     ]
    }
   ],
   "source": [
    "train_meta = pd.DataFrame()\n",
    "for i in train_list:\n",
    "    train_meta = pd.concat([train_meta, pd.read_csv(i)], axis = 1)\n",
    "    print(train_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53093, 6)\n",
      "(53093, 12)\n",
      "(53093, 18)\n",
      "(53093, 24)\n",
      "(53093, 30)\n",
      "(53093, 36)\n",
      "(53093, 42)\n",
      "(53093, 48)\n",
      "(53093, 54)\n",
      "(53093, 60)\n",
      "(53093, 66)\n",
      "(53093, 72)\n",
      "(53093, 78)\n",
      "(53093, 84)\n",
      "(53093, 90)\n",
      "(53093, 96)\n",
      "(53093, 102)\n",
      "(53093, 108)\n",
      "(53093, 114)\n",
      "(53093, 120)\n",
      "(53093, 126)\n",
      "(53093, 132)\n",
      "(53093, 138)\n",
      "(53093, 144)\n",
      "(53093, 150)\n",
      "(53093, 156)\n",
      "(53093, 162)\n",
      "(53093, 168)\n",
      "(53093, 258)\n"
     ]
    }
   ],
   "source": [
    "test_meta = pd.DataFrame()\n",
    "for i in test_list:\n",
    "    try:\n",
    "        x = pd.read_csv(i).drop('file_id', axis = 1)\n",
    "    except:\n",
    "        x = pd.read_csv(i)\n",
    "    test_meta = pd.concat([test_meta, x], axis = 1)\n",
    "    print(test_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta = test_meta[train_meta.columns]\n",
    "test_meta.columns == train_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116624, 258), (53093, 258))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.shape, test_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_FE.index = range(test_FE.shape[0])\n",
    "test_meta.index = range(test_meta.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multiclass\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.05,\n",
    "          \"num_leaves\": 3,\n",
    "          # \"max_bin\": 128,\n",
    "          \"feature_fraction\": 0.85,\n",
    "#           \"min_child_samples\": 10,\n",
    "#           \"min_child_weight\": 150,\n",
    "#           \"min_split_gain\": 0,\n",
    "          \"subsample\": 0.85,\n",
    "          #'metric':'logloss',\n",
    "#            'lambda_l1':0.1,\n",
    "#            'lambda_l2':0.1,\n",
    "          'seed':666,\n",
    "          'num_class':6\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libo/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/libo/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((116624, 1001), (53093, 999))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_FE['label'] = label\n",
    "\n",
    "test_FE.drop('id', axis = 1, inplace = True)\n",
    "\n",
    "train_FE.shape, test_FE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_FE = pd.concat([train_FE, train_meta], axis = 1)\n",
    "test_FE = pd.concat([test_FE, test_meta], axis = 1)\n",
    "\n",
    "train_FE.shape, test_FE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1257"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_FE.drop(['id', 'label'], axis = 1).columns == test_FE.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((111545, 1259), (5079, 1259))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = train_FE[train_FE['label'] == 0]\n",
    "\n",
    "x1 = train_FE[train_FE['label'] != 0]\n",
    "\n",
    "x0.shape, x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = pd.read_csv('/home/libo/Security/index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multiclass\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.05,\n",
    "          \"num_leaves\": 3,\n",
    "          # \"max_bin\": 128,\n",
    "          \"feature_fraction\": 0.85,\n",
    "#           \"min_child_samples\": 10,\n",
    "#           \"min_child_weight\": 150,\n",
    "#           \"min_split_gain\": 0,\n",
    "          \"subsample\": 0.85,\n",
    "          #'metric':'logloss',\n",
    "#            'lambda_l1':0.02,\n",
    "#            'lambda_l2':0.02,\n",
    "          'seed':666,\n",
    "          'num_class':6\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********第0次抽样***********\n",
      "负样本抽样数量：\n",
      "(40279, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.522239 + 0.00173581\n",
      "[20]\tcv_agg's multi_logloss: 0.53413 + 0.00105499\n",
      "[40]\tcv_agg's multi_logloss: 0.214341 + 0.000960128\n",
      "[60]\tcv_agg's multi_logloss: 0.104722 + 0.000655142\n",
      "[80]\tcv_agg's multi_logloss: 0.0654616 + 0.000741861\n",
      "[100]\tcv_agg's multi_logloss: 0.0513879 + 0.0008086\n",
      "[120]\tcv_agg's multi_logloss: 0.0463053 + 0.000835119\n",
      "[140]\tcv_agg's multi_logloss: 0.0445138 + 0.000835377\n",
      "[160]\tcv_agg's multi_logloss: 0.0438362 + 0.000909081\n",
      "[180]\tcv_agg's multi_logloss: 0.0436343 + 0.000944185\n",
      "[200]\tcv_agg's multi_logloss: 0.0435945 + 0.00101815\n",
      "[220]\tcv_agg's multi_logloss: 0.0436075 + 0.00107943\n",
      "[240]\tcv_agg's multi_logloss: 0.0436564 + 0.0011532\n",
      "最优结果0.04357055898915886\n",
      "总loss[0.02694491490357993, 0.04357055898915886]\n",
      "[10]\ttraining's multi_logloss: 0.909304\n",
      "[20]\ttraining's multi_logloss: 0.531743\n",
      "[30]\ttraining's multi_logloss: 0.32801\n",
      "[40]\ttraining's multi_logloss: 0.210631\n",
      "[50]\ttraining's multi_logloss: 0.141288\n",
      "[60]\ttraining's multi_logloss: 0.0996385\n",
      "[70]\ttraining's multi_logloss: 0.0744295\n",
      "[80]\ttraining's multi_logloss: 0.0589919\n",
      "[90]\ttraining's multi_logloss: 0.0492926\n",
      "[100]\ttraining's multi_logloss: 0.0430278\n",
      "[110]\ttraining's multi_logloss: 0.0388011\n",
      "[120]\ttraining's multi_logloss: 0.0358232\n",
      "[130]\ttraining's multi_logloss: 0.0336342\n",
      "[140]\ttraining's multi_logloss: 0.0318859\n",
      "[150]\ttraining's multi_logloss: 0.0304452\n",
      "[160]\ttraining's multi_logloss: 0.029108\n",
      "[170]\ttraining's multi_logloss: 0.0279782\n",
      "[180]\ttraining's multi_logloss: 0.0269943\n",
      "[190]\ttraining's multi_logloss: 0.0260575\n",
      "[200]\ttraining's multi_logloss: 0.0250784\n",
      "[210]\ttraining's multi_logloss: 0.0242773\n",
      "[220]\ttraining's multi_logloss: 0.0235448\n",
      "[230]\ttraining's multi_logloss: 0.0228382\n",
      "***********第2次抽样***********\n",
      "负样本抽样数量：\n",
      "(43391, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.521114 + 0.00079736\n",
      "[40]\tcv_agg's multi_logloss: 0.19821 + 0.00124992\n",
      "[60]\tcv_agg's multi_logloss: 0.0873328 + 0.00158554\n",
      "[80]\tcv_agg's multi_logloss: 0.0476799 + 0.00193054\n",
      "[100]\tcv_agg's multi_logloss: 0.0333814 + 0.0022509\n",
      "[120]\tcv_agg's multi_logloss: 0.0281981 + 0.00251358\n",
      "[140]\tcv_agg's multi_logloss: 0.0263798 + 0.00264496\n",
      "[160]\tcv_agg's multi_logloss: 0.0257407 + 0.00275179\n",
      "[180]\tcv_agg's multi_logloss: 0.025495 + 0.00280346\n",
      "[200]\tcv_agg's multi_logloss: 0.0254309 + 0.00285868\n",
      "[220]\tcv_agg's multi_logloss: 0.0253907 + 0.00293167\n",
      "[240]\tcv_agg's multi_logloss: 0.0253862 + 0.00295928\n",
      "[260]\tcv_agg's multi_logloss: 0.0254026 + 0.00300701\n",
      "最优结果0.025386178466265685\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685]\n",
      "[10]\ttraining's multi_logloss: 0.900768\n",
      "[20]\ttraining's multi_logloss: 0.520085\n",
      "[30]\ttraining's multi_logloss: 0.31464\n",
      "[40]\ttraining's multi_logloss: 0.196455\n",
      "[50]\ttraining's multi_logloss: 0.126696\n",
      "[60]\ttraining's multi_logloss: 0.0849025\n",
      "[70]\ttraining's multi_logloss: 0.0597152\n",
      "[80]\ttraining's multi_logloss: 0.0444213\n",
      "[90]\ttraining's multi_logloss: 0.035003\n",
      "[100]\ttraining's multi_logloss: 0.0290752\n",
      "[110]\ttraining's multi_logloss: 0.0252587\n",
      "[120]\ttraining's multi_logloss: 0.0227332\n",
      "[130]\ttraining's multi_logloss: 0.0209589\n",
      "[140]\ttraining's multi_logloss: 0.0196864\n",
      "[150]\ttraining's multi_logloss: 0.0186705\n",
      "[160]\ttraining's multi_logloss: 0.0178763\n",
      "[170]\ttraining's multi_logloss: 0.0171359\n",
      "[180]\ttraining's multi_logloss: 0.016471\n",
      "[190]\ttraining's multi_logloss: 0.0158816\n",
      "[200]\ttraining's multi_logloss: 0.0153411\n",
      "[210]\ttraining's multi_logloss: 0.0148762\n",
      "[220]\ttraining's multi_logloss: 0.0144251\n",
      "[230]\ttraining's multi_logloss: 0.0140268\n",
      "[240]\ttraining's multi_logloss: 0.0136379\n",
      "[250]\ttraining's multi_logloss: 0.0132654\n",
      "[260]\ttraining's multi_logloss: 0.012905\n",
      "***********第3次抽样***********\n",
      "负样本抽样数量：\n",
      "(23367, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.530647 + 0.00140507\n",
      "[40]\tcv_agg's multi_logloss: 0.210071 + 0.002371\n",
      "[60]\tcv_agg's multi_logloss: 0.100133 + 0.003149\n",
      "[80]\tcv_agg's multi_logloss: 0.060965 + 0.00381469\n",
      "[100]\tcv_agg's multi_logloss: 0.0469546 + 0.00439419\n",
      "[120]\tcv_agg's multi_logloss: 0.0419878 + 0.00476131\n",
      "[140]\tcv_agg's multi_logloss: 0.0403889 + 0.0049438\n",
      "[160]\tcv_agg's multi_logloss: 0.0397719 + 0.00503833\n",
      "[180]\tcv_agg's multi_logloss: 0.0395311 + 0.00506574\n",
      "[200]\tcv_agg's multi_logloss: 0.0394647 + 0.00514926\n",
      "[220]\tcv_agg's multi_logloss: 0.0394707 + 0.00530065\n",
      "[240]\tcv_agg's multi_logloss: 0.0394981 + 0.00543777\n",
      "最优结果0.03944566832825077\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077]\n",
      "[10]\ttraining's multi_logloss: 0.907122\n",
      "[20]\ttraining's multi_logloss: 0.528741\n",
      "[30]\ttraining's multi_logloss: 0.324539\n",
      "[40]\ttraining's multi_logloss: 0.206946\n",
      "[50]\ttraining's multi_logloss: 0.137525\n",
      "[60]\ttraining's multi_logloss: 0.0958917\n",
      "[70]\ttraining's multi_logloss: 0.0707324\n",
      "[80]\ttraining's multi_logloss: 0.0553718\n",
      "[90]\ttraining's multi_logloss: 0.0457313\n",
      "[100]\ttraining's multi_logloss: 0.0395822\n",
      "[110]\ttraining's multi_logloss: 0.0354812\n",
      "[120]\ttraining's multi_logloss: 0.032636\n",
      "[130]\ttraining's multi_logloss: 0.0305562\n",
      "[140]\ttraining's multi_logloss: 0.0289579\n",
      "[150]\ttraining's multi_logloss: 0.0275447\n",
      "[160]\ttraining's multi_logloss: 0.026364\n",
      "[170]\ttraining's multi_logloss: 0.0253343\n",
      "[180]\ttraining's multi_logloss: 0.0243668\n",
      "[190]\ttraining's multi_logloss: 0.0235129\n",
      "[200]\ttraining's multi_logloss: 0.0227188\n",
      "[210]\ttraining's multi_logloss: 0.0219745\n",
      "[220]\ttraining's multi_logloss: 0.021252\n",
      "[230]\ttraining's multi_logloss: 0.0206154\n",
      "***********第4次抽样***********\n",
      "负样本抽样数量：\n",
      "(23314, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.531155 + 0.00162405\n",
      "[40]\tcv_agg's multi_logloss: 0.210736 + 0.0022483\n",
      "[60]\tcv_agg's multi_logloss: 0.100759 + 0.00264927\n",
      "[80]\tcv_agg's multi_logloss: 0.0615638 + 0.00287271\n",
      "[100]\tcv_agg's multi_logloss: 0.0474694 + 0.00283532\n",
      "[120]\tcv_agg's multi_logloss: 0.0423575 + 0.0027396\n",
      "[140]\tcv_agg's multi_logloss: 0.0405419 + 0.00265264\n",
      "[160]\tcv_agg's multi_logloss: 0.0398666 + 0.0025877\n",
      "[180]\tcv_agg's multi_logloss: 0.0397039 + 0.00247642\n",
      "[200]\tcv_agg's multi_logloss: 0.0397329 + 0.00235129\n",
      "最优结果0.039703902139247335\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335]\n",
      "[10]\ttraining's multi_logloss: 0.907545\n",
      "[20]\ttraining's multi_logloss: 0.529292\n",
      "[30]\ttraining's multi_logloss: 0.325215\n",
      "[40]\ttraining's multi_logloss: 0.207724\n",
      "[50]\ttraining's multi_logloss: 0.138294\n",
      "[60]\ttraining's multi_logloss: 0.0966274\n",
      "[70]\ttraining's multi_logloss: 0.0714567\n",
      "[80]\ttraining's multi_logloss: 0.0560423\n",
      "[90]\ttraining's multi_logloss: 0.0463703\n",
      "[100]\ttraining's multi_logloss: 0.040148\n",
      "[110]\ttraining's multi_logloss: 0.0359879\n",
      "[120]\ttraining's multi_logloss: 0.0330796\n",
      "[130]\ttraining's multi_logloss: 0.030962\n",
      "[140]\ttraining's multi_logloss: 0.0293414\n",
      "[150]\ttraining's multi_logloss: 0.0279577\n",
      "[160]\ttraining's multi_logloss: 0.0267369\n",
      "[170]\ttraining's multi_logloss: 0.0257213\n",
      "[180]\ttraining's multi_logloss: 0.0247287\n",
      "[190]\ttraining's multi_logloss: 0.0238582\n",
      "***********第5次抽样***********\n",
      "负样本抽样数量：\n",
      "(34102, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.524158 + 0.000856299\n",
      "[40]\tcv_agg's multi_logloss: 0.202026 + 0.00122238\n",
      "[60]\tcv_agg's multi_logloss: 0.0913963 + 0.00158661\n",
      "[80]\tcv_agg's multi_logloss: 0.0518832 + 0.00188961\n",
      "[100]\tcv_agg's multi_logloss: 0.0375764 + 0.00221707\n",
      "[120]\tcv_agg's multi_logloss: 0.0324717 + 0.00254514\n",
      "[140]\tcv_agg's multi_logloss: 0.0305962 + 0.00283861\n",
      "[160]\tcv_agg's multi_logloss: 0.0300032 + 0.00309398\n",
      "[180]\tcv_agg's multi_logloss: 0.0297502 + 0.00322732\n",
      "[200]\tcv_agg's multi_logloss: 0.0296502 + 0.00326525\n",
      "[220]\tcv_agg's multi_logloss: 0.029609 + 0.00330831\n",
      "[240]\tcv_agg's multi_logloss: 0.0295972 + 0.00342335\n",
      "[260]\tcv_agg's multi_logloss: 0.029634 + 0.00349081\n",
      "最优结果0.029588663806432925\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925]\n",
      "[10]\ttraining's multi_logloss: 0.902827\n",
      "[20]\ttraining's multi_logloss: 0.522819\n",
      "[30]\ttraining's multi_logloss: 0.317884\n",
      "[40]\ttraining's multi_logloss: 0.19987\n",
      "[50]\ttraining's multi_logloss: 0.130187\n",
      "[60]\ttraining's multi_logloss: 0.0884812\n",
      "[70]\ttraining's multi_logloss: 0.0633207\n",
      "[80]\ttraining's multi_logloss: 0.0480138\n",
      "[90]\ttraining's multi_logloss: 0.0385471\n",
      "[100]\ttraining's multi_logloss: 0.0325591\n",
      "[110]\ttraining's multi_logloss: 0.0286493\n",
      "[120]\ttraining's multi_logloss: 0.0259872\n",
      "[130]\ttraining's multi_logloss: 0.0241027\n",
      "[140]\ttraining's multi_logloss: 0.0226632\n",
      "[150]\ttraining's multi_logloss: 0.0214832\n",
      "[160]\ttraining's multi_logloss: 0.0205722\n",
      "[170]\ttraining's multi_logloss: 0.0197775\n",
      "[180]\ttraining's multi_logloss: 0.0190333\n",
      "[190]\ttraining's multi_logloss: 0.0183517\n",
      "[200]\ttraining's multi_logloss: 0.01772\n",
      "[210]\ttraining's multi_logloss: 0.0171408\n",
      "[220]\ttraining's multi_logloss: 0.0166131\n",
      "[230]\ttraining's multi_logloss: 0.0161162\n",
      "[240]\ttraining's multi_logloss: 0.0156466\n",
      "[250]\ttraining's multi_logloss: 0.0151916\n",
      "***********第6次抽样***********\n",
      "负样本抽样数量：\n",
      "(38855, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.5232 + 0.00136225\n",
      "[40]\tcv_agg's multi_logloss: 0.200805 + 0.00210651\n",
      "[60]\tcv_agg's multi_logloss: 0.0900663 + 0.00272204\n",
      "[80]\tcv_agg's multi_logloss: 0.0503786 + 0.00308144\n",
      "[100]\tcv_agg's multi_logloss: 0.0360424 + 0.0033095\n",
      "[120]\tcv_agg's multi_logloss: 0.0308581 + 0.00347854\n",
      "[140]\tcv_agg's multi_logloss: 0.0289806 + 0.00355454\n",
      "[160]\tcv_agg's multi_logloss: 0.0283156 + 0.00356012\n",
      "[180]\tcv_agg's multi_logloss: 0.0280555 + 0.00362258\n",
      "[200]\tcv_agg's multi_logloss: 0.027905 + 0.00368672\n",
      "[220]\tcv_agg's multi_logloss: 0.0278217 + 0.00374947\n",
      "[240]\tcv_agg's multi_logloss: 0.0277921 + 0.00383161\n",
      "[260]\tcv_agg's multi_logloss: 0.0277838 + 0.00387935\n",
      "最优结果0.027776737458289312\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312]\n",
      "[10]\ttraining's multi_logloss: 0.902283\n",
      "[20]\ttraining's multi_logloss: 0.522222\n",
      "[30]\ttraining's multi_logloss: 0.317101\n",
      "[40]\ttraining's multi_logloss: 0.199047\n",
      "[50]\ttraining's multi_logloss: 0.12932\n",
      "[60]\ttraining's multi_logloss: 0.0875537\n",
      "[70]\ttraining's multi_logloss: 0.0623356\n",
      "[80]\ttraining's multi_logloss: 0.0470205\n",
      "[90]\ttraining's multi_logloss: 0.0375408\n",
      "[100]\ttraining's multi_logloss: 0.0315582\n",
      "[110]\ttraining's multi_logloss: 0.0276655\n",
      "[120]\ttraining's multi_logloss: 0.0250801\n",
      "[130]\ttraining's multi_logloss: 0.0232318\n",
      "[140]\ttraining's multi_logloss: 0.0218778\n",
      "[150]\ttraining's multi_logloss: 0.0207871\n",
      "[160]\ttraining's multi_logloss: 0.0198573\n",
      "[170]\ttraining's multi_logloss: 0.0190279\n",
      "[180]\ttraining's multi_logloss: 0.0183254\n",
      "[190]\ttraining's multi_logloss: 0.0177042\n",
      "[200]\ttraining's multi_logloss: 0.0171335\n",
      "[210]\ttraining's multi_logloss: 0.0166065\n",
      "[220]\ttraining's multi_logloss: 0.0161226\n",
      "[230]\ttraining's multi_logloss: 0.0156692\n",
      "[240]\ttraining's multi_logloss: 0.0152383\n",
      "[250]\ttraining's multi_logloss: 0.0148316\n",
      "[260]\ttraining's multi_logloss: 0.0144449\n",
      "[270]\ttraining's multi_logloss: 0.0140696\n",
      "***********第7次抽样***********\n",
      "负样本抽样数量：\n",
      "(25762, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.529341 + 0.00156682\n",
      "[40]\tcv_agg's multi_logloss: 0.208291 + 0.00216914\n",
      "[60]\tcv_agg's multi_logloss: 0.0980114 + 0.002494\n",
      "[80]\tcv_agg's multi_logloss: 0.0585675 + 0.002623\n",
      "[100]\tcv_agg's multi_logloss: 0.0443856 + 0.00267101\n",
      "[120]\tcv_agg's multi_logloss: 0.0392732 + 0.00270353\n",
      "[140]\tcv_agg's multi_logloss: 0.0375041 + 0.00277744\n",
      "[160]\tcv_agg's multi_logloss: 0.0368987 + 0.00282366\n",
      "[180]\tcv_agg's multi_logloss: 0.0366461 + 0.00292705\n",
      "[200]\tcv_agg's multi_logloss: 0.0364605 + 0.00297532\n",
      "[220]\tcv_agg's multi_logloss: 0.0363704 + 0.00305562\n",
      "[240]\tcv_agg's multi_logloss: 0.0363131 + 0.00311344\n",
      "[260]\tcv_agg's multi_logloss: 0.0363337 + 0.00317544\n",
      "最优结果0.036310787258282566\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566]\n",
      "[10]\ttraining's multi_logloss: 0.906573\n",
      "[20]\ttraining's multi_logloss: 0.5278\n",
      "[30]\ttraining's multi_logloss: 0.323373\n",
      "[40]\ttraining's multi_logloss: 0.205712\n",
      "[50]\ttraining's multi_logloss: 0.136205\n",
      "[60]\ttraining's multi_logloss: 0.0945401\n",
      "[70]\ttraining's multi_logloss: 0.0693771\n",
      "[80]\ttraining's multi_logloss: 0.054\n",
      "[90]\ttraining's multi_logloss: 0.0444018\n",
      "[100]\ttraining's multi_logloss: 0.0382777\n",
      "[110]\ttraining's multi_logloss: 0.0341866\n",
      "[120]\ttraining's multi_logloss: 0.0313735\n",
      "[130]\ttraining's multi_logloss: 0.0293649\n",
      "[140]\ttraining's multi_logloss: 0.0278117\n",
      "[150]\ttraining's multi_logloss: 0.026503\n",
      "[160]\ttraining's multi_logloss: 0.0253797\n",
      "[170]\ttraining's multi_logloss: 0.0243247\n",
      "[180]\ttraining's multi_logloss: 0.0233633\n",
      "[190]\ttraining's multi_logloss: 0.0225476\n",
      "[200]\ttraining's multi_logloss: 0.0217475\n",
      "[210]\ttraining's multi_logloss: 0.0210529\n",
      "[220]\ttraining's multi_logloss: 0.020384\n",
      "[230]\ttraining's multi_logloss: 0.0197861\n",
      "[240]\ttraining's multi_logloss: 0.0192164\n",
      "[250]\ttraining's multi_logloss: 0.0186801\n",
      "[260]\ttraining's multi_logloss: 0.0181909\n",
      "***********第8次抽样***********\n",
      "负样本抽样数量：\n",
      "(23053, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.531698 + 0.00156903\n",
      "[40]\tcv_agg's multi_logloss: 0.211229 + 0.00231826\n",
      "[60]\tcv_agg's multi_logloss: 0.101325 + 0.00290254\n",
      "[80]\tcv_agg's multi_logloss: 0.0621606 + 0.00325879\n",
      "[100]\tcv_agg's multi_logloss: 0.0482164 + 0.00339374\n",
      "[120]\tcv_agg's multi_logloss: 0.0431782 + 0.00354243\n",
      "[140]\tcv_agg's multi_logloss: 0.0414007 + 0.00369891\n",
      "[160]\tcv_agg's multi_logloss: 0.0409092 + 0.00375778\n",
      "[180]\tcv_agg's multi_logloss: 0.0406472 + 0.00377001\n",
      "[200]\tcv_agg's multi_logloss: 0.0406401 + 0.00376866\n",
      "最优结果0.040622262484798395\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395]\n",
      "[10]\ttraining's multi_logloss: 0.90792\n",
      "[20]\ttraining's multi_logloss: 0.529798\n",
      "[30]\ttraining's multi_logloss: 0.325793\n",
      "[40]\ttraining's multi_logloss: 0.208331\n",
      "[50]\ttraining's multi_logloss: 0.138942\n",
      "[60]\ttraining's multi_logloss: 0.097298\n",
      "[70]\ttraining's multi_logloss: 0.0721167\n",
      "[80]\ttraining's multi_logloss: 0.0567288\n",
      "[90]\ttraining's multi_logloss: 0.0470943\n",
      "[100]\ttraining's multi_logloss: 0.0408939\n",
      "[110]\ttraining's multi_logloss: 0.0367312\n",
      "[120]\ttraining's multi_logloss: 0.0338439\n",
      "[130]\ttraining's multi_logloss: 0.0317185\n",
      "[140]\ttraining's multi_logloss: 0.0300504\n",
      "[150]\ttraining's multi_logloss: 0.028705\n",
      "[160]\ttraining's multi_logloss: 0.0274791\n",
      "[170]\ttraining's multi_logloss: 0.0263867\n",
      "[180]\ttraining's multi_logloss: 0.0254358\n",
      "[190]\ttraining's multi_logloss: 0.0245264\n",
      "[200]\ttraining's multi_logloss: 0.023732\n",
      "***********第9次抽样***********\n",
      "负样本抽样数量：\n",
      "(29643, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.526935 + 0.000973562\n",
      "[40]\tcv_agg's multi_logloss: 0.20543 + 0.00136773\n",
      "[60]\tcv_agg's multi_logloss: 0.0951272 + 0.00159407\n",
      "[80]\tcv_agg's multi_logloss: 0.0557093 + 0.00180594\n",
      "[100]\tcv_agg's multi_logloss: 0.0415602 + 0.00188728\n",
      "[120]\tcv_agg's multi_logloss: 0.0365266 + 0.00190213\n",
      "[140]\tcv_agg's multi_logloss: 0.0347304 + 0.00199216\n",
      "[160]\tcv_agg's multi_logloss: 0.0341358 + 0.00203397\n",
      "[180]\tcv_agg's multi_logloss: 0.0339552 + 0.00201367\n",
      "[200]\tcv_agg's multi_logloss: 0.033899 + 0.00207374\n",
      "[220]\tcv_agg's multi_logloss: 0.0338559 + 0.00206152\n",
      "[240]\tcv_agg's multi_logloss: 0.0338565 + 0.00206169\n",
      "最优结果0.0338477240919984\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395, 0.0338477240919984]\n",
      "[10]\ttraining's multi_logloss: 0.90495\n",
      "[20]\ttraining's multi_logloss: 0.525639\n",
      "[30]\ttraining's multi_logloss: 0.321018\n",
      "[40]\ttraining's multi_logloss: 0.203207\n",
      "[50]\ttraining's multi_logloss: 0.133613\n",
      "[60]\ttraining's multi_logloss: 0.0918945\n",
      "[70]\ttraining's multi_logloss: 0.0667104\n",
      "[80]\ttraining's multi_logloss: 0.0513567\n",
      "[90]\ttraining's multi_logloss: 0.0418277\n",
      "[100]\ttraining's multi_logloss: 0.0357844\n",
      "[110]\ttraining's multi_logloss: 0.0318065\n",
      "[120]\ttraining's multi_logloss: 0.0290412\n",
      "[130]\ttraining's multi_logloss: 0.0271072\n",
      "[140]\ttraining's multi_logloss: 0.0255436\n",
      "[150]\ttraining's multi_logloss: 0.0243573\n",
      "[160]\ttraining's multi_logloss: 0.0233572\n",
      "[170]\ttraining's multi_logloss: 0.0224478\n",
      "[180]\ttraining's multi_logloss: 0.0216012\n",
      "[190]\ttraining's multi_logloss: 0.0208054\n",
      "[200]\ttraining's multi_logloss: 0.020102\n",
      "[210]\ttraining's multi_logloss: 0.0194489\n",
      "[220]\ttraining's multi_logloss: 0.0188637\n",
      "[230]\ttraining's multi_logloss: 0.0182841\n",
      "***********第10次抽样***********\n",
      "负样本抽样数量：\n",
      "(32666, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.525326 + 0.00140973\n",
      "[40]\tcv_agg's multi_logloss: 0.203479 + 0.0022552\n",
      "[60]\tcv_agg's multi_logloss: 0.0930828 + 0.00276179\n",
      "[80]\tcv_agg's multi_logloss: 0.0537098 + 0.00301157\n",
      "[100]\tcv_agg's multi_logloss: 0.0394365 + 0.00312845\n",
      "[120]\tcv_agg's multi_logloss: 0.0343019 + 0.0031394\n",
      "[140]\tcv_agg's multi_logloss: 0.0324218 + 0.00315673\n",
      "[160]\tcv_agg's multi_logloss: 0.0317996 + 0.00320728\n",
      "[180]\tcv_agg's multi_logloss: 0.031568 + 0.00323891\n",
      "[200]\tcv_agg's multi_logloss: 0.0314842 + 0.0032322\n",
      "[220]\tcv_agg's multi_logloss: 0.0314383 + 0.00326808\n",
      "[240]\tcv_agg's multi_logloss: 0.0314453 + 0.00335914\n",
      "[260]\tcv_agg's multi_logloss: 0.0315239 + 0.00342462\n",
      "最优结果0.031426275528943004\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395, 0.0338477240919984, 0.031426275528943004]\n",
      "[10]\ttraining's multi_logloss: 0.903729\n",
      "[20]\ttraining's multi_logloss: 0.524096\n",
      "[30]\ttraining's multi_logloss: 0.319324\n",
      "[40]\ttraining's multi_logloss: 0.201479\n",
      "[50]\ttraining's multi_logloss: 0.131888\n",
      "[60]\ttraining's multi_logloss: 0.090179\n",
      "[70]\ttraining's multi_logloss: 0.0650082\n",
      "[80]\ttraining's multi_logloss: 0.0496836\n",
      "[90]\ttraining's multi_logloss: 0.0401944\n",
      "[100]\ttraining's multi_logloss: 0.0341731\n",
      "[110]\ttraining's multi_logloss: 0.0302028\n",
      "[120]\ttraining's multi_logloss: 0.027487\n",
      "[130]\ttraining's multi_logloss: 0.0255712\n",
      "[140]\ttraining's multi_logloss: 0.0241447\n",
      "[150]\ttraining's multi_logloss: 0.0229962\n",
      "[160]\ttraining's multi_logloss: 0.0219853\n",
      "[170]\ttraining's multi_logloss: 0.0211024\n",
      "[180]\ttraining's multi_logloss: 0.0203299\n",
      "[190]\ttraining's multi_logloss: 0.019632\n",
      "[200]\ttraining's multi_logloss: 0.0189977\n",
      "[210]\ttraining's multi_logloss: 0.0183955\n",
      "[220]\ttraining's multi_logloss: 0.0178515\n",
      "[230]\ttraining's multi_logloss: 0.0173482\n",
      "[240]\ttraining's multi_logloss: 0.0168834\n",
      "[250]\ttraining's multi_logloss: 0.016416\n",
      "***********第11次抽样***********\n",
      "负样本抽样数量：\n",
      "(29346, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.526532 + 0.000909441\n",
      "[40]\tcv_agg's multi_logloss: 0.204714 + 0.00117788\n",
      "[60]\tcv_agg's multi_logloss: 0.0942665 + 0.00152615\n",
      "[80]\tcv_agg's multi_logloss: 0.0547881 + 0.0017826\n",
      "[100]\tcv_agg's multi_logloss: 0.0405253 + 0.0020572\n",
      "[120]\tcv_agg's multi_logloss: 0.0353296 + 0.00218927\n",
      "[140]\tcv_agg's multi_logloss: 0.0335487 + 0.00240439\n",
      "[160]\tcv_agg's multi_logloss: 0.0330196 + 0.00258874\n",
      "[180]\tcv_agg's multi_logloss: 0.0327847 + 0.00272057\n",
      "[200]\tcv_agg's multi_logloss: 0.0326619 + 0.00284581\n",
      "[220]\tcv_agg's multi_logloss: 0.0326114 + 0.00298916\n",
      "[240]\tcv_agg's multi_logloss: 0.0326068 + 0.00307321\n",
      "[260]\tcv_agg's multi_logloss: 0.0326308 + 0.00316334\n",
      "最优结果0.032598245507739744\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395, 0.0338477240919984, 0.031426275528943004, 0.032598245507739744]\n",
      "[10]\ttraining's multi_logloss: 0.904582\n",
      "[20]\ttraining's multi_logloss: 0.525071\n",
      "[30]\ttraining's multi_logloss: 0.320357\n",
      "[40]\ttraining's multi_logloss: 0.202535\n",
      "[50]\ttraining's multi_logloss: 0.13295\n",
      "[60]\ttraining's multi_logloss: 0.0911904\n",
      "[70]\ttraining's multi_logloss: 0.065976\n",
      "[80]\ttraining's multi_logloss: 0.0506119\n",
      "[90]\ttraining's multi_logloss: 0.0410637\n",
      "[100]\ttraining's multi_logloss: 0.0349902\n",
      "[110]\ttraining's multi_logloss: 0.0309719\n",
      "[120]\ttraining's multi_logloss: 0.0282285\n",
      "[130]\ttraining's multi_logloss: 0.026235\n",
      "[140]\ttraining's multi_logloss: 0.0247608\n",
      "[150]\ttraining's multi_logloss: 0.02352\n",
      "[160]\ttraining's multi_logloss: 0.0224724\n",
      "[170]\ttraining's multi_logloss: 0.0215149\n",
      "[180]\ttraining's multi_logloss: 0.0206703\n",
      "[190]\ttraining's multi_logloss: 0.0199039\n",
      "[200]\ttraining's multi_logloss: 0.0192222\n",
      "[210]\ttraining's multi_logloss: 0.0185844\n",
      "[220]\ttraining's multi_logloss: 0.0180088\n",
      "[230]\ttraining's multi_logloss: 0.0174551\n",
      "[240]\ttraining's multi_logloss: 0.0169513\n",
      "[250]\ttraining's multi_logloss: 0.0164705\n",
      "***********第12次抽样***********\n",
      "负样本抽样数量：\n",
      "(29965, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.52725 + 0.000955523\n",
      "[40]\tcv_agg's multi_logloss: 0.205622 + 0.00144771\n",
      "[60]\tcv_agg's multi_logloss: 0.0952667 + 0.00176775\n",
      "[80]\tcv_agg's multi_logloss: 0.0558403 + 0.00203973\n",
      "[100]\tcv_agg's multi_logloss: 0.0416052 + 0.00218174\n",
      "[120]\tcv_agg's multi_logloss: 0.0365425 + 0.00236772\n",
      "[140]\tcv_agg's multi_logloss: 0.0348049 + 0.00259968\n",
      "[160]\tcv_agg's multi_logloss: 0.0342318 + 0.00279565\n",
      "[180]\tcv_agg's multi_logloss: 0.0340328 + 0.00291015\n",
      "[200]\tcv_agg's multi_logloss: 0.0338789 + 0.00298318\n",
      "[220]\tcv_agg's multi_logloss: 0.0338475 + 0.00301804\n",
      "[240]\tcv_agg's multi_logloss: 0.033818 + 0.00307737\n",
      "[260]\tcv_agg's multi_logloss: 0.0338072 + 0.00310923\n",
      "[280]\tcv_agg's multi_logloss: 0.0338073 + 0.00316491\n",
      "最优结果0.033797054803081775\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395, 0.0338477240919984, 0.031426275528943004, 0.032598245507739744, 0.033797054803081775]\n",
      "[10]\ttraining's multi_logloss: 0.904839\n",
      "[20]\ttraining's multi_logloss: 0.525669\n",
      "[30]\ttraining's multi_logloss: 0.321041\n",
      "[40]\ttraining's multi_logloss: 0.203192\n",
      "[50]\ttraining's multi_logloss: 0.133573\n",
      "[60]\ttraining's multi_logloss: 0.0918843\n",
      "[70]\ttraining's multi_logloss: 0.0667532\n",
      "[80]\ttraining's multi_logloss: 0.0514122\n",
      "[90]\ttraining's multi_logloss: 0.0418936\n",
      "[100]\ttraining's multi_logloss: 0.0358581\n",
      "[110]\ttraining's multi_logloss: 0.0319049\n",
      "[120]\ttraining's multi_logloss: 0.0291947\n",
      "[130]\ttraining's multi_logloss: 0.0272623\n",
      "[140]\ttraining's multi_logloss: 0.0258311\n",
      "[150]\ttraining's multi_logloss: 0.0246156\n",
      "[160]\ttraining's multi_logloss: 0.0235439\n",
      "[170]\ttraining's multi_logloss: 0.0225742\n",
      "[180]\ttraining's multi_logloss: 0.0217685\n",
      "[190]\ttraining's multi_logloss: 0.0210141\n",
      "[200]\ttraining's multi_logloss: 0.0203222\n",
      "[210]\ttraining's multi_logloss: 0.0197007\n",
      "[220]\ttraining's multi_logloss: 0.0190978\n",
      "[230]\ttraining's multi_logloss: 0.0185343\n",
      "[240]\ttraining's multi_logloss: 0.018009\n",
      "[250]\ttraining's multi_logloss: 0.0174952\n",
      "[260]\ttraining's multi_logloss: 0.0170196\n",
      "[270]\ttraining's multi_logloss: 0.0165724\n",
      "[280]\ttraining's multi_logloss: 0.0161465\n",
      "***********第13次抽样***********\n",
      "负样本抽样数量：\n",
      "(23954, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.531002 + 0.00137867\n",
      "[40]\tcv_agg's multi_logloss: 0.210384 + 0.00189981\n",
      "[60]\tcv_agg's multi_logloss: 0.100388 + 0.0021807\n",
      "[80]\tcv_agg's multi_logloss: 0.0611766 + 0.00245419\n",
      "[100]\tcv_agg's multi_logloss: 0.0470681 + 0.00254597\n",
      "[120]\tcv_agg's multi_logloss: 0.0420068 + 0.00257813\n",
      "[140]\tcv_agg's multi_logloss: 0.0402347 + 0.00256117\n",
      "[160]\tcv_agg's multi_logloss: 0.0395683 + 0.00252126\n",
      "[180]\tcv_agg's multi_logloss: 0.0392471 + 0.00248806\n",
      "[200]\tcv_agg's multi_logloss: 0.0391358 + 0.0024635\n",
      "[220]\tcv_agg's multi_logloss: 0.0391009 + 0.00246128\n",
      "[240]\tcv_agg's multi_logloss: 0.0391256 + 0.00245364\n",
      "最优结果0.03909386747168482\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395, 0.0338477240919984, 0.031426275528943004, 0.032598245507739744, 0.033797054803081775, 0.03909386747168482]\n",
      "[10]\ttraining's multi_logloss: 0.907395\n",
      "[20]\ttraining's multi_logloss: 0.529227\n",
      "[30]\ttraining's multi_logloss: 0.325136\n",
      "[40]\ttraining's multi_logloss: 0.207638\n",
      "[50]\ttraining's multi_logloss: 0.138255\n",
      "[60]\ttraining's multi_logloss: 0.0966295\n",
      "[70]\ttraining's multi_logloss: 0.0714794\n",
      "[80]\ttraining's multi_logloss: 0.056132\n",
      "[90]\ttraining's multi_logloss: 0.0465091\n",
      "[100]\ttraining's multi_logloss: 0.0403167\n",
      "[110]\ttraining's multi_logloss: 0.0361815\n",
      "[120]\ttraining's multi_logloss: 0.0332957\n",
      "[130]\ttraining's multi_logloss: 0.0312331\n",
      "[140]\ttraining's multi_logloss: 0.0295461\n",
      "[150]\ttraining's multi_logloss: 0.0282536\n",
      "[160]\ttraining's multi_logloss: 0.0271043\n",
      "[170]\ttraining's multi_logloss: 0.0260665\n",
      "[180]\ttraining's multi_logloss: 0.0251146\n",
      "[190]\ttraining's multi_logloss: 0.0242488\n",
      "[200]\ttraining's multi_logloss: 0.0234773\n",
      "[210]\ttraining's multi_logloss: 0.0227384\n",
      "[220]\ttraining's multi_logloss: 0.0220213\n",
      "[230]\ttraining's multi_logloss: 0.0213791\n",
      "[240]\ttraining's multi_logloss: 0.020755\n",
      "***********第14次抽样***********\n",
      "负样本抽样数量：\n",
      "(19710, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.534827 + 0.000914063\n",
      "[40]\tcv_agg's multi_logloss: 0.215286 + 0.00162907\n",
      "[60]\tcv_agg's multi_logloss: 0.105901 + 0.00233428\n",
      "[80]\tcv_agg's multi_logloss: 0.0669124 + 0.00274845\n",
      "[100]\tcv_agg's multi_logloss: 0.05291 + 0.00306302\n",
      "[120]\tcv_agg's multi_logloss: 0.0478438 + 0.00332129\n",
      "[140]\tcv_agg's multi_logloss: 0.0460236 + 0.00336315\n",
      "[160]\tcv_agg's multi_logloss: 0.0453476 + 0.00336061\n",
      "[180]\tcv_agg's multi_logloss: 0.0450216 + 0.00338502\n",
      "[200]\tcv_agg's multi_logloss: 0.0448844 + 0.00334112\n",
      "[220]\tcv_agg's multi_logloss: 0.0448328 + 0.00331402\n",
      "最优结果0.044816453327352526\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395, 0.0338477240919984, 0.031426275528943004, 0.032598245507739744, 0.033797054803081775, 0.03909386747168482, 0.044816453327352526]\n",
      "[10]\ttraining's multi_logloss: 0.90979\n",
      "[20]\ttraining's multi_logloss: 0.532392\n",
      "[30]\ttraining's multi_logloss: 0.328812\n",
      "[40]\ttraining's multi_logloss: 0.211513\n",
      "[50]\ttraining's multi_logloss: 0.142304\n",
      "[60]\ttraining's multi_logloss: 0.100769\n",
      "[70]\ttraining's multi_logloss: 0.0756008\n",
      "[80]\ttraining's multi_logloss: 0.0601886\n",
      "[90]\ttraining's multi_logloss: 0.0505132\n",
      "[100]\ttraining's multi_logloss: 0.0442657\n",
      "[110]\ttraining's multi_logloss: 0.0400165\n",
      "[120]\ttraining's multi_logloss: 0.0370032\n",
      "[130]\ttraining's multi_logloss: 0.0348099\n",
      "[140]\ttraining's multi_logloss: 0.0329569\n",
      "[150]\ttraining's multi_logloss: 0.0314783\n",
      "[160]\ttraining's multi_logloss: 0.0301415\n",
      "[170]\ttraining's multi_logloss: 0.0289271\n",
      "[180]\ttraining's multi_logloss: 0.0278686\n",
      "[190]\ttraining's multi_logloss: 0.026927\n",
      "[200]\ttraining's multi_logloss: 0.0259997\n",
      "[210]\ttraining's multi_logloss: 0.0251195\n",
      "[220]\ttraining's multi_logloss: 0.0243404\n",
      "***********第15次抽样***********\n",
      "负样本抽样数量：\n",
      "(43648, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.521212 + 0.00132097\n",
      "[40]\tcv_agg's multi_logloss: 0.197955 + 0.00185596\n",
      "[60]\tcv_agg's multi_logloss: 0.0869594 + 0.00229504\n",
      "[80]\tcv_agg's multi_logloss: 0.0472119 + 0.002571\n",
      "[100]\tcv_agg's multi_logloss: 0.0327912 + 0.00274145\n",
      "[120]\tcv_agg's multi_logloss: 0.027616 + 0.00280877\n",
      "[140]\tcv_agg's multi_logloss: 0.0258044 + 0.00282358\n",
      "[160]\tcv_agg's multi_logloss: 0.0252007 + 0.00290058\n",
      "[180]\tcv_agg's multi_logloss: 0.02501 + 0.00293525\n",
      "[200]\tcv_agg's multi_logloss: 0.0249627 + 0.00294462\n",
      "[220]\tcv_agg's multi_logloss: 0.0249768 + 0.0030057\n",
      "最优结果0.02494793669885238\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395, 0.0338477240919984, 0.031426275528943004, 0.032598245507739744, 0.033797054803081775, 0.03909386747168482, 0.044816453327352526, 0.02494793669885238]\n",
      "[10]\ttraining's multi_logloss: 0.900899\n",
      "[20]\ttraining's multi_logloss: 0.520175\n",
      "[30]\ttraining's multi_logloss: 0.314672\n",
      "[40]\ttraining's multi_logloss: 0.196374\n",
      "[50]\ttraining's multi_logloss: 0.126584\n",
      "[60]\ttraining's multi_logloss: 0.0847858\n",
      "[70]\ttraining's multi_logloss: 0.0595838\n",
      "[80]\ttraining's multi_logloss: 0.044258\n",
      "[90]\ttraining's multi_logloss: 0.0347846\n",
      "[100]\ttraining's multi_logloss: 0.0288641\n",
      "[110]\ttraining's multi_logloss: 0.025034\n",
      "[120]\ttraining's multi_logloss: 0.022479\n",
      "[130]\ttraining's multi_logloss: 0.0206956\n",
      "[140]\ttraining's multi_logloss: 0.0194183\n",
      "[150]\ttraining's multi_logloss: 0.0183816\n",
      "[160]\ttraining's multi_logloss: 0.0175621\n",
      "[170]\ttraining's multi_logloss: 0.0168764\n",
      "[180]\ttraining's multi_logloss: 0.0162188\n",
      "[190]\ttraining's multi_logloss: 0.0156635\n",
      "[200]\ttraining's multi_logloss: 0.0151324\n",
      "[210]\ttraining's multi_logloss: 0.0146594\n",
      "[220]\ttraining's multi_logloss: 0.0141824\n",
      "***********第16次抽样***********\n",
      "负样本抽样数量：\n",
      "(26813, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.528519 + 0.000737287\n",
      "[40]\tcv_agg's multi_logloss: 0.207408 + 0.00122754\n",
      "[60]\tcv_agg's multi_logloss: 0.0974187 + 0.00138098\n",
      "[80]\tcv_agg's multi_logloss: 0.0581611 + 0.00147238\n",
      "[100]\tcv_agg's multi_logloss: 0.0438854 + 0.00144022\n",
      "[120]\tcv_agg's multi_logloss: 0.0388664 + 0.00140605\n",
      "[140]\tcv_agg's multi_logloss: 0.0370554 + 0.00135356\n",
      "[160]\tcv_agg's multi_logloss: 0.0364263 + 0.00137795\n",
      "[180]\tcv_agg's multi_logloss: 0.0362424 + 0.00142837\n",
      "[200]\tcv_agg's multi_logloss: 0.0362261 + 0.00143312\n",
      "[220]\tcv_agg's multi_logloss: 0.0362406 + 0.00141195\n",
      "[240]\tcv_agg's multi_logloss: 0.0362736 + 0.00142957\n",
      "最优结果0.03620192269369673\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395, 0.0338477240919984, 0.031426275528943004, 0.032598245507739744, 0.033797054803081775, 0.03909386747168482, 0.044816453327352526, 0.02494793669885238, 0.03620192269369673]\n",
      "[10]\ttraining's multi_logloss: 0.9061\n",
      "[20]\ttraining's multi_logloss: 0.527078\n",
      "[30]\ttraining's multi_logloss: 0.322614\n",
      "[40]\ttraining's multi_logloss: 0.20493\n",
      "[50]\ttraining's multi_logloss: 0.135442\n",
      "[60]\ttraining's multi_logloss: 0.0937884\n",
      "[70]\ttraining's multi_logloss: 0.0686274\n",
      "[80]\ttraining's multi_logloss: 0.0532591\n",
      "[90]\ttraining's multi_logloss: 0.0436592\n",
      "[100]\ttraining's multi_logloss: 0.0375475\n",
      "[110]\ttraining's multi_logloss: 0.0334869\n",
      "[120]\ttraining's multi_logloss: 0.0306845\n",
      "[130]\ttraining's multi_logloss: 0.0286877\n",
      "[140]\ttraining's multi_logloss: 0.0271541\n",
      "[150]\ttraining's multi_logloss: 0.0259099\n",
      "[160]\ttraining's multi_logloss: 0.0247347\n",
      "[170]\ttraining's multi_logloss: 0.0237448\n",
      "[180]\ttraining's multi_logloss: 0.0228127\n",
      "[190]\ttraining's multi_logloss: 0.0220242\n",
      "[200]\ttraining's multi_logloss: 0.0213079\n",
      "[210]\ttraining's multi_logloss: 0.0206505\n",
      "[220]\ttraining's multi_logloss: 0.0200201\n",
      "[230]\ttraining's multi_logloss: 0.0193824\n",
      "***********第17次抽样***********\n",
      "负样本抽样数量：\n",
      "(31287, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.525822 + 0.000875438\n",
      "[40]\tcv_agg's multi_logloss: 0.204178 + 0.00116844\n",
      "[60]\tcv_agg's multi_logloss: 0.0938613 + 0.00144475\n",
      "[80]\tcv_agg's multi_logloss: 0.0544061 + 0.00165794\n",
      "[100]\tcv_agg's multi_logloss: 0.0402232 + 0.00181779\n",
      "[120]\tcv_agg's multi_logloss: 0.0351633 + 0.00200166\n",
      "[140]\tcv_agg's multi_logloss: 0.0334226 + 0.00210891\n",
      "[160]\tcv_agg's multi_logloss: 0.032709 + 0.00218732\n",
      "[180]\tcv_agg's multi_logloss: 0.0323655 + 0.00226476\n",
      "[200]\tcv_agg's multi_logloss: 0.0321986 + 0.00234771\n",
      "[220]\tcv_agg's multi_logloss: 0.0321447 + 0.00242087\n",
      "[240]\tcv_agg's multi_logloss: 0.0321075 + 0.00247528\n",
      "[260]\tcv_agg's multi_logloss: 0.0321321 + 0.00255398\n",
      "最优结果0.0321007795563126\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395, 0.0338477240919984, 0.031426275528943004, 0.032598245507739744, 0.033797054803081775, 0.03909386747168482, 0.044816453327352526, 0.02494793669885238, 0.03620192269369673, 0.0321007795563126]\n",
      "[10]\ttraining's multi_logloss: 0.904076\n",
      "[20]\ttraining's multi_logloss: 0.52458\n",
      "[30]\ttraining's multi_logloss: 0.31989\n",
      "[40]\ttraining's multi_logloss: 0.202126\n",
      "[50]\ttraining's multi_logloss: 0.132546\n",
      "[60]\ttraining's multi_logloss: 0.0908724\n",
      "[70]\ttraining's multi_logloss: 0.0657005\n",
      "[80]\ttraining's multi_logloss: 0.0503471\n",
      "[90]\ttraining's multi_logloss: 0.0408198\n",
      "[100]\ttraining's multi_logloss: 0.0347626\n",
      "[110]\ttraining's multi_logloss: 0.0308042\n",
      "[120]\ttraining's multi_logloss: 0.0280866\n",
      "[130]\ttraining's multi_logloss: 0.0261964\n",
      "[140]\ttraining's multi_logloss: 0.0247492\n",
      "[150]\ttraining's multi_logloss: 0.0236025\n",
      "[160]\ttraining's multi_logloss: 0.0225431\n",
      "[170]\ttraining's multi_logloss: 0.0216859\n",
      "[180]\ttraining's multi_logloss: 0.0208644\n",
      "[190]\ttraining's multi_logloss: 0.0201208\n",
      "[200]\ttraining's multi_logloss: 0.019455\n",
      "[210]\ttraining's multi_logloss: 0.0188418\n",
      "[220]\ttraining's multi_logloss: 0.0182667\n",
      "[230]\ttraining's multi_logloss: 0.017734\n",
      "[240]\ttraining's multi_logloss: 0.0172401\n",
      "[250]\ttraining's multi_logloss: 0.0167419\n",
      "[260]\ttraining's multi_logloss: 0.0163024\n",
      "***********第18次抽样***********\n",
      "负样本抽样数量：\n",
      "(21786, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.53241 + 0.00158128\n",
      "[40]\tcv_agg's multi_logloss: 0.212087 + 0.002107\n",
      "[60]\tcv_agg's multi_logloss: 0.102389 + 0.00255864\n",
      "[80]\tcv_agg's multi_logloss: 0.0632344 + 0.00284857\n",
      "[100]\tcv_agg's multi_logloss: 0.0492169 + 0.00317932\n",
      "[120]\tcv_agg's multi_logloss: 0.0441495 + 0.00354582\n",
      "[140]\tcv_agg's multi_logloss: 0.0425082 + 0.00370484\n",
      "[160]\tcv_agg's multi_logloss: 0.0418982 + 0.00385256\n",
      "[180]\tcv_agg's multi_logloss: 0.0416696 + 0.00391188\n",
      "[200]\tcv_agg's multi_logloss: 0.0415183 + 0.00387658\n",
      "[220]\tcv_agg's multi_logloss: 0.0414579 + 0.00390633\n",
      "[240]\tcv_agg's multi_logloss: 0.0414616 + 0.00394289\n",
      "最优结果0.04142017891137314\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395, 0.0338477240919984, 0.031426275528943004, 0.032598245507739744, 0.033797054803081775, 0.03909386747168482, 0.044816453327352526, 0.02494793669885238, 0.03620192269369673, 0.0321007795563126, 0.04142017891137314]\n",
      "[10]\ttraining's multi_logloss: 0.908354\n",
      "[20]\ttraining's multi_logloss: 0.530396\n",
      "[30]\ttraining's multi_logloss: 0.326383\n",
      "[40]\ttraining's multi_logloss: 0.208941\n",
      "[50]\ttraining's multi_logloss: 0.139613\n",
      "[60]\ttraining's multi_logloss: 0.0980059\n",
      "[70]\ttraining's multi_logloss: 0.0728024\n",
      "[80]\ttraining's multi_logloss: 0.057409\n",
      "[90]\ttraining's multi_logloss: 0.0477608\n",
      "[100]\ttraining's multi_logloss: 0.0415744\n",
      "[110]\ttraining's multi_logloss: 0.0373681\n",
      "[120]\ttraining's multi_logloss: 0.0344014\n",
      "[130]\ttraining's multi_logloss: 0.0322024\n",
      "[140]\ttraining's multi_logloss: 0.0304616\n",
      "[150]\ttraining's multi_logloss: 0.0290069\n",
      "[160]\ttraining's multi_logloss: 0.0278285\n",
      "[170]\ttraining's multi_logloss: 0.0267114\n",
      "[180]\ttraining's multi_logloss: 0.0257451\n",
      "[190]\ttraining's multi_logloss: 0.0248012\n",
      "[200]\ttraining's multi_logloss: 0.023952\n",
      "[210]\ttraining's multi_logloss: 0.0231703\n",
      "[220]\ttraining's multi_logloss: 0.02244\n",
      "[230]\ttraining's multi_logloss: 0.0217381\n",
      "[240]\ttraining's multi_logloss: 0.021082\n",
      "***********第19次抽样***********\n",
      "负样本抽样数量：\n",
      "(35614, 1259)\n",
      "[20]\tcv_agg's multi_logloss: 0.523355 + 0.00140788\n",
      "[40]\tcv_agg's multi_logloss: 0.200993 + 0.00213006\n",
      "[60]\tcv_agg's multi_logloss: 0.0903128 + 0.0027884\n",
      "[80]\tcv_agg's multi_logloss: 0.0507242 + 0.00325393\n",
      "[100]\tcv_agg's multi_logloss: 0.0364727 + 0.00353449\n",
      "[120]\tcv_agg's multi_logloss: 0.0313328 + 0.0037522\n",
      "[140]\tcv_agg's multi_logloss: 0.0295329 + 0.00384159\n",
      "[160]\tcv_agg's multi_logloss: 0.0288742 + 0.00387806\n",
      "[180]\tcv_agg's multi_logloss: 0.0286719 + 0.00397442\n",
      "[200]\tcv_agg's multi_logloss: 0.0285673 + 0.00402133\n",
      "[220]\tcv_agg's multi_logloss: 0.0284972 + 0.0040693\n",
      "[240]\tcv_agg's multi_logloss: 0.0284242 + 0.00410211\n",
      "[260]\tcv_agg's multi_logloss: 0.028428 + 0.00413084\n",
      "[280]\tcv_agg's multi_logloss: 0.0284193 + 0.00418684\n",
      "最优结果0.028409596713563145\n",
      "总loss[0.02694491490357993, 0.04357055898915886, 0.025386178466265685, 0.03944566832825077, 0.039703902139247335, 0.029588663806432925, 0.027776737458289312, 0.036310787258282566, 0.040622262484798395, 0.0338477240919984, 0.031426275528943004, 0.032598245507739744, 0.033797054803081775, 0.03909386747168482, 0.044816453327352526, 0.02494793669885238, 0.03620192269369673, 0.0321007795563126, 0.04142017891137314, 0.028409596713563145]\n",
      "[10]\ttraining's multi_logloss: 0.90245\n",
      "[20]\ttraining's multi_logloss: 0.522301\n",
      "[30]\ttraining's multi_logloss: 0.317198\n",
      "[40]\ttraining's multi_logloss: 0.19913\n",
      "[50]\ttraining's multi_logloss: 0.129418\n",
      "[60]\ttraining's multi_logloss: 0.0876505\n",
      "[70]\ttraining's multi_logloss: 0.0624426\n",
      "[80]\ttraining's multi_logloss: 0.0471389\n",
      "[90]\ttraining's multi_logloss: 0.0376628\n",
      "[100]\ttraining's multi_logloss: 0.031683\n",
      "[110]\ttraining's multi_logloss: 0.0277944\n",
      "[120]\ttraining's multi_logloss: 0.0251553\n",
      "[130]\ttraining's multi_logloss: 0.0233004\n",
      "[140]\ttraining's multi_logloss: 0.0219358\n",
      "[150]\ttraining's multi_logloss: 0.0208705\n",
      "[160]\ttraining's multi_logloss: 0.0199593\n",
      "[170]\ttraining's multi_logloss: 0.0191212\n",
      "[180]\ttraining's multi_logloss: 0.0184254\n",
      "[190]\ttraining's multi_logloss: 0.0177774\n",
      "[200]\ttraining's multi_logloss: 0.0171492\n",
      "[210]\ttraining's multi_logloss: 0.0165911\n",
      "[220]\ttraining's multi_logloss: 0.0161086\n",
      "[230]\ttraining's multi_logloss: 0.0156588\n",
      "[240]\ttraining's multi_logloss: 0.0152158\n",
      "[250]\ttraining's multi_logloss: 0.0148008\n",
      "[260]\ttraining's multi_logloss: 0.0144064\n",
      "[270]\ttraining's multi_logloss: 0.0140222\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "pred = pd.DataFrame()\n",
    "for i in range(20):\n",
    "    print('***********第%s次抽样***********'%i)\n",
    "    x0_part = x0.copy()\n",
    "    x0_part.index = range(x0_part.shape[0])\n",
    "    x1.index = range(x1.shape[0])\n",
    "    permutation = np.random.permutation(x0_part.shape[0])\n",
    "    \n",
    "    random.seed(i)\n",
    "    ratio = int(x0.shape[0]*random.uniform(1.5, 4)/10)\n",
    "\n",
    "    select = x0_part.iloc[permutation[:ratio], :]\n",
    "    \n",
    "    print('负样本抽样数量：')\n",
    "    print(select.shape)\n",
    "    \n",
    "    data_new = pd.concat([x1, select], axis = 0)\n",
    "    \n",
    "    permutation = np.random.permutation(data_new.shape[0])\n",
    "\n",
    "    data_new = data_new.iloc[permutation, :]\n",
    "    \n",
    "    label_new = data_new['label'].values\n",
    "    \n",
    "    data_new = data_new.drop(['label', 'id'], axis = 1)\n",
    "    \n",
    "    dtrain = lgb.Dataset(data_new, label=label_new)\n",
    "\n",
    "    hist = lgb.cv(params, dtrain, num_boost_round = 1000, verbose_eval=20, early_stopping_rounds=30, stratified=False)\n",
    "    \n",
    "    print('最优结果%s'%hist['multi_logloss-mean'][-1])\n",
    "    loss_list.append(hist['multi_logloss-mean'][-1])\n",
    "    print('总loss%s'%loss_list)\n",
    "    model = lgb.train(params=params, train_set=dtrain, num_boost_round=int(1.1*len(hist['multi_logloss-mean'])), verbose_eval=10,\\\n",
    "                      valid_sets=dtrain)\n",
    "    \n",
    "    pred_temp = pd.DataFrame(model.predict(test_FE))\n",
    "    \n",
    "    pred = pd.concat([pred, pred_temp], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame(sub_sample.file_id)\n",
    "\n",
    "p['prob0'] = pred[0].mean(axis = 1)\n",
    "p['prob1'] = pred[1].mean(axis = 1)\n",
    "p['prob2'] = pred[2].mean(axis = 1)\n",
    "p['prob3'] = pred[3].mean(axis = 1)\n",
    "p['prob4'] = pred[4].mean(axis = 1)\n",
    "p['prob5'] = pred[5].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_id    26546.000000\n",
       "prob0          0.899607\n",
       "prob1          0.007780\n",
       "prob2          0.014583\n",
       "prob3          0.012210\n",
       "prob4          0.001655\n",
       "prob5          0.064165\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.to_csv('/home/libo/Security/sub/8.24_bagging_v1.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_list_use = ['lgb_train_v1.csv', 'lgb_train_v2.csv', 'lgb_train_v3.csv']\\\n",
    "+ ['xgb_train_v1.csv', 'xgb_train_v3.csv']\\\n",
    "+['NN_train_v1.csv', 'NN_train_v2.csv', 'AE_type1_train_v1.csv', 'AE_type1_train_v3.csv']\\\n",
    "+['CNN_api_train_v1.csv'] + ['dabai_train.csv']\n",
    "\n",
    "test_list_use = ['lgb_test_v1.csv', 'lgb_test_v2.csv', 'lgb_test_v3.csv'] + ['xgb_test_v1.csv', 'xgb_test_v3.csv']\\\n",
    "+['NN_test_v1.csv', 'NN_test_v2.csv', 'AE_type1_test_v1.csv', 'AE_type1_test_v3.csv']\\\n",
    "+['CNN_api_test_v1.csv'] + ['dabai_test.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgb_train_v1.csv\n",
      "(116624, 6)\n",
      "lgb_train_v2.csv\n",
      "(116624, 12)\n",
      "lgb_train_v3.csv\n",
      "(116624, 18)\n",
      "xgb_train_v1.csv\n",
      "(116624, 24)\n",
      "xgb_train_v3.csv\n",
      "(116624, 30)\n",
      "NN_train_v1.csv\n",
      "(116624, 36)\n",
      "NN_train_v2.csv\n",
      "(116624, 42)\n",
      "AE_type1_train_v1.csv\n",
      "(116624, 48)\n",
      "AE_type1_train_v3.csv\n",
      "(116624, 54)\n",
      "CNN_api_train_v1.csv\n",
      "(116624, 60)\n",
      "dabai_train.csv\n",
      "(116624, 150)\n"
     ]
    }
   ],
   "source": [
    "train_meta = pd.DataFrame()\n",
    "for i in train_list_use:\n",
    "    print(i)\n",
    "    train_meta = pd.concat([train_meta, pd.read_csv(i)], axis = 1)\n",
    "    print(train_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lgb_test_v1.csv\n",
      "(53093, 6)\n",
      "lgb_test_v2.csv\n",
      "(53093, 12)\n",
      "lgb_test_v3.csv\n",
      "(53093, 18)\n",
      "xgb_test_v1.csv\n",
      "(53093, 24)\n",
      "xgb_test_v3.csv\n",
      "(53093, 30)\n",
      "NN_test_v1.csv\n",
      "(53093, 36)\n",
      "NN_test_v2.csv\n",
      "(53093, 42)\n",
      "AE_type1_test_v1.csv\n",
      "(53093, 48)\n",
      "AE_type1_test_v3.csv\n",
      "(53093, 54)\n",
      "CNN_api_test_v1.csv\n",
      "(53093, 60)\n",
      "dabai_test.csv\n",
      "(53093, 150)\n"
     ]
    }
   ],
   "source": [
    "test_meta = pd.DataFrame()\n",
    "for i in test_list_use:\n",
    "    print(i)\n",
    "    try:\n",
    "        x = pd.read_csv(i).drop('file_id', axis = 1)\n",
    "    except:\n",
    "        x = pd.read_csv(i)\n",
    "    test_meta = pd.concat([test_meta, x], axis = 1)\n",
    "    print(test_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta = test_meta[train_meta.columns]\n",
    "test_meta.columns == train_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116624, 150), (53093, 150))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.shape, test_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_FE = train_meta.copy()\n",
    "test_FE = test_meta.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116624, 150), (53093, 150))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_FE.shape, test_FE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_FE['label'] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((111545, 151), (5079, 151))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = train_FE[train_FE['label'] == 0]\n",
    "\n",
    "x1 = train_FE[train_FE['label'] != 0]\n",
    "\n",
    "x0.shape, x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = pd.read_csv('/home/libo/Security/index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multiclass\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.05,\n",
    "          \"num_leaves\": 3,\n",
    "          # \"max_bin\": 128,\n",
    "          \"feature_fraction\": 0.85,\n",
    "#           \"min_child_samples\": 10,\n",
    "#           \"min_child_weight\": 150,\n",
    "#           \"min_split_gain\": 0,\n",
    "          \"subsample\": 0.85,\n",
    "          #'metric':'logloss',\n",
    "#            'lambda_l1':0.02,\n",
    "#            'lambda_l2':0.02,\n",
    "          'seed':666,\n",
    "          'num_class':6\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********第0次抽样***********\n",
      "负样本抽样数量：\n",
      "(40279, 151)\n",
      "[20]\tcv_agg's multi_logloss: 0.522343 + 0.00170798\n",
      "[40]\tcv_agg's multi_logloss: 0.199545 + 0.0025768\n",
      "[60]\tcv_agg's multi_logloss: 0.0886813 + 0.00301587\n",
      "[80]\tcv_agg's multi_logloss: 0.0489693 + 0.00331693\n",
      "[100]\tcv_agg's multi_logloss: 0.0347014 + 0.00358443\n",
      "[120]\tcv_agg's multi_logloss: 0.0295438 + 0.00380038\n",
      "[140]\tcv_agg's multi_logloss: 0.0276366 + 0.00389922\n",
      "[160]\tcv_agg's multi_logloss: 0.0269312 + 0.00398551\n",
      "[180]\tcv_agg's multi_logloss: 0.0266648 + 0.00406272\n",
      "[200]\tcv_agg's multi_logloss: 0.026506 + 0.00405759\n",
      "[220]\tcv_agg's multi_logloss: 0.0264162 + 0.00404613\n",
      "[240]\tcv_agg's multi_logloss: 0.0263772 + 0.00404248\n",
      "[260]\tcv_agg's multi_logloss: 0.0263699 + 0.00403675\n",
      "[280]\tcv_agg's multi_logloss: 0.0263662 + 0.00404325\n",
      "最优结果0.02635044486702374\n",
      "总loss[0.02635044486702374]\n",
      "[10]\ttraining's multi_logloss: 0.901764\n",
      "[20]\ttraining's multi_logloss: 0.521359\n",
      "[30]\ttraining's multi_logloss: 0.316132\n",
      "[40]\ttraining's multi_logloss: 0.197981\n",
      "[50]\ttraining's multi_logloss: 0.128233\n",
      "[60]\ttraining's multi_logloss: 0.0865634\n",
      "[70]\ttraining's multi_logloss: 0.0614441\n",
      "[80]\ttraining's multi_logloss: 0.0461924\n",
      "[90]\ttraining's multi_logloss: 0.0368665\n",
      "[100]\ttraining's multi_logloss: 0.0310593\n",
      "[110]\ttraining's multi_logloss: 0.0274102\n",
      "[120]\ttraining's multi_logloss: 0.02501\n",
      "[130]\ttraining's multi_logloss: 0.0234052\n",
      "[140]\ttraining's multi_logloss: 0.0221371\n",
      "[150]\ttraining's multi_logloss: 0.0212751\n",
      "[160]\ttraining's multi_logloss: 0.0205743\n",
      "[170]\ttraining's multi_logloss: 0.0199522\n",
      "[180]\ttraining's multi_logloss: 0.0193659\n",
      "[190]\ttraining's multi_logloss: 0.0188821\n",
      "[200]\ttraining's multi_logloss: 0.0184546\n",
      "[210]\ttraining's multi_logloss: 0.018063\n",
      "[220]\ttraining's multi_logloss: 0.0176881\n",
      "[230]\ttraining's multi_logloss: 0.0173072\n",
      "[240]\ttraining's multi_logloss: 0.0169658\n",
      "[250]\ttraining's multi_logloss: 0.0166458\n",
      "[260]\ttraining's multi_logloss: 0.0163337\n",
      "[270]\ttraining's multi_logloss: 0.0160207\n",
      "[280]\ttraining's multi_logloss: 0.0157256\n",
      "[290]\ttraining's multi_logloss: 0.0154493\n",
      "***********第1次抽样***********\n",
      "负样本抽样数量：\n",
      "(20478, 151)\n",
      "[20]\tcv_agg's multi_logloss: 0.533562 + 0.00145404\n",
      "[40]\tcv_agg's multi_logloss: 0.213564 + 0.0022388\n",
      "[60]\tcv_agg's multi_logloss: 0.103922 + 0.00276438\n",
      "[80]\tcv_agg's multi_logloss: 0.064911 + 0.0032163\n",
      "[100]\tcv_agg's multi_logloss: 0.0509823 + 0.00359966\n",
      "[120]\tcv_agg's multi_logloss: 0.0459431 + 0.00381911\n",
      "[140]\tcv_agg's multi_logloss: 0.0439456 + 0.00386529\n",
      "[160]\tcv_agg's multi_logloss: 0.0430411 + 0.00393994\n",
      "[180]\tcv_agg's multi_logloss: 0.0426272 + 0.00402758\n",
      "[200]\tcv_agg's multi_logloss: 0.0424104 + 0.00410517\n",
      "[220]\tcv_agg's multi_logloss: 0.0423048 + 0.00417905\n",
      "[240]\tcv_agg's multi_logloss: 0.0422544 + 0.00414636\n",
      "[260]\tcv_agg's multi_logloss: 0.0422834 + 0.00417412\n",
      "最优结果0.04224954423794289\n",
      "总loss[0.02635044486702374, 0.04224954423794289]\n",
      "[10]\ttraining's multi_logloss: 0.909141\n",
      "[20]\ttraining's multi_logloss: 0.531549\n",
      "[30]\ttraining's multi_logloss: 0.327865\n",
      "[40]\ttraining's multi_logloss: 0.210614\n",
      "[50]\ttraining's multi_logloss: 0.141373\n",
      "[60]\ttraining's multi_logloss: 0.0999616\n",
      "[70]\ttraining's multi_logloss: 0.0748763\n",
      "[80]\ttraining's multi_logloss: 0.0595888\n",
      "[90]\ttraining's multi_logloss: 0.0500849\n",
      "[100]\ttraining's multi_logloss: 0.0440447\n",
      "[110]\ttraining's multi_logloss: 0.0401164\n",
      "[120]\ttraining's multi_logloss: 0.0374486\n",
      "[130]\ttraining's multi_logloss: 0.0354875\n",
      "[140]\ttraining's multi_logloss: 0.0339915\n",
      "[150]\ttraining's multi_logloss: 0.0326378\n",
      "[160]\ttraining's multi_logloss: 0.0315765\n",
      "[170]\ttraining's multi_logloss: 0.0305208\n",
      "[180]\ttraining's multi_logloss: 0.0296719\n",
      "[190]\ttraining's multi_logloss: 0.0289051\n",
      "[200]\ttraining's multi_logloss: 0.0281936\n",
      "[210]\ttraining's multi_logloss: 0.027498\n",
      "[220]\ttraining's multi_logloss: 0.026902\n",
      "[230]\ttraining's multi_logloss: 0.0263166\n",
      "[240]\ttraining's multi_logloss: 0.0257612\n",
      "[250]\ttraining's multi_logloss: 0.0252039\n",
      "[260]\ttraining's multi_logloss: 0.0246622\n",
      "[20]\tcv_agg's multi_logloss: 0.53054 + 0.0019181\n",
      "[40]\tcv_agg's multi_logloss: 0.209996 + 0.00329297\n",
      "[60]\tcv_agg's multi_logloss: 0.100194 + 0.00431847\n",
      "[80]\tcv_agg's multi_logloss: 0.0610491 + 0.00514789\n",
      "[100]\tcv_agg's multi_logloss: 0.0470052 + 0.0056803\n",
      "[120]\tcv_agg's multi_logloss: 0.0419004 + 0.0061138\n",
      "[140]\tcv_agg's multi_logloss: 0.0400013 + 0.00649011\n",
      "[160]\tcv_agg's multi_logloss: 0.039263 + 0.00668477\n",
      "[180]\tcv_agg's multi_logloss: 0.0389093 + 0.00675159\n",
      "[200]\tcv_agg's multi_logloss: 0.0387386 + 0.00676633\n",
      "[220]\tcv_agg's multi_logloss: 0.038715 + 0.00679143\n",
      "[240]\tcv_agg's multi_logloss: 0.0387768 + 0.00690244\n",
      "最优结果0.03869013209147135\n",
      "总loss[0.02635044486702374, 0.04224954423794289, 0.02496861360220759, 0.038405853375496915, 0.04021016187754299, 0.030782232055128634, 0.026567243236387907, 0.03704286699892332, 0.03869013209147135]\n",
      "[10]\ttraining's multi_logloss: 0.907396\n",
      "[20]\ttraining's multi_logloss: 0.529123\n",
      "[30]\ttraining's multi_logloss: 0.325055\n",
      "[40]\ttraining's multi_logloss: 0.207566\n",
      "[50]\ttraining's multi_logloss: 0.13816\n",
      "[60]\ttraining's multi_logloss: 0.0966451\n",
      "[70]\ttraining's multi_logloss: 0.0715594\n",
      "[80]\ttraining's multi_logloss: 0.0563141\n",
      "[90]\ttraining's multi_logloss: 0.0468815\n",
      "[100]\ttraining's multi_logloss: 0.0409011\n",
      "[110]\ttraining's multi_logloss: 0.0369893\n",
      "[120]\ttraining's multi_logloss: 0.0343802\n",
      "[130]\ttraining's multi_logloss: 0.0324057\n",
      "[140]\ttraining's multi_logloss: 0.0309236\n",
      "[150]\ttraining's multi_logloss: 0.0298281\n",
      "[160]\ttraining's multi_logloss: 0.0287483\n",
      "[170]\ttraining's multi_logloss: 0.0277591\n",
      "[180]\ttraining's multi_logloss: 0.0269756\n",
      "[190]\ttraining's multi_logloss: 0.0262985\n",
      "[200]\ttraining's multi_logloss: 0.025646\n",
      "[210]\ttraining's multi_logloss: 0.0249802\n",
      "[220]\ttraining's multi_logloss: 0.0244187\n",
      "[230]\ttraining's multi_logloss: 0.0238869\n",
      "***********第9次抽样***********\n",
      "负样本抽样数量：\n",
      "(29643, 151)\n",
      "[20]\tcv_agg's multi_logloss: 0.526531 + 0.00106083\n",
      "[40]\tcv_agg's multi_logloss: 0.204846 + 0.00141184\n",
      "[60]\tcv_agg's multi_logloss: 0.0944959 + 0.00154831\n",
      "[80]\tcv_agg's multi_logloss: 0.0550705 + 0.00166181\n",
      "[100]\tcv_agg's multi_logloss: 0.0409226 + 0.00183514\n",
      "[120]\tcv_agg's multi_logloss: 0.0357832 + 0.00202912\n",
      "[140]\tcv_agg's multi_logloss: 0.0338979 + 0.00212777\n",
      "[160]\tcv_agg's multi_logloss: 0.0331187 + 0.00216585\n",
      "[180]\tcv_agg's multi_logloss: 0.0328015 + 0.00220426\n",
      "[200]\tcv_agg's multi_logloss: 0.0326093 + 0.00213104\n",
      "[220]\tcv_agg's multi_logloss: 0.0325323 + 0.0021529\n",
      "[240]\tcv_agg's multi_logloss: 0.0325354 + 0.00216608\n",
      "最优结果0.032523221827667606\n",
      "总loss[0.02635044486702374, 0.04224954423794289, 0.02496861360220759, 0.038405853375496915, 0.04021016187754299, 0.030782232055128634, 0.026567243236387907, 0.03704286699892332, 0.03869013209147135, 0.032523221827667606]\n",
      "[10]\ttraining's multi_logloss: 0.904776\n",
      "[20]\ttraining's multi_logloss: 0.525365\n",
      "[30]\ttraining's multi_logloss: 0.320711\n",
      "[40]\ttraining's multi_logloss: 0.202949\n",
      "[50]\ttraining's multi_logloss: 0.13349\n",
      "[60]\ttraining's multi_logloss: 0.0919247\n",
      "[70]\ttraining's multi_logloss: 0.0668201\n",
      "[80]\ttraining's multi_logloss: 0.0516038\n",
      "[90]\ttraining's multi_logloss: 0.0422214\n",
      "[100]\ttraining's multi_logloss: 0.0363524\n",
      "[110]\ttraining's multi_logloss: 0.0325255\n",
      "[120]\ttraining's multi_logloss: 0.0300244\n",
      "[130]\ttraining's multi_logloss: 0.0282478\n",
      "[140]\ttraining's multi_logloss: 0.0267831\n",
      "[150]\ttraining's multi_logloss: 0.0257759\n",
      "[160]\ttraining's multi_logloss: 0.0248757\n",
      "[170]\ttraining's multi_logloss: 0.0240904\n",
      "[180]\ttraining's multi_logloss: 0.0233913\n",
      "[190]\ttraining's multi_logloss: 0.0227937\n",
      "[200]\ttraining's multi_logloss: 0.0222566\n",
      "[210]\ttraining's multi_logloss: 0.0217339\n",
      "[220]\ttraining's multi_logloss: 0.02128\n",
      "[230]\ttraining's multi_logloss: 0.0208301\n",
      "***********第10次抽样***********\n",
      "负样本抽样数量：\n",
      "(32666, 151)\n",
      "[20]\tcv_agg's multi_logloss: 0.524839 + 0.000465855\n",
      "[40]\tcv_agg's multi_logloss: 0.203015 + 0.000762072\n",
      "[60]\tcv_agg's multi_logloss: 0.0925367 + 0.00100683\n",
      "[80]\tcv_agg's multi_logloss: 0.0530834 + 0.001006\n",
      "[100]\tcv_agg's multi_logloss: 0.038864 + 0.00104702\n",
      "[120]\tcv_agg's multi_logloss: 0.033777 + 0.0010565\n",
      "[140]\tcv_agg's multi_logloss: 0.0318507 + 0.0010975\n",
      "[160]\tcv_agg's multi_logloss: 0.0311023 + 0.00112599\n",
      "[180]\tcv_agg's multi_logloss: 0.0308602 + 0.00114623\n",
      "[200]\tcv_agg's multi_logloss: 0.0307261 + 0.00111115\n",
      "[220]\tcv_agg's multi_logloss: 0.0306842 + 0.0010715\n",
      "[240]\tcv_agg's multi_logloss: 0.030672 + 0.00103577\n",
      "[260]\tcv_agg's multi_logloss: 0.0306804 + 0.000991581\n",
      "最优结果0.030660186888467528\n",
      "总loss[0.02635044486702374, 0.04224954423794289, 0.02496861360220759, 0.038405853375496915, 0.04021016187754299, 0.030782232055128634, 0.026567243236387907, 0.03704286699892332, 0.03869013209147135, 0.032523221827667606, 0.030660186888467528]\n",
      "[10]\ttraining's multi_logloss: 0.903457\n",
      "[20]\ttraining's multi_logloss: 0.52377\n",
      "[30]\ttraining's multi_logloss: 0.318932\n",
      "[40]\ttraining's multi_logloss: 0.201157\n",
      "[50]\ttraining's multi_logloss: 0.1316\n",
      "[60]\ttraining's multi_logloss: 0.0899843\n",
      "[70]\ttraining's multi_logloss: 0.0648801\n",
      "[80]\ttraining's multi_logloss: 0.0496487\n",
      "[90]\ttraining's multi_logloss: 0.0402992\n",
      "[100]\ttraining's multi_logloss: 0.0344335\n",
      "[110]\ttraining's multi_logloss: 0.0306824\n",
      "[120]\ttraining's multi_logloss: 0.0282154\n",
      "[130]\ttraining's multi_logloss: 0.0264351\n",
      "[140]\ttraining's multi_logloss: 0.0250636\n",
      "[150]\ttraining's multi_logloss: 0.0241051\n",
      "[160]\ttraining's multi_logloss: 0.0232957\n",
      "[170]\ttraining's multi_logloss: 0.0226069\n",
      "[180]\ttraining's multi_logloss: 0.0218807\n",
      "[190]\ttraining's multi_logloss: 0.0213016\n",
      "[200]\ttraining's multi_logloss: 0.0208029\n",
      "[210]\ttraining's multi_logloss: 0.0203138\n",
      "[220]\ttraining's multi_logloss: 0.0198773\n",
      "[230]\ttraining's multi_logloss: 0.0194481\n",
      "[240]\ttraining's multi_logloss: 0.0190368\n",
      "[250]\ttraining's multi_logloss: 0.0186676\n",
      "[260]\ttraining's multi_logloss: 0.0183296\n",
      "***********第11次抽样***********\n",
      "负样本抽样数量：\n",
      "(29346, 151)\n",
      "[20]\tcv_agg's multi_logloss: 0.526877 + 0.00139962\n",
      "[40]\tcv_agg's multi_logloss: 0.205086 + 0.00201601\n",
      "[60]\tcv_agg's multi_logloss: 0.0947455 + 0.00224285\n",
      "[80]\tcv_agg's multi_logloss: 0.0552766 + 0.00238091\n",
      "[100]\tcv_agg's multi_logloss: 0.0409876 + 0.00231214\n",
      "[120]\tcv_agg's multi_logloss: 0.0358022 + 0.00224881\n",
      "[140]\tcv_agg's multi_logloss: 0.0338783 + 0.00224538\n",
      "[160]\tcv_agg's multi_logloss: 0.033153 + 0.00220294\n",
      "[180]\tcv_agg's multi_logloss: 0.0328601 + 0.00213591\n",
      "[200]\tcv_agg's multi_logloss: 0.0327227 + 0.00207211\n",
      "[220]\tcv_agg's multi_logloss: 0.0326632 + 0.00206366\n",
      "[240]\tcv_agg's multi_logloss: 0.0326179 + 0.00204135\n",
      "[260]\tcv_agg's multi_logloss: 0.032605 + 0.00204071\n",
      "[280]\tcv_agg's multi_logloss: 0.0326145 + 0.00203841\n",
      "最优结果0.03259822135714373\n",
      "总loss[0.02635044486702374, 0.04224954423794289, 0.02496861360220759, 0.038405853375496915, 0.04021016187754299, 0.030782232055128634, 0.026567243236387907, 0.03704286699892332, 0.03869013209147135, 0.032523221827667606, 0.030660186888467528, 0.03259822135714373]\n",
      "[10]\ttraining's multi_logloss: 0.904958\n",
      "[20]\ttraining's multi_logloss: 0.525569\n",
      "[30]\ttraining's multi_logloss: 0.320925\n",
      "[40]\ttraining's multi_logloss: 0.203133\n",
      "[50]\ttraining's multi_logloss: 0.133631\n",
      "[60]\ttraining's multi_logloss: 0.0920479\n",
      "[70]\ttraining's multi_logloss: 0.0669335\n",
      "[80]\ttraining's multi_logloss: 0.051671\n",
      "[90]\ttraining's multi_logloss: 0.0422742\n",
      "[100]\ttraining's multi_logloss: 0.0364127\n",
      "[110]\ttraining's multi_logloss: 0.0325957\n",
      "[120]\ttraining's multi_logloss: 0.0300545\n",
      "[130]\ttraining's multi_logloss: 0.028252\n",
      "[140]\ttraining's multi_logloss: 0.0269306\n",
      "[150]\ttraining's multi_logloss: 0.025823\n",
      "[160]\ttraining's multi_logloss: 0.0249902\n",
      "[170]\ttraining's multi_logloss: 0.0241184\n",
      "[180]\ttraining's multi_logloss: 0.0234309\n",
      "[190]\ttraining's multi_logloss: 0.0227989\n",
      "[200]\ttraining's multi_logloss: 0.0222447\n",
      "[210]\ttraining's multi_logloss: 0.0217284\n",
      "[220]\ttraining's multi_logloss: 0.021251\n",
      "[230]\ttraining's multi_logloss: 0.0208078\n",
      "[240]\ttraining's multi_logloss: 0.0203856\n",
      "[250]\ttraining's multi_logloss: 0.0199666\n",
      "[260]\ttraining's multi_logloss: 0.019569\n",
      "[270]\ttraining's multi_logloss: 0.0191943\n",
      "[280]\ttraining's multi_logloss: 0.0188449\n",
      "[290]\ttraining's multi_logloss: 0.0185038\n",
      "***********第12次抽样***********\n",
      "负样本抽样数量：\n",
      "(29965, 151)\n",
      "[20]\tcv_agg's multi_logloss: 0.526648 + 0.00153126\n",
      "[40]\tcv_agg's multi_logloss: 0.204964 + 0.00222711\n",
      "[60]\tcv_agg's multi_logloss: 0.0947099 + 0.00263495\n",
      "[80]\tcv_agg's multi_logloss: 0.0552853 + 0.00305221\n",
      "[100]\tcv_agg's multi_logloss: 0.0410508 + 0.00317785\n",
      "[120]\tcv_agg's multi_logloss: 0.0358891 + 0.00323258\n",
      "[140]\tcv_agg's multi_logloss: 0.0339629 + 0.00315038\n",
      "[160]\tcv_agg's multi_logloss: 0.0332485 + 0.00311518\n",
      "[180]\tcv_agg's multi_logloss: 0.0329172 + 0.00307763\n",
      "[200]\tcv_agg's multi_logloss: 0.0327418 + 0.00304683\n",
      "[220]\tcv_agg's multi_logloss: 0.0326786 + 0.00308733\n",
      "[240]\tcv_agg's multi_logloss: 0.0326953 + 0.00309445\n",
      "最优结果0.03267079848840921\n",
      "总loss[0.02635044486702374, 0.04224954423794289, 0.02496861360220759, 0.038405853375496915, 0.04021016187754299, 0.030782232055128634, 0.026567243236387907, 0.03704286699892332, 0.03869013209147135, 0.032523221827667606, 0.030660186888467528, 0.03259822135714373, 0.03267079848840921]\n",
      "[10]\ttraining's multi_logloss: 0.904655\n",
      "[20]\ttraining's multi_logloss: 0.525308\n",
      "[30]\ttraining's multi_logloss: 0.320624\n",
      "[40]\ttraining's multi_logloss: 0.202841\n",
      "[50]\ttraining's multi_logloss: 0.133366\n",
      "[60]\ttraining's multi_logloss: 0.0917429\n",
      "[70]\ttraining's multi_logloss: 0.066639\n",
      "[80]\ttraining's multi_logloss: 0.051385\n",
      "[90]\ttraining's multi_logloss: 0.0419874\n",
      "[100]\ttraining's multi_logloss: 0.0361077\n",
      "[110]\ttraining's multi_logloss: 0.0323175\n",
      "[120]\ttraining's multi_logloss: 0.0298293\n",
      "[130]\ttraining's multi_logloss: 0.0280239\n",
      "[140]\ttraining's multi_logloss: 0.0266658\n",
      "[150]\ttraining's multi_logloss: 0.0256022\n",
      "[160]\ttraining's multi_logloss: 0.0247733\n",
      "[170]\ttraining's multi_logloss: 0.0239827\n",
      "[180]\ttraining's multi_logloss: 0.0232283\n",
      "[190]\ttraining's multi_logloss: 0.0225744\n",
      "[200]\ttraining's multi_logloss: 0.0219911\n",
      "[210]\ttraining's multi_logloss: 0.0214563\n",
      "[220]\ttraining's multi_logloss: 0.0209599\n",
      "[230]\ttraining's multi_logloss: 0.0205126\n",
      "[240]\ttraining's multi_logloss: 0.020096\n",
      "***********第13次抽样***********\n",
      "负样本抽样数量：\n",
      "(23954, 151)\n",
      "[20]\tcv_agg's multi_logloss: 0.529835 + 0.00196654\n",
      "[40]\tcv_agg's multi_logloss: 0.208845 + 0.00343382\n",
      "[60]\tcv_agg's multi_logloss: 0.0989217 + 0.00457901\n",
      "[80]\tcv_agg's multi_logloss: 0.0597778 + 0.00518386\n",
      "[100]\tcv_agg's multi_logloss: 0.0456113 + 0.00557115\n",
      "[120]\tcv_agg's multi_logloss: 0.0405502 + 0.00585351\n",
      "[140]\tcv_agg's multi_logloss: 0.0386376 + 0.00621093\n",
      "[160]\tcv_agg's multi_logloss: 0.0379231 + 0.0063891\n",
      "[180]\tcv_agg's multi_logloss: 0.03755 + 0.0064161\n",
      "[200]\tcv_agg's multi_logloss: 0.0373975 + 0.00647277\n",
      "[220]\tcv_agg's multi_logloss: 0.0373587 + 0.00651233\n",
      "[240]\tcv_agg's multi_logloss: 0.0373487 + 0.00657337\n",
      "最优结果0.03733905454451644\n",
      "总loss[0.02635044486702374, 0.04224954423794289, 0.02496861360220759, 0.038405853375496915, 0.04021016187754299, 0.030782232055128634, 0.026567243236387907, 0.03704286699892332, 0.03869013209147135, 0.032523221827667606, 0.030660186888467528, 0.03259822135714373, 0.03267079848840921, 0.03733905454451644]\n",
      "[10]\ttraining's multi_logloss: 0.906863\n",
      "[20]\ttraining's multi_logloss: 0.528259\n",
      "[30]\ttraining's multi_logloss: 0.324\n",
      "[40]\ttraining's multi_logloss: 0.206419\n",
      "[50]\ttraining's multi_logloss: 0.137014\n",
      "[60]\ttraining's multi_logloss: 0.0954627\n",
      "[70]\ttraining's multi_logloss: 0.0704046\n",
      "[80]\ttraining's multi_logloss: 0.0551552\n",
      "[90]\ttraining's multi_logloss: 0.0457072\n",
      "[100]\ttraining's multi_logloss: 0.039776\n",
      "[110]\ttraining's multi_logloss: 0.0359017\n",
      "[120]\ttraining's multi_logloss: 0.0332988\n",
      "[130]\ttraining's multi_logloss: 0.0313406\n",
      "[140]\ttraining's multi_logloss: 0.0298753\n",
      "[150]\ttraining's multi_logloss: 0.0287716\n",
      "[160]\ttraining's multi_logloss: 0.0278273\n",
      "[170]\ttraining's multi_logloss: 0.0269117\n",
      "[180]\ttraining's multi_logloss: 0.0261129\n",
      "[190]\ttraining's multi_logloss: 0.0254089\n",
      "[200]\ttraining's multi_logloss: 0.0247539\n",
      "[210]\ttraining's multi_logloss: 0.024164\n",
      "[220]\ttraining's multi_logloss: 0.0236439\n",
      "[230]\ttraining's multi_logloss: 0.0231586\n",
      "***********第14次抽样***********\n",
      "负样本抽样数量：\n",
      "(19710, 151)\n",
      "[20]\tcv_agg's multi_logloss: 0.535126 + 0.00175575\n",
      "[40]\tcv_agg's multi_logloss: 0.215725 + 0.00280052\n",
      "[60]\tcv_agg's multi_logloss: 0.106403 + 0.00361043\n",
      "[80]\tcv_agg's multi_logloss: 0.0675926 + 0.00427003\n",
      "[100]\tcv_agg's multi_logloss: 0.0538033 + 0.00468252\n",
      "[120]\tcv_agg's multi_logloss: 0.0489893 + 0.00497061\n",
      "[140]\tcv_agg's multi_logloss: 0.0472106 + 0.00511592\n",
      "[160]\tcv_agg's multi_logloss: 0.0465996 + 0.00515072\n",
      "[180]\tcv_agg's multi_logloss: 0.0463335 + 0.00525651\n",
      "[200]\tcv_agg's multi_logloss: 0.0461619 + 0.00537781\n",
      "[220]\tcv_agg's multi_logloss: 0.0461466 + 0.0054328\n",
      "[240]\tcv_agg's multi_logloss: 0.0461148 + 0.00542603\n",
      "[260]\tcv_agg's multi_logloss: 0.046207 + 0.00549113\n",
      "最优结果0.04609705101834301\n",
      "总loss[0.02635044486702374, 0.04224954423794289, 0.02496861360220759, 0.038405853375496915, 0.04021016187754299, 0.030782232055128634, 0.026567243236387907, 0.03704286699892332, 0.03869013209147135, 0.032523221827667606, 0.030660186888467528, 0.03259822135714373, 0.03267079848840921, 0.03733905454451644, 0.04609705101834301]\n",
      "[10]\ttraining's multi_logloss: 0.910416\n",
      "[20]\ttraining's multi_logloss: 0.533208\n",
      "[30]\ttraining's multi_logloss: 0.329582\n",
      "[40]\ttraining's multi_logloss: 0.212351\n",
      "[50]\ttraining's multi_logloss: 0.143234\n",
      "[60]\ttraining's multi_logloss: 0.101853\n",
      "[70]\ttraining's multi_logloss: 0.076821\n",
      "[80]\ttraining's multi_logloss: 0.0615497\n",
      "[90]\ttraining's multi_logloss: 0.0520239\n",
      "[100]\ttraining's multi_logloss: 0.0459691\n",
      "[110]\ttraining's multi_logloss: 0.0419897\n",
      "[120]\ttraining's multi_logloss: 0.0392231\n",
      "[130]\ttraining's multi_logloss: 0.0372302\n",
      "[140]\ttraining's multi_logloss: 0.0357107\n",
      "[150]\ttraining's multi_logloss: 0.0344607\n",
      "[160]\ttraining's multi_logloss: 0.0332982\n",
      "[170]\ttraining's multi_logloss: 0.0322424\n",
      "[180]\ttraining's multi_logloss: 0.0313396\n",
      "[190]\ttraining's multi_logloss: 0.030497\n",
      "[200]\ttraining's multi_logloss: 0.0297132\n",
      "[210]\ttraining's multi_logloss: 0.0290359\n",
      "[220]\ttraining's multi_logloss: 0.0283756\n",
      "[230]\ttraining's multi_logloss: 0.0277494\n",
      "[240]\ttraining's multi_logloss: 0.0271604\n",
      "[250]\ttraining's multi_logloss: 0.0265876\n",
      "***********第15次抽样***********\n",
      "负样本抽样数量：\n",
      "(43648, 151)\n",
      "[20]\tcv_agg's multi_logloss: 0.521197 + 0.00114593\n",
      "[40]\tcv_agg's multi_logloss: 0.198068 + 0.00142118\n",
      "[60]\tcv_agg's multi_logloss: 0.087073 + 0.00158087\n",
      "[80]\tcv_agg's multi_logloss: 0.0472873 + 0.00172617\n",
      "[100]\tcv_agg's multi_logloss: 0.0329052 + 0.00185271\n",
      "[120]\tcv_agg's multi_logloss: 0.0277262 + 0.00194411\n",
      "[140]\tcv_agg's multi_logloss: 0.0257941 + 0.00202259\n",
      "[160]\tcv_agg's multi_logloss: 0.025109 + 0.00200139\n",
      "[180]\tcv_agg's multi_logloss: 0.0248259 + 0.00202689\n",
      "[200]\tcv_agg's multi_logloss: 0.0246879 + 0.00206422\n",
      "[220]\tcv_agg's multi_logloss: 0.0246035 + 0.00210453\n",
      "[240]\tcv_agg's multi_logloss: 0.0245543 + 0.00210382\n",
      "[260]\tcv_agg's multi_logloss: 0.0245449 + 0.00214976\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "pred = pd.DataFrame()\n",
    "for i in range(20):\n",
    "    print('***********第%s次抽样***********'%i)\n",
    "    x0_part = x0.copy()\n",
    "    x0_part.index = range(x0_part.shape[0])\n",
    "    x1.index = range(x1.shape[0])\n",
    "    permutation = np.random.permutation(x0_part.shape[0])\n",
    "    \n",
    "    random.seed(i)\n",
    "    ratio = int(x0.shape[0]*random.uniform(1.5, 4)/10)\n",
    "\n",
    "    select = x0_part.iloc[permutation[:ratio], :]\n",
    "    \n",
    "    print('负样本抽样数量：')\n",
    "    print(select.shape)\n",
    "    \n",
    "    data_new = pd.concat([x1, select], axis = 0)\n",
    "    \n",
    "    permutation = np.random.permutation(data_new.shape[0])\n",
    "\n",
    "    data_new = data_new.iloc[permutation, :]\n",
    "    \n",
    "    label_new = data_new['label'].values\n",
    "    try:\n",
    "        data_new = data_new.drop(['label', 'id'], axis = 1)\n",
    "    except:\n",
    "        data_new = data_new.drop(['label'], axis = 1)\n",
    "    dtrain = lgb.Dataset(data_new, label=label_new)\n",
    "\n",
    "    hist = lgb.cv(params, dtrain, num_boost_round = 1000, verbose_eval=20, early_stopping_rounds=30, stratified=False)\n",
    "    \n",
    "    print('最优结果%s'%hist['multi_logloss-mean'][-1])\n",
    "    loss_list.append(hist['multi_logloss-mean'][-1])\n",
    "    print('总loss%s'%loss_list)\n",
    "    model = lgb.train(params=params, train_set=dtrain, num_boost_round=int(1.1*len(hist['multi_logloss-mean'])), verbose_eval=10,\\\n",
    "                      valid_sets=dtrain)\n",
    "    \n",
    "    pred_temp = pd.DataFrame(model.predict(test_FE))\n",
    "    \n",
    "    pred = pd.concat([pred, pred_temp], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999387</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.998805</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.999127</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.998987</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.998670</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.999129</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999899</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999792</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999624</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.999376</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.999627</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999921</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999825</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999646</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.999131</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.999374</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.999756</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.999470</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.999740</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999856</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.999153</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.997786</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.998645</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.992610</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.991630</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.006142</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006003</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.994045</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.001424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.999754</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999809</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.999919</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999820</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.999908</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.999764</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.999332</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.998965</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.998937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.999589</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.999292</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.999524</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.997113</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.994315</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.997243</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.001269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.999873</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999729</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999827</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.999880</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999737</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999809</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.999508</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.999247</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.999500</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.999779</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.999882</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.999780</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.061063</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.931997</td>\n",
       "      <td>0.038798</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.959227</td>\n",
       "      <td>0.015648</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.978724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.999863</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.999723</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.999804</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.999660</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.999342</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.999567</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.999801</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.999558</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.999757</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.998387</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.995961</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.998409</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999824</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999886</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.998772</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.998699</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.999038</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.999884</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.999754</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.999838</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53063</th>\n",
       "      <td>0.999841</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.999690</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.999769</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53064</th>\n",
       "      <td>0.999896</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999772</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.999837</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53065</th>\n",
       "      <td>0.999474</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.999026</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.999151</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53066</th>\n",
       "      <td>0.999184</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.998499</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.999090</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53067</th>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.996527</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.003293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.993859</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.994781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53068</th>\n",
       "      <td>0.999909</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999815</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999876</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53069</th>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.999335</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53070</th>\n",
       "      <td>0.999915</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999810</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999869</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53071</th>\n",
       "      <td>0.999861</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999807</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53072</th>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999811</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53073</th>\n",
       "      <td>0.999912</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53074</th>\n",
       "      <td>0.999909</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999856</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53075</th>\n",
       "      <td>0.999785</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.999568</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.999800</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53076</th>\n",
       "      <td>0.999790</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.999726</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53077</th>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999809</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53078</th>\n",
       "      <td>0.999855</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.999824</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53079</th>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999809</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999883</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53080</th>\n",
       "      <td>0.999764</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.999468</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.999765</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53081</th>\n",
       "      <td>0.994480</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>0.990203</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.994134</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.003692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53082</th>\n",
       "      <td>0.999467</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.999057</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.999369</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53083</th>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.999654</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.999808</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53084</th>\n",
       "      <td>0.999907</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999814</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53085</th>\n",
       "      <td>0.999866</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.999758</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.999806</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53086</th>\n",
       "      <td>0.999859</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.999739</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.999824</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53087</th>\n",
       "      <td>0.998447</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.996753</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002748</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.998294</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53088</th>\n",
       "      <td>0.986768</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.006588</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.984425</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.006848</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>0.989139</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.002845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53089</th>\n",
       "      <td>0.999146</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.997975</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.998987</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53090</th>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999816</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53091</th>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.998981</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.999406</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53092</th>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.999806</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.999884</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53093 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         0  \\\n",
       "0      0.999387  0.000056  0.000109  0.000031  0.000033  0.000384  0.998805   \n",
       "1      0.998987  0.000030  0.000738  0.000012  0.000029  0.000203  0.998670   \n",
       "2      0.999899  0.000006  0.000042  0.000005  0.000005  0.000044  0.999792   \n",
       "3      0.999624  0.000027  0.000112  0.000047  0.000019  0.000170  0.999376   \n",
       "4      0.999921  0.000006  0.000020  0.000005  0.000005  0.000044  0.999825   \n",
       "5      0.999646  0.000011  0.000188  0.000008  0.000009  0.000138  0.999131   \n",
       "6      0.999756  0.000038  0.000074  0.000010  0.000010  0.000113  0.999470   \n",
       "7      0.999911  0.000006  0.000030  0.000005  0.000005  0.000044  0.999800   \n",
       "8      0.999153  0.000024  0.000407  0.000043  0.000024  0.000349  0.997786   \n",
       "9      0.992610  0.000246  0.005882  0.000258  0.000080  0.000923  0.991630   \n",
       "10     0.999887  0.000007  0.000041  0.000006  0.000006  0.000053  0.999754   \n",
       "11     0.999910  0.000012  0.000024  0.000005  0.000005  0.000044  0.999809   \n",
       "12     0.999919  0.000006  0.000022  0.000005  0.000005  0.000044  0.999820   \n",
       "13     0.999908  0.000006  0.000030  0.000005  0.000005  0.000046  0.999764   \n",
       "14     0.000376  0.000041  0.000141  0.000071  0.000040  0.999332  0.000344   \n",
       "15     0.999589  0.000034  0.000133  0.000013  0.000012  0.000218  0.999292   \n",
       "16     0.997113  0.000380  0.000603  0.000420  0.000154  0.001330  0.994315   \n",
       "17     0.999873  0.000006  0.000068  0.000005  0.000005  0.000044  0.999729   \n",
       "18     0.999880  0.000006  0.000061  0.000005  0.000005  0.000044  0.999737   \n",
       "19     0.999508  0.000017  0.000287  0.000014  0.000014  0.000160  0.999247   \n",
       "20     0.999883  0.000007  0.000043  0.000005  0.000006  0.000056  0.999779   \n",
       "21     0.999896  0.000008  0.000035  0.000005  0.000005  0.000050  0.999780   \n",
       "22     0.061063  0.000597  0.005342  0.000650  0.000352  0.931997  0.038798   \n",
       "23     0.999863  0.000014  0.000063  0.000006  0.000005  0.000050  0.999723   \n",
       "24     0.999660  0.000026  0.000101  0.000015  0.000016  0.000182  0.999342   \n",
       "25     0.999801  0.000055  0.000058  0.000006  0.000009  0.000071  0.999558   \n",
       "26     0.998387  0.000044  0.000898  0.000334  0.000030  0.000308  0.995961   \n",
       "27     0.999911  0.000006  0.000029  0.000005  0.000005  0.000044  0.999824   \n",
       "28     0.998772  0.000021  0.001080  0.000009  0.000008  0.000110  0.998699   \n",
       "29     0.999884  0.000015  0.000036  0.000006  0.000006  0.000053  0.999754   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "53063  0.999841  0.000010  0.000045  0.000008  0.000008  0.000088  0.999690   \n",
       "53064  0.999896  0.000006  0.000043  0.000007  0.000005  0.000044  0.999772   \n",
       "53065  0.999474  0.000038  0.000094  0.000015  0.000033  0.000347  0.999026   \n",
       "53066  0.999184  0.000046  0.000395  0.000020  0.000024  0.000330  0.998499   \n",
       "53067  0.000233  0.000124  0.000944  0.002051  0.000121  0.996527  0.000269   \n",
       "53068  0.999909  0.000006  0.000023  0.000005  0.000014  0.000044  0.999815   \n",
       "53069  0.999692  0.000012  0.000193  0.000010  0.000009  0.000084  0.999335   \n",
       "53070  0.999915  0.000006  0.000026  0.000005  0.000005  0.000044  0.999810   \n",
       "53071  0.999861  0.000049  0.000030  0.000005  0.000010  0.000045  0.999702   \n",
       "53072  0.999910  0.000012  0.000024  0.000005  0.000005  0.000044  0.999811   \n",
       "53073  0.999912  0.000006  0.000029  0.000005  0.000005  0.000044  0.999822   \n",
       "53074  0.999909  0.000006  0.000031  0.000005  0.000005  0.000044  0.999800   \n",
       "53075  0.999785  0.000021  0.000100  0.000008  0.000015  0.000072  0.999568   \n",
       "53076  0.999790  0.000015  0.000043  0.000010  0.000012  0.000131  0.999672   \n",
       "53077  0.999910  0.000012  0.000024  0.000005  0.000005  0.000044  0.999809   \n",
       "53078  0.999855  0.000013  0.000071  0.000007  0.000005  0.000048  0.999735   \n",
       "53079  0.999910  0.000012  0.000024  0.000005  0.000005  0.000044  0.999809   \n",
       "53080  0.999764  0.000012  0.000115  0.000010  0.000011  0.000088  0.999468   \n",
       "53081  0.994480  0.000613  0.000818  0.000266  0.000235  0.003588  0.990203   \n",
       "53082  0.999467  0.000083  0.000150  0.000049  0.000021  0.000230  0.999057   \n",
       "53083  0.999852  0.000017  0.000053  0.000010  0.000006  0.000062  0.999654   \n",
       "53084  0.999907  0.000006  0.000034  0.000005  0.000005  0.000044  0.999814   \n",
       "53085  0.999866  0.000007  0.000068  0.000006  0.000005  0.000048  0.999758   \n",
       "53086  0.999859  0.000009  0.000034  0.000007  0.000007  0.000084  0.999739   \n",
       "53087  0.998447  0.000035  0.001078  0.000036  0.000023  0.000381  0.996753   \n",
       "53088  0.986768  0.002605  0.006588  0.000365  0.000128  0.003546  0.984425   \n",
       "53089  0.999146  0.000051  0.000451  0.000017  0.000034  0.000301  0.997975   \n",
       "53090  0.999917  0.000006  0.000024  0.000005  0.000005  0.000044  0.999816   \n",
       "53091  0.999451  0.000056  0.000130  0.000028  0.000041  0.000294  0.998981   \n",
       "53092  0.999910  0.000012  0.000024  0.000005  0.000005  0.000044  0.999806   \n",
       "\n",
       "              1         2         3    ...            2         3         4  \\\n",
       "0      0.000090  0.000237  0.000038    ...     0.000284  0.000058  0.000040   \n",
       "1      0.000053  0.000729  0.000023    ...     0.000797  0.000026  0.000044   \n",
       "2      0.000011  0.000084  0.000010    ...     0.000086  0.000011  0.000013   \n",
       "3      0.000040  0.000208  0.000051    ...     0.000177  0.000051  0.000022   \n",
       "4      0.000011  0.000051  0.000010    ...     0.000066  0.000011  0.000013   \n",
       "5      0.000026  0.000525  0.000020    ...     0.000551  0.000023  0.000025   \n",
       "6      0.000074  0.000206  0.000021    ...     0.000166  0.000019  0.000020   \n",
       "7      0.000011  0.000075  0.000010    ...     0.000095  0.000012  0.000013   \n",
       "8      0.000066  0.001193  0.000072    ...     0.001764  0.000109  0.000074   \n",
       "9      0.000333  0.006142  0.000491    ...     0.006003  0.000643  0.000126   \n",
       "10     0.000014  0.000092  0.000012    ...     0.000108  0.000013  0.000014   \n",
       "11     0.000023  0.000055  0.000010    ...     0.000074  0.000012  0.000013   \n",
       "12     0.000011  0.000055  0.000010    ...     0.000074  0.000012  0.000013   \n",
       "13     0.000013  0.000088  0.000011    ...     0.000104  0.000013  0.000014   \n",
       "14     0.000069  0.000216  0.000086    ...     0.000289  0.000135  0.000063   \n",
       "15     0.000071  0.000184  0.000022    ...     0.000195  0.000027  0.000028   \n",
       "16     0.000752  0.001134  0.001017    ...     0.000708  0.000362  0.000124   \n",
       "17     0.000011  0.000146  0.000010    ...     0.000180  0.000011  0.000013   \n",
       "18     0.000011  0.000139  0.000010    ...     0.000101  0.000012  0.000013   \n",
       "19     0.000029  0.000402  0.000026    ...     0.000415  0.000034  0.000033   \n",
       "20     0.000013  0.000071  0.000011    ...     0.000085  0.000012  0.000013   \n",
       "21     0.000013  0.000084  0.000010    ...     0.000107  0.000012  0.000013   \n",
       "22     0.000453  0.002890  0.000673    ...     0.003257  0.000921  0.000297   \n",
       "23     0.000027  0.000116  0.000011    ...     0.000125  0.000014  0.000014   \n",
       "24     0.000064  0.000147  0.000019    ...     0.000144  0.000023  0.000019   \n",
       "25     0.000140  0.000109  0.000011    ...     0.000129  0.000015  0.000015   \n",
       "26     0.000092  0.001797  0.001278    ...     0.001137  0.000403  0.000051   \n",
       "27     0.000011  0.000051  0.000010    ...     0.000066  0.000012  0.000013   \n",
       "28     0.000039  0.000964  0.000017    ...     0.000919  0.000021  0.000021   \n",
       "29     0.000024  0.000088  0.000011    ...     0.000096  0.000013  0.000013   \n",
       "...         ...       ...       ...    ...          ...       ...       ...   \n",
       "53063  0.000018  0.000105  0.000016    ...     0.000120  0.000016  0.000018   \n",
       "53064  0.000011  0.000103  0.000010    ...     0.000111  0.000016  0.000013   \n",
       "53065  0.000066  0.000187  0.000028    ...     0.000240  0.000035  0.000037   \n",
       "53066  0.000090  0.000677  0.000041    ...     0.000656  0.000051  0.000066   \n",
       "53067  0.000179  0.001052  0.003293    ...     0.000855  0.004623  0.000115   \n",
       "53068  0.000011  0.000055  0.000010    ...     0.000074  0.000012  0.000013   \n",
       "53069  0.000025  0.000383  0.000022    ...     0.000341  0.000023  0.000023   \n",
       "53070  0.000012  0.000064  0.000010    ...     0.000089  0.000012  0.000013   \n",
       "53071  0.000124  0.000055  0.000010    ...     0.000074  0.000012  0.000013   \n",
       "53072  0.000020  0.000055  0.000010    ...     0.000074  0.000012  0.000013   \n",
       "53073  0.000011  0.000053  0.000010    ...     0.000082  0.000011  0.000013   \n",
       "53074  0.000011  0.000075  0.000010    ...     0.000095  0.000012  0.000013   \n",
       "53075  0.000041  0.000178  0.000019    ...     0.000157  0.000018  0.000019   \n",
       "53076  0.000023  0.000100  0.000018    ...     0.000116  0.000019  0.000022   \n",
       "53077  0.000023  0.000055  0.000010    ...     0.000074  0.000012  0.000013   \n",
       "53078  0.000025  0.000112  0.000011    ...     0.000158  0.000017  0.000014   \n",
       "53079  0.000023  0.000055  0.000010    ...     0.000074  0.000012  0.000013   \n",
       "53080  0.000026  0.000269  0.000020    ...     0.000312  0.000023  0.000023   \n",
       "53081  0.001289  0.001581  0.000419    ...     0.001321  0.000305  0.000200   \n",
       "53082  0.000133  0.000294  0.000055    ...     0.000405  0.000092  0.000055   \n",
       "53083  0.000030  0.000127  0.000015    ...     0.000137  0.000028  0.000019   \n",
       "53084  0.000011  0.000061  0.000010    ...     0.000087  0.000012  0.000013   \n",
       "53085  0.000012  0.000103  0.000011    ...     0.000108  0.000014  0.000014   \n",
       "53086  0.000016  0.000078  0.000014    ...     0.000107  0.000016  0.000018   \n",
       "53087  0.000094  0.002242  0.000090    ...     0.002748  0.000117  0.000083   \n",
       "53088  0.003974  0.006848  0.000368    ...     0.002781  0.000558  0.000110   \n",
       "53089  0.000159  0.001009  0.000037    ...     0.001108  0.000040  0.000045   \n",
       "53090  0.000012  0.000058  0.000010    ...     0.000074  0.000012  0.000013   \n",
       "53091  0.000096  0.000299  0.000037    ...     0.000317  0.000053  0.000045   \n",
       "53092  0.000023  0.000058  0.000010    ...     0.000074  0.000012  0.000013   \n",
       "\n",
       "              5         0         1         2         3         4         5  \n",
       "0      0.000727  0.999127  0.000058  0.000172  0.000051  0.000035  0.000558  \n",
       "1      0.000308  0.999129  0.000031  0.000554  0.000015  0.000030  0.000242  \n",
       "2      0.000100  0.999846  0.000010  0.000061  0.000008  0.000007  0.000069  \n",
       "3      0.000343  0.999627  0.000025  0.000110  0.000047  0.000019  0.000171  \n",
       "4      0.000100  0.999883  0.000008  0.000034  0.000007  0.000006  0.000061  \n",
       "5      0.000260  0.999374  0.000016  0.000304  0.000013  0.000012  0.000281  \n",
       "6      0.000160  0.999740  0.000047  0.000092  0.000011  0.000010  0.000100  \n",
       "7      0.000100  0.999856  0.000009  0.000055  0.000007  0.000007  0.000066  \n",
       "8      0.000806  0.998645  0.000036  0.000727  0.000057  0.000029  0.000507  \n",
       "9      0.001101  0.994045  0.000222  0.003780  0.000453  0.000076  0.001424  \n",
       "10     0.000113  0.999819  0.000011  0.000073  0.000009  0.000008  0.000080  \n",
       "11     0.000100  0.999883  0.000013  0.000033  0.000006  0.000006  0.000058  \n",
       "12     0.000100  0.999889  0.000008  0.000033  0.000006  0.000006  0.000058  \n",
       "13     0.000112  0.999869  0.000008  0.000050  0.000007  0.000006  0.000060  \n",
       "14     0.998965  0.000544  0.000077  0.000280  0.000107  0.000055  0.998937  \n",
       "15     0.000373  0.999524  0.000036  0.000174  0.000016  0.000015  0.000235  \n",
       "16     0.001595  0.997243  0.000328  0.000591  0.000408  0.000161  0.001269  \n",
       "17     0.000100  0.999827  0.000010  0.000080  0.000008  0.000007  0.000069  \n",
       "18     0.000100  0.999809  0.000010  0.000098  0.000008  0.000007  0.000069  \n",
       "19     0.000292  0.999500  0.000020  0.000263  0.000016  0.000015  0.000185  \n",
       "20     0.000112  0.999882  0.000008  0.000039  0.000006  0.000006  0.000058  \n",
       "21     0.000108  0.999852  0.000011  0.000057  0.000007  0.000007  0.000066  \n",
       "22     0.959227  0.015648  0.000482  0.004054  0.000756  0.000336  0.978724  \n",
       "23     0.000112  0.999804  0.000018  0.000082  0.000009  0.000008  0.000079  \n",
       "24     0.000246  0.999567  0.000029  0.000110  0.000020  0.000019  0.000256  \n",
       "25     0.000171  0.999757  0.000073  0.000052  0.000008  0.000011  0.000100  \n",
       "26     0.000479  0.998409  0.000043  0.000929  0.000278  0.000039  0.000303  \n",
       "27     0.000100  0.999886  0.000008  0.000032  0.000007  0.000006  0.000061  \n",
       "28     0.000266  0.999038  0.000026  0.000748  0.000013  0.000012  0.000164  \n",
       "29     0.000103  0.999838  0.000016  0.000058  0.000008  0.000007  0.000073  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "53063  0.000148  0.999769  0.000013  0.000083  0.000010  0.000010  0.000115  \n",
       "53064  0.000108  0.999837  0.000009  0.000072  0.000010  0.000007  0.000066  \n",
       "53065  0.000502  0.999151  0.000041  0.000186  0.000020  0.000034  0.000567  \n",
       "53066  0.000578  0.999090  0.000044  0.000302  0.000023  0.000030  0.000512  \n",
       "53067  0.993859  0.000331  0.000158  0.001295  0.003288  0.000148  0.994781  \n",
       "53068  0.000100  0.999876  0.000009  0.000035  0.000007  0.000012  0.000062  \n",
       "53069  0.000179  0.999620  0.000015  0.000231  0.000012  0.000011  0.000110  \n",
       "53070  0.000100  0.999869  0.000008  0.000052  0.000006  0.000006  0.000058  \n",
       "53071  0.000100  0.999807  0.000079  0.000035  0.000007  0.000011  0.000062  \n",
       "53072  0.000100  0.999883  0.000013  0.000033  0.000006  0.000006  0.000058  \n",
       "53073  0.000100  0.999880  0.000008  0.000041  0.000006  0.000006  0.000058  \n",
       "53074  0.000100  0.999856  0.000009  0.000055  0.000007  0.000007  0.000066  \n",
       "53075  0.000141  0.999800  0.000019  0.000076  0.000010  0.000013  0.000082  \n",
       "53076  0.000175  0.999726  0.000021  0.000071  0.000014  0.000021  0.000146  \n",
       "53077  0.000100  0.999883  0.000013  0.000033  0.000006  0.000006  0.000058  \n",
       "53078  0.000115  0.999824  0.000015  0.000078  0.000011  0.000007  0.000066  \n",
       "53079  0.000100  0.999883  0.000013  0.000033  0.000006  0.000006  0.000058  \n",
       "53080  0.000183  0.999765  0.000012  0.000110  0.000010  0.000009  0.000094  \n",
       "53081  0.004766  0.994134  0.000629  0.001025  0.000281  0.000240  0.003692  \n",
       "53082  0.000483  0.999369  0.000080  0.000222  0.000054  0.000025  0.000249  \n",
       "53083  0.000161  0.999808  0.000018  0.000073  0.000016  0.000008  0.000077  \n",
       "53084  0.000100  0.999851  0.000009  0.000061  0.000007  0.000007  0.000066  \n",
       "53085  0.000113  0.999806  0.000010  0.000093  0.000008  0.000008  0.000074  \n",
       "53086  0.000147  0.999824  0.000012  0.000047  0.000009  0.000009  0.000099  \n",
       "53087  0.001040  0.998294  0.000042  0.001162  0.000054  0.000026  0.000422  \n",
       "53088  0.001960  0.989139  0.002225  0.005362  0.000330  0.000099  0.002845  \n",
       "53089  0.000508  0.998987  0.000056  0.000587  0.000021  0.000036  0.000313  \n",
       "53090  0.000100  0.999889  0.000008  0.000032  0.000006  0.000006  0.000058  \n",
       "53091  0.000389  0.999406  0.000050  0.000155  0.000037  0.000035  0.000316  \n",
       "53092  0.000100  0.999884  0.000013  0.000032  0.000006  0.000006  0.000058  \n",
       "\n",
       "[53093 rows x 120 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame(sub_sample.file_id)\n",
    "\n",
    "p['prob0'] = pred[0].mean(axis = 1)\n",
    "p['prob1'] = pred[1].mean(axis = 1)\n",
    "p['prob2'] = pred[2].mean(axis = 1)\n",
    "p['prob3'] = pred[3].mean(axis = 1)\n",
    "p['prob4'] = pred[4].mean(axis = 1)\n",
    "p['prob5'] = pred[5].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_id    26546.000000\n",
       "prob0          0.899657\n",
       "prob1          0.007737\n",
       "prob2          0.014635\n",
       "prob3          0.012296\n",
       "prob4          0.001661\n",
       "prob5          0.064014\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv('/home/libo/Security/sub/8.24_bagging_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_id    26546.000000\n",
       "prob0          0.899607\n",
       "prob1          0.007780\n",
       "prob2          0.014583\n",
       "prob3          0.012210\n",
       "prob4          0.001655\n",
       "prob5          0.064165\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.       , 0.9997724],\n",
       "       [0.9997724, 1.       ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef([p['prob0'], x['prob0']], rowvar=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = x.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred['prob0'] = 0.5*x['prob0'] + 0.5*p['prob0']\n",
    "pred['prob1'] = 0.5*x['prob1'] + 0.5*p['prob1']\n",
    "pred['prob2'] = 0.5*x['prob2'] + 0.5*p['prob2']\n",
    "pred['prob3'] = 0.5*x['prob3'] + 0.5*p['prob3']\n",
    "pred['prob4'] = 0.5*x['prob4'] + 0.5*p['prob4']\n",
    "pred['prob5'] = 0.5*x['prob5'] + 0.5*p['prob5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_id    26546.000000\n",
       "prob0          0.899632\n",
       "prob1          0.007758\n",
       "prob2          0.014609\n",
       "prob3          0.012253\n",
       "prob4          0.001658\n",
       "prob5          0.064089\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.to_csv('/home/libo/Security/sub/8.25_bagging_v1_v2.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116624, 300), (53093, 300))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_sample = pd.read_csv('/home/libo/Security/3rd_security_submit_sample.csv')\n",
    "\n",
    "label = pd.read_pickle('/home/libo/Security/label.pkl')\n",
    "label = label[0].values\n",
    "\n",
    "feature = pd.read_pickle('/home/libo/Security/top_feature1000.pkl')\n",
    "\n",
    "feature = feature.iloc[:,:301]\n",
    "\n",
    "feature['id'] = feature.iloc[:, 0].values\n",
    "\n",
    "feature.drop('file_id', axis = 1, inplace = True)\n",
    "\n",
    "train_FE = feature.iloc[:len(label),:]\n",
    "test_FE = feature.iloc[len(label):,:]\n",
    "\n",
    "train_FE.shape, test_FE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/home/libo/Security/stack'\n",
    "os.chdir(path)\n",
    "\n",
    "list_ = os.listdir()\n",
    "train_list = [x for x in list_ if 'train' in x]\n",
    "test_list = [x for x in list_ if 'test' in x]\n",
    "\n",
    "len(train_list) == len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116624, 6)\n",
      "(116624, 12)\n",
      "(116624, 18)\n",
      "(116624, 24)\n",
      "(116624, 30)\n",
      "(116624, 36)\n",
      "(116624, 42)\n",
      "(116624, 48)\n",
      "(116624, 54)\n",
      "(116624, 60)\n",
      "(116624, 66)\n",
      "(116624, 72)\n",
      "(116624, 78)\n",
      "(116624, 84)\n",
      "(116624, 90)\n",
      "(116624, 96)\n",
      "(116624, 102)\n",
      "(116624, 108)\n",
      "(116624, 114)\n",
      "(116624, 204)\n",
      "(116624, 210)\n",
      "(116624, 216)\n",
      "(116624, 222)\n",
      "(116624, 228)\n",
      "(116624, 234)\n",
      "(116624, 240)\n",
      "(116624, 246)\n",
      "(116624, 252)\n",
      "(116624, 258)\n",
      "(116624, 264)\n",
      "(116624, 270)\n"
     ]
    }
   ],
   "source": [
    "train_meta = pd.DataFrame()\n",
    "for i in train_list:\n",
    "    train_meta = pd.concat([train_meta, pd.read_csv(i)], axis = 1)\n",
    "    print(train_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53093, 6)\n",
      "(53093, 12)\n",
      "(53093, 18)\n",
      "(53093, 24)\n",
      "(53093, 30)\n",
      "(53093, 36)\n",
      "(53093, 42)\n",
      "(53093, 48)\n",
      "(53093, 54)\n",
      "(53093, 60)\n",
      "(53093, 66)\n",
      "(53093, 72)\n",
      "(53093, 78)\n",
      "(53093, 84)\n",
      "(53093, 90)\n",
      "(53093, 96)\n",
      "(53093, 102)\n",
      "(53093, 108)\n",
      "(53093, 114)\n",
      "(53093, 120)\n",
      "(53093, 126)\n",
      "(53093, 132)\n",
      "(53093, 138)\n",
      "(53093, 144)\n",
      "(53093, 150)\n",
      "(53093, 156)\n",
      "(53093, 162)\n",
      "(53093, 168)\n",
      "(53093, 174)\n",
      "(53093, 180)\n",
      "(53093, 270)\n"
     ]
    }
   ],
   "source": [
    "test_meta = pd.DataFrame()\n",
    "for i in test_list:\n",
    "    try:\n",
    "        x = pd.read_csv(i).drop('file_id', axis = 1)\n",
    "    except:\n",
    "        x = pd.read_csv(i)\n",
    "    test_meta = pd.concat([test_meta, x], axis = 1)\n",
    "    print(test_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta = test_meta[train_meta.columns]\n",
    "test_meta.columns == train_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116624, 270), (53093, 270))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta.shape, test_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_FE.index = range(test_FE.shape[0])\n",
    "test_meta.index = range(test_meta.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"objective\": \"multiclass\",\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.05,\n",
    "          \"num_leaves\": 15,\n",
    "          # \"max_bin\": 128,\n",
    "          \"feature_fraction\": 0.85,\n",
    "#           \"min_child_samples\": 10,\n",
    "#           \"min_child_weight\": 150,\n",
    "#           \"min_split_gain\": 0,\n",
    "          \"subsample\": 0.85,\n",
    "          #'metric':'logloss',\n",
    "           'lambda_l1':2,\n",
    "           'lambda_l2':2,\n",
    "          'seed':666,\n",
    "          'num_class':6\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/libo/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/libo/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((116624, 301), (53093, 299))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_FE['label'] = label\n",
    "\n",
    "test_FE.drop('id', axis = 1, inplace = True)\n",
    "\n",
    "train_FE.shape, test_FE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116624, 571), (53093, 569))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_FE = pd.concat([train_FE, train_meta], axis = 1)\n",
    "test_FE = pd.concat([test_FE, test_meta], axis = 1)\n",
    "\n",
    "train_FE.shape, test_FE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(train_FE.drop(['id', 'label'], axis = 1).columns == test_FE.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((111545, 571), (5079, 571))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = train_FE[train_FE['label'] == 0]\n",
    "\n",
    "x1 = train_FE[train_FE['label'] != 0]\n",
    "\n",
    "x0.shape, x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = pd.read_csv('/home/libo/Security/index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********第0次抽样***********\n",
      "负样本抽样数量：\n",
      "(17946, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.536136 + 0.00328952\n",
      "[40]\tcv_agg's multi_logloss: 0.216745 + 0.00440766\n",
      "[60]\tcv_agg's multi_logloss: 0.107337 + 0.00509375\n",
      "[80]\tcv_agg's multi_logloss: 0.0685143 + 0.00557223\n",
      "[100]\tcv_agg's multi_logloss: 0.054815 + 0.00584947\n",
      "[120]\tcv_agg's multi_logloss: 0.0501334 + 0.00602871\n",
      "[140]\tcv_agg's multi_logloss: 0.0487183 + 0.00617431\n",
      "[160]\tcv_agg's multi_logloss: 0.0485376 + 0.00640683\n",
      "[180]\tcv_agg's multi_logloss: 0.0487981 + 0.00664318\n",
      "最优结果0.04852498567484852\n",
      "总loss[0.04852498567484852]\n",
      "[10]\ttraining's multi_logloss: 0.909154\n",
      "[20]\ttraining's multi_logloss: 0.53043\n",
      "[30]\ttraining's multi_logloss: 0.32542\n",
      "[40]\ttraining's multi_logloss: 0.206719\n",
      "[50]\ttraining's multi_logloss: 0.135874\n",
      "[60]\ttraining's multi_logloss: 0.0927706\n",
      "[70]\ttraining's multi_logloss: 0.0660614\n",
      "[80]\ttraining's multi_logloss: 0.0493261\n",
      "[90]\ttraining's multi_logloss: 0.0384006\n",
      "[100]\ttraining's multi_logloss: 0.0309029\n",
      "[110]\ttraining's multi_logloss: 0.0254266\n",
      "[120]\ttraining's multi_logloss: 0.0216751\n",
      "[130]\ttraining's multi_logloss: 0.0187917\n",
      "[140]\ttraining's multi_logloss: 0.0166292\n",
      "[150]\ttraining's multi_logloss: 0.014882\n",
      "[160]\ttraining's multi_logloss: 0.0133759\n",
      "[170]\ttraining's multi_logloss: 0.0121697\n",
      "***********第1次抽样***********\n",
      "负样本抽样数量：\n",
      "(36323, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.523138 + 0.000793593\n",
      "[40]\tcv_agg's multi_logloss: 0.200499 + 0.00117145\n",
      "[60]\tcv_agg's multi_logloss: 0.0897965 + 0.0015443\n",
      "[80]\tcv_agg's multi_logloss: 0.0503241 + 0.00182297\n",
      "[100]\tcv_agg's multi_logloss: 0.0362014 + 0.0020693\n",
      "[120]\tcv_agg's multi_logloss: 0.0312826 + 0.00230499\n",
      "[140]\tcv_agg's multi_logloss: 0.0296152 + 0.00250316\n",
      "[160]\tcv_agg's multi_logloss: 0.0291423 + 0.00267687\n",
      "[180]\tcv_agg's multi_logloss: 0.0291535 + 0.00275581\n",
      "最优结果0.029106363761690495\n",
      "总loss[0.04852498567484852, 0.029106363761690495]\n",
      "[10]\ttraining's multi_logloss: 0.90127\n",
      "[20]\ttraining's multi_logloss: 0.520165\n",
      "[30]\ttraining's multi_logloss: 0.314106\n",
      "[40]\ttraining's multi_logloss: 0.195052\n",
      "[50]\ttraining's multi_logloss: 0.124354\n",
      "[60]\ttraining's multi_logloss: 0.0817705\n",
      "[70]\ttraining's multi_logloss: 0.0557481\n",
      "[80]\ttraining's multi_logloss: 0.0396923\n",
      "[90]\ttraining's multi_logloss: 0.0295322\n",
      "[100]\ttraining's multi_logloss: 0.0228914\n",
      "[110]\ttraining's multi_logloss: 0.0183893\n",
      "[120]\ttraining's multi_logloss: 0.0152092\n",
      "[130]\ttraining's multi_logloss: 0.0128916\n",
      "[140]\ttraining's multi_logloss: 0.0112447\n",
      "[150]\ttraining's multi_logloss: 0.009989\n",
      "[160]\ttraining's multi_logloss: 0.00896843\n",
      "[170]\ttraining's multi_logloss: 0.00813856\n",
      "[180]\ttraining's multi_logloss: 0.0074314\n",
      "***********第2次抽样***********\n",
      "负样本抽样数量：\n",
      "(18702, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.534727 + 0.000818835\n",
      "[40]\tcv_agg's multi_logloss: 0.214893 + 0.00108425\n",
      "[60]\tcv_agg's multi_logloss: 0.10539 + 0.00142255\n",
      "[80]\tcv_agg's multi_logloss: 0.0665182 + 0.0017395\n",
      "[100]\tcv_agg's multi_logloss: 0.0529844 + 0.00202593\n",
      "[120]\tcv_agg's multi_logloss: 0.0483529 + 0.00207665\n",
      "[140]\tcv_agg's multi_logloss: 0.0469117 + 0.00225844\n",
      "[160]\tcv_agg's multi_logloss: 0.0466713 + 0.00246202\n",
      "[180]\tcv_agg's multi_logloss: 0.046771 + 0.00256063\n",
      "最优结果0.04664073876496425\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425]\n",
      "[10]\ttraining's multi_logloss: 0.908535\n",
      "[20]\ttraining's multi_logloss: 0.529601\n",
      "[30]\ttraining's multi_logloss: 0.324332\n",
      "[40]\ttraining's multi_logloss: 0.205614\n",
      "[50]\ttraining's multi_logloss: 0.134802\n",
      "[60]\ttraining's multi_logloss: 0.0916472\n",
      "[70]\ttraining's multi_logloss: 0.0649966\n",
      "[80]\ttraining's multi_logloss: 0.0483055\n",
      "[90]\ttraining's multi_logloss: 0.0374136\n",
      "[100]\ttraining's multi_logloss: 0.0300349\n",
      "[110]\ttraining's multi_logloss: 0.0245802\n",
      "[120]\ttraining's multi_logloss: 0.0208523\n",
      "[130]\ttraining's multi_logloss: 0.0180956\n",
      "[40]\tcv_agg's multi_logloss: 0.214984 + 0.00309876\n",
      "[60]\tcv_agg's multi_logloss: 0.105639 + 0.00422645\n",
      "[80]\tcv_agg's multi_logloss: 0.0667656 + 0.00517232\n",
      "[100]\tcv_agg's multi_logloss: 0.0528466 + 0.00559294\n",
      "[120]\tcv_agg's multi_logloss: 0.0480635 + 0.00578825\n",
      "[140]\tcv_agg's multi_logloss: 0.0467516 + 0.00603616\n",
      "[160]\tcv_agg's multi_logloss: 0.0466688 + 0.00622228\n",
      "[180]\tcv_agg's multi_logloss: 0.0469077 + 0.00644214\n",
      "最优结果0.0466088713028769\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425, 0.0274934099341403, 0.026576197708340167, 0.026339973788190602, 0.03560644719328749, 0.04724403952185012, 0.026356844328479134, 0.0466088713028769]\n",
      "[10]\ttraining's multi_logloss: 0.908525\n",
      "[20]\ttraining's multi_logloss: 0.529634\n",
      "[30]\ttraining's multi_logloss: 0.324421\n",
      "[40]\ttraining's multi_logloss: 0.205644\n",
      "[50]\ttraining's multi_logloss: 0.134813\n",
      "[60]\ttraining's multi_logloss: 0.0917593\n",
      "[70]\ttraining's multi_logloss: 0.0650527\n",
      "[80]\ttraining's multi_logloss: 0.048236\n",
      "[90]\ttraining's multi_logloss: 0.0372757\n",
      "[100]\ttraining's multi_logloss: 0.029935\n",
      "[110]\ttraining's multi_logloss: 0.0245773\n",
      "[120]\ttraining's multi_logloss: 0.0208367\n",
      "[130]\ttraining's multi_logloss: 0.018099\n",
      "[140]\ttraining's multi_logloss: 0.0159732\n",
      "[150]\ttraining's multi_logloss: 0.0142662\n",
      "[160]\ttraining's multi_logloss: 0.0128236\n",
      "***********第10次抽样***********\n",
      "负样本抽样数量：\n",
      "(25950, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.528366 + 0.00244653\n",
      "[40]\tcv_agg's multi_logloss: 0.206938 + 0.00341671\n",
      "[60]\tcv_agg's multi_logloss: 0.0967613 + 0.00395444\n",
      "[80]\tcv_agg's multi_logloss: 0.057608 + 0.00438011\n",
      "[100]\tcv_agg's multi_logloss: 0.0436478 + 0.00459067\n",
      "[120]\tcv_agg's multi_logloss: 0.0388366 + 0.00490064\n",
      "[140]\tcv_agg's multi_logloss: 0.0371357 + 0.00505507\n",
      "[160]\tcv_agg's multi_logloss: 0.0367755 + 0.00519856\n",
      "[180]\tcv_agg's multi_logloss: 0.0368871 + 0.00532958\n",
      "最优结果0.03676851532131863\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425, 0.0274934099341403, 0.026576197708340167, 0.026339973788190602, 0.03560644719328749, 0.04724403952185012, 0.026356844328479134, 0.0466088713028769, 0.03676851532131863]\n",
      "[10]\ttraining's multi_logloss: 0.904634\n",
      "[20]\ttraining's multi_logloss: 0.524442\n",
      "[30]\ttraining's multi_logloss: 0.318787\n",
      "[40]\ttraining's multi_logloss: 0.199929\n",
      "[50]\ttraining's multi_logloss: 0.129134\n",
      "[60]\ttraining's multi_logloss: 0.086399\n",
      "[70]\ttraining's multi_logloss: 0.0601174\n",
      "[80]\ttraining's multi_logloss: 0.0438369\n",
      "[90]\ttraining's multi_logloss: 0.0332007\n",
      "[100]\ttraining's multi_logloss: 0.0262581\n",
      "[110]\ttraining's multi_logloss: 0.0213399\n",
      "[120]\ttraining's multi_logloss: 0.0178545\n",
      "[130]\ttraining's multi_logloss: 0.0153462\n",
      "[140]\ttraining's multi_logloss: 0.0134483\n",
      "[150]\ttraining's multi_logloss: 0.0120067\n",
      "[160]\ttraining's multi_logloss: 0.0107461\n",
      "[170]\ttraining's multi_logloss: 0.00975788\n",
      "[180]\ttraining's multi_logloss: 0.00892131\n",
      "***********第11次抽样***********\n",
      "负样本抽样数量：\n",
      "(16908, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.536468 + 0.00197906\n",
      "[40]\tcv_agg's multi_logloss: 0.216863 + 0.00286217\n",
      "[60]\tcv_agg's multi_logloss: 0.107526 + 0.00357824\n",
      "[80]\tcv_agg's multi_logloss: 0.0687822 + 0.00405949\n",
      "[100]\tcv_agg's multi_logloss: 0.0551547 + 0.00433479\n",
      "[120]\tcv_agg's multi_logloss: 0.05073 + 0.00442981\n",
      "[140]\tcv_agg's multi_logloss: 0.0493759 + 0.00442625\n",
      "[160]\tcv_agg's multi_logloss: 0.04914 + 0.00456768\n",
      "[180]\tcv_agg's multi_logloss: 0.0494215 + 0.00467025\n",
      "最优结果0.04910697491473538\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425, 0.0274934099341403, 0.026576197708340167, 0.026339973788190602, 0.03560644719328749, 0.04724403952185012, 0.026356844328479134, 0.0466088713028769, 0.03676851532131863, 0.04910697491473538]\n",
      "[10]\ttraining's multi_logloss: 0.90947\n",
      "[20]\ttraining's multi_logloss: 0.530936\n",
      "[30]\ttraining's multi_logloss: 0.325978\n",
      "[40]\ttraining's multi_logloss: 0.207292\n",
      "[50]\ttraining's multi_logloss: 0.136396\n",
      "[60]\ttraining's multi_logloss: 0.0933106\n",
      "[70]\ttraining's multi_logloss: 0.066561\n",
      "[80]\ttraining's multi_logloss: 0.0496871\n",
      "[90]\ttraining's multi_logloss: 0.0386013\n",
      "[100]\ttraining's multi_logloss: 0.0311721\n",
      "[110]\ttraining's multi_logloss: 0.0257433\n",
      "[120]\ttraining's multi_logloss: 0.021828\n",
      "[130]\ttraining's multi_logloss: 0.0188987\n",
      "[140]\ttraining's multi_logloss: 0.0166068\n",
      "[150]\ttraining's multi_logloss: 0.0148604\n",
      "[160]\ttraining's multi_logloss: 0.0133866\n",
      "***********第12次抽样***********\n",
      "负样本抽样数量：\n",
      "(37208, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.522879 + 0.00161777\n",
      "[40]\tcv_agg's multi_logloss: 0.20011 + 0.00206243\n",
      "[60]\tcv_agg's multi_logloss: 0.0892063 + 0.00226373\n",
      "[80]\tcv_agg's multi_logloss: 0.0495625 + 0.00241918\n",
      "[100]\tcv_agg's multi_logloss: 0.0353808 + 0.00252981\n",
      "[120]\tcv_agg's multi_logloss: 0.0303074 + 0.00259028\n",
      "[140]\tcv_agg's multi_logloss: 0.0285677 + 0.00273795\n",
      "[160]\tcv_agg's multi_logloss: 0.0280468 + 0.00281009\n",
      "[180]\tcv_agg's multi_logloss: 0.0280297 + 0.00289583\n",
      "[200]\tcv_agg's multi_logloss: 0.0281834 + 0.0029691\n",
      "最优结果0.027991616574996703\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425, 0.0274934099341403, 0.026576197708340167, 0.026339973788190602, 0.03560644719328749, 0.04724403952185012, 0.026356844328479134, 0.0466088713028769, 0.03676851532131863, 0.04910697491473538, 0.027991616574996703]\n",
      "[10]\ttraining's multi_logloss: 0.901028\n",
      "[20]\ttraining's multi_logloss: 0.519807\n",
      "[30]\ttraining's multi_logloss: 0.313665\n",
      "[40]\ttraining's multi_logloss: 0.194617\n",
      "[50]\ttraining's multi_logloss: 0.123932\n",
      "[60]\ttraining's multi_logloss: 0.0813486\n",
      "[70]\ttraining's multi_logloss: 0.0553397\n",
      "[80]\ttraining's multi_logloss: 0.039289\n",
      "[90]\ttraining's multi_logloss: 0.029144\n",
      "[100]\ttraining's multi_logloss: 0.022509\n",
      "[110]\ttraining's multi_logloss: 0.017965\n",
      "[120]\ttraining's multi_logloss: 0.0148367\n",
      "[130]\ttraining's multi_logloss: 0.0125723\n",
      "[140]\ttraining's multi_logloss: 0.0109094\n",
      "[150]\ttraining's multi_logloss: 0.00967878\n",
      "[160]\ttraining's multi_logloss: 0.00871394\n",
      "[170]\ttraining's multi_logloss: 0.00791283\n",
      "[180]\ttraining's multi_logloss: 0.00723255\n",
      "***********第13次抽样***********\n",
      "负样本抽样数量：\n",
      "(25360, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.528993 + 0.00246026\n",
      "[40]\tcv_agg's multi_logloss: 0.20784 + 0.00365287\n",
      "[60]\tcv_agg's multi_logloss: 0.0976609 + 0.00452432\n",
      "[80]\tcv_agg's multi_logloss: 0.0582645 + 0.00521158\n",
      "[100]\tcv_agg's multi_logloss: 0.0444045 + 0.00581534\n",
      "[120]\tcv_agg's multi_logloss: 0.0394916 + 0.00637591\n",
      "[140]\tcv_agg's multi_logloss: 0.0379159 + 0.00675597\n",
      "[160]\tcv_agg's multi_logloss: 0.0375588 + 0.00717604\n",
      "[180]\tcv_agg's multi_logloss: 0.0375021 + 0.00737412\n",
      "[200]\tcv_agg's multi_logloss: 0.0376941 + 0.00759508\n",
      "最优结果0.037472764778355636\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425, 0.0274934099341403, 0.026576197708340167, 0.026339973788190602, 0.03560644719328749, 0.04724403952185012, 0.026356844328479134, 0.0466088713028769, 0.03676851532131863, 0.04910697491473538, 0.027991616574996703, 0.037472764778355636]\n",
      "[10]\ttraining's multi_logloss: 0.904905\n",
      "[20]\ttraining's multi_logloss: 0.524855\n",
      "[30]\ttraining's multi_logloss: 0.319287\n",
      "[40]\ttraining's multi_logloss: 0.200426\n",
      "[50]\ttraining's multi_logloss: 0.129694\n",
      "[60]\ttraining's multi_logloss: 0.0868519\n",
      "[70]\ttraining's multi_logloss: 0.0605267\n",
      "[80]\ttraining's multi_logloss: 0.0441559\n",
      "[90]\ttraining's multi_logloss: 0.0335255\n",
      "[100]\ttraining's multi_logloss: 0.0265627\n",
      "[110]\ttraining's multi_logloss: 0.0215929\n",
      "[120]\ttraining's multi_logloss: 0.0180842\n",
      "[130]\ttraining's multi_logloss: 0.0155377\n",
      "[140]\ttraining's multi_logloss: 0.013654\n",
      "[150]\ttraining's multi_logloss: 0.012173\n",
      "[160]\ttraining's multi_logloss: 0.0109772\n",
      "[170]\ttraining's multi_logloss: 0.00997649\n",
      "[180]\ttraining's multi_logloss: 0.00914805\n",
      "[190]\ttraining's multi_logloss: 0.00843004\n",
      "***********第14次抽样***********\n",
      "负样本抽样数量：\n",
      "(41538, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.5219 + 0.000984303\n",
      "[40]\tcv_agg's multi_logloss: 0.199087 + 0.0013287\n",
      "[60]\tcv_agg's multi_logloss: 0.0882352 + 0.00145357\n",
      "[80]\tcv_agg's multi_logloss: 0.0485296 + 0.00151059\n",
      "[100]\tcv_agg's multi_logloss: 0.0341855 + 0.00152406\n",
      "[120]\tcv_agg's multi_logloss: 0.0290866 + 0.00158009\n",
      "[140]\tcv_agg's multi_logloss: 0.0273279 + 0.00162776\n",
      "[160]\tcv_agg's multi_logloss: 0.0268337 + 0.00167235\n",
      "[180]\tcv_agg's multi_logloss: 0.0268088 + 0.0017197\n",
      "[200]\tcv_agg's multi_logloss: 0.0269594 + 0.00177498\n",
      "最优结果0.026792865398878374\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425, 0.0274934099341403, 0.026576197708340167, 0.026339973788190602, 0.03560644719328749, 0.04724403952185012, 0.026356844328479134, 0.0466088713028769, 0.03676851532131863, 0.04910697491473538, 0.027991616574996703, 0.037472764778355636, 0.026792865398878374]\n",
      "[10]\ttraining's multi_logloss: 0.900448\n",
      "[20]\ttraining's multi_logloss: 0.519023\n",
      "[30]\ttraining's multi_logloss: 0.312855\n",
      "[40]\ttraining's multi_logloss: 0.193887\n",
      "[50]\ttraining's multi_logloss: 0.1233\n",
      "[60]\ttraining's multi_logloss: 0.0806882\n",
      "[70]\ttraining's multi_logloss: 0.0546842\n",
      "[80]\ttraining's multi_logloss: 0.0386448\n",
      "[90]\ttraining's multi_logloss: 0.028553\n",
      "[100]\ttraining's multi_logloss: 0.0219799\n",
      "[110]\ttraining's multi_logloss: 0.017541\n",
      "[120]\ttraining's multi_logloss: 0.0144596\n",
      "[130]\ttraining's multi_logloss: 0.012299\n",
      "[140]\ttraining's multi_logloss: 0.0107074\n",
      "[150]\ttraining's multi_logloss: 0.0095169\n",
      "[160]\ttraining's multi_logloss: 0.00857343\n",
      "[170]\ttraining's multi_logloss: 0.00779301\n",
      "[180]\ttraining's multi_logloss: 0.00713283\n",
      "[190]\ttraining's multi_logloss: 0.00655954\n",
      "***********第15次抽样***********\n",
      "负样本抽样数量：\n",
      "(37598, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.522964 + 0.00177355\n",
      "[40]\tcv_agg's multi_logloss: 0.20038 + 0.00224969\n",
      "[60]\tcv_agg's multi_logloss: 0.0898254 + 0.00252284\n",
      "[80]\tcv_agg's multi_logloss: 0.0503805 + 0.00261339\n",
      "[100]\tcv_agg's multi_logloss: 0.0363855 + 0.0027746\n",
      "[120]\tcv_agg's multi_logloss: 0.0314821 + 0.00297343\n",
      "[140]\tcv_agg's multi_logloss: 0.0299347 + 0.00312957\n",
      "[160]\tcv_agg's multi_logloss: 0.0295517 + 0.00328226\n",
      "[180]\tcv_agg's multi_logloss: 0.0296115 + 0.00341083\n",
      "最优结果0.029519982879886435\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425, 0.0274934099341403, 0.026576197708340167, 0.026339973788190602, 0.03560644719328749, 0.04724403952185012, 0.026356844328479134, 0.0466088713028769, 0.03676851532131863, 0.04910697491473538, 0.027991616574996703, 0.037472764778355636, 0.026792865398878374, 0.029519982879886435]\n",
      "[10]\ttraining's multi_logloss: 0.901134\n",
      "[20]\ttraining's multi_logloss: 0.519952\n",
      "[30]\ttraining's multi_logloss: 0.313877\n",
      "[40]\ttraining's multi_logloss: 0.19483\n",
      "[50]\ttraining's multi_logloss: 0.124176\n",
      "[60]\ttraining's multi_logloss: 0.0815896\n",
      "[70]\ttraining's multi_logloss: 0.0555446\n",
      "[80]\ttraining's multi_logloss: 0.0395213\n",
      "[90]\ttraining's multi_logloss: 0.0293778\n",
      "[100]\ttraining's multi_logloss: 0.0227397\n",
      "[110]\ttraining's multi_logloss: 0.0182713\n",
      "[120]\ttraining's multi_logloss: 0.0151369\n",
      "[130]\ttraining's multi_logloss: 0.0128697\n",
      "[140]\ttraining's multi_logloss: 0.0112141\n",
      "[150]\ttraining's multi_logloss: 0.00997329\n",
      "[160]\ttraining's multi_logloss: 0.00898202\n",
      "[170]\ttraining's multi_logloss: 0.00817339\n",
      "[180]\ttraining's multi_logloss: 0.007489\n",
      "***********第16次抽样***********\n",
      "负样本抽样数量：\n",
      "(35740, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.523662 + 0.00109226\n",
      "[40]\tcv_agg's multi_logloss: 0.201299 + 0.00181227\n",
      "[60]\tcv_agg's multi_logloss: 0.0907827 + 0.00251713\n",
      "[80]\tcv_agg's multi_logloss: 0.0513575 + 0.00307547\n",
      "[100]\tcv_agg's multi_logloss: 0.0372489 + 0.00346428\n",
      "[120]\tcv_agg's multi_logloss: 0.0321686 + 0.00380298\n",
      "[140]\tcv_agg's multi_logloss: 0.0304207 + 0.00407501\n",
      "[160]\tcv_agg's multi_logloss: 0.0299315 + 0.00427691\n",
      "[180]\tcv_agg's multi_logloss: 0.0298982 + 0.00441732\n",
      "[200]\tcv_agg's multi_logloss: 0.0300438 + 0.00458239\n",
      "最优结果0.029880590619671365\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425, 0.0274934099341403, 0.026576197708340167, 0.026339973788190602, 0.03560644719328749, 0.04724403952185012, 0.026356844328479134, 0.0466088713028769, 0.03676851532131863, 0.04910697491473538, 0.027991616574996703, 0.037472764778355636, 0.026792865398878374, 0.029519982879886435, 0.029880590619671365]\n",
      "[10]\ttraining's multi_logloss: 0.901615\n",
      "[20]\ttraining's multi_logloss: 0.520571\n",
      "[30]\ttraining's multi_logloss: 0.314561\n",
      "[40]\ttraining's multi_logloss: 0.19558\n",
      "[50]\ttraining's multi_logloss: 0.124939\n",
      "[60]\ttraining's multi_logloss: 0.0823407\n",
      "[70]\ttraining's multi_logloss: 0.0563177\n",
      "[80]\ttraining's multi_logloss: 0.0401812\n",
      "[90]\ttraining's multi_logloss: 0.0299215\n",
      "[100]\ttraining's multi_logloss: 0.0231726\n",
      "[110]\ttraining's multi_logloss: 0.0186503\n",
      "[120]\ttraining's multi_logloss: 0.0154627\n",
      "[130]\ttraining's multi_logloss: 0.0131786\n",
      "[140]\ttraining's multi_logloss: 0.0115179\n",
      "[150]\ttraining's multi_logloss: 0.0102691\n",
      "[160]\ttraining's multi_logloss: 0.00922773\n",
      "[170]\ttraining's multi_logloss: 0.00837714\n",
      "[180]\ttraining's multi_logloss: 0.00767471\n",
      "***********第17次抽样***********\n",
      "负样本抽样数量：\n",
      "(25546, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.52857 + 0.00142732\n",
      "[40]\tcv_agg's multi_logloss: 0.207459 + 0.00182407\n",
      "[60]\tcv_agg's multi_logloss: 0.0973174 + 0.0019975\n",
      "[80]\tcv_agg's multi_logloss: 0.0580342 + 0.00228253\n",
      "[100]\tcv_agg's multi_logloss: 0.0440851 + 0.00258851\n",
      "[120]\tcv_agg's multi_logloss: 0.0392331 + 0.00278579\n",
      "[140]\tcv_agg's multi_logloss: 0.0377016 + 0.00300909\n",
      "[160]\tcv_agg's multi_logloss: 0.0373598 + 0.00311809\n",
      "[180]\tcv_agg's multi_logloss: 0.0374648 + 0.00320637\n",
      "最优结果0.037358582176927724\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425, 0.0274934099341403, 0.026576197708340167, 0.026339973788190602, 0.03560644719328749, 0.04724403952185012, 0.026356844328479134, 0.0466088713028769, 0.03676851532131863, 0.04910697491473538, 0.027991616574996703, 0.037472764778355636, 0.026792865398878374, 0.029519982879886435, 0.029880590619671365, 0.037358582176927724]\n",
      "[10]\ttraining's multi_logloss: 0.904864\n",
      "[20]\ttraining's multi_logloss: 0.524834\n",
      "[30]\ttraining's multi_logloss: 0.319248\n",
      "[40]\ttraining's multi_logloss: 0.200457\n",
      "[50]\ttraining's multi_logloss: 0.129706\n",
      "[60]\ttraining's multi_logloss: 0.0868806\n",
      "[70]\ttraining's multi_logloss: 0.0605393\n",
      "[80]\ttraining's multi_logloss: 0.0440786\n",
      "[90]\ttraining's multi_logloss: 0.0335286\n",
      "[100]\ttraining's multi_logloss: 0.0264885\n",
      "[110]\ttraining's multi_logloss: 0.0214544\n",
      "[120]\ttraining's multi_logloss: 0.0179155\n",
      "[130]\ttraining's multi_logloss: 0.0153849\n",
      "[140]\ttraining's multi_logloss: 0.0134976\n",
      "[150]\ttraining's multi_logloss: 0.0120066\n",
      "[160]\ttraining's multi_logloss: 0.0107768\n",
      "[170]\ttraining's multi_logloss: 0.00975924\n",
      "***********第18次抽样***********\n",
      "负样本抽样数量：\n",
      "(34245, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.524074 + 0.00108939\n",
      "[40]\tcv_agg's multi_logloss: 0.201717 + 0.00158981\n",
      "[60]\tcv_agg's multi_logloss: 0.0911875 + 0.00189953\n",
      "[80]\tcv_agg's multi_logloss: 0.0517009 + 0.00216235\n",
      "[100]\tcv_agg's multi_logloss: 0.0375966 + 0.00223198\n",
      "[120]\tcv_agg's multi_logloss: 0.0326026 + 0.00233299\n",
      "[140]\tcv_agg's multi_logloss: 0.0309526 + 0.00234068\n",
      "[160]\tcv_agg's multi_logloss: 0.0305655 + 0.00240928\n",
      "[180]\tcv_agg's multi_logloss: 0.0305823 + 0.00240747\n",
      "最优结果0.030529320831376716\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425, 0.0274934099341403, 0.026576197708340167, 0.026339973788190602, 0.03560644719328749, 0.04724403952185012, 0.026356844328479134, 0.0466088713028769, 0.03676851532131863, 0.04910697491473538, 0.027991616574996703, 0.037472764778355636, 0.026792865398878374, 0.029519982879886435, 0.029880590619671365, 0.037358582176927724, 0.030529320831376716]\n",
      "[10]\ttraining's multi_logloss: 0.901822\n",
      "[20]\ttraining's multi_logloss: 0.520866\n",
      "[30]\ttraining's multi_logloss: 0.314867\n",
      "[40]\ttraining's multi_logloss: 0.195898\n",
      "[50]\ttraining's multi_logloss: 0.125277\n",
      "[60]\ttraining's multi_logloss: 0.0826641\n",
      "[70]\ttraining's multi_logloss: 0.0565642\n",
      "[80]\ttraining's multi_logloss: 0.0403921\n",
      "[90]\ttraining's multi_logloss: 0.0301836\n",
      "[100]\ttraining's multi_logloss: 0.0234518\n",
      "[110]\ttraining's multi_logloss: 0.0188533\n",
      "[120]\ttraining's multi_logloss: 0.0156326\n",
      "[130]\ttraining's multi_logloss: 0.0132963\n",
      "[140]\ttraining's multi_logloss: 0.0116184\n",
      "[150]\ttraining's multi_logloss: 0.0103147\n",
      "[160]\ttraining's multi_logloss: 0.00928304\n",
      "[170]\ttraining's multi_logloss: 0.00843496\n",
      "[180]\ttraining's multi_logloss: 0.00771759\n",
      "***********第19次抽样***********\n",
      "负样本抽样数量：\n",
      "(18814, 571)\n",
      "[20]\tcv_agg's multi_logloss: 0.534667 + 0.00216067\n",
      "[40]\tcv_agg's multi_logloss: 0.214985 + 0.00294932\n",
      "[60]\tcv_agg's multi_logloss: 0.10562 + 0.00346916\n",
      "[80]\tcv_agg's multi_logloss: 0.0667711 + 0.00376604\n",
      "[100]\tcv_agg's multi_logloss: 0.0529549 + 0.004021\n",
      "[120]\tcv_agg's multi_logloss: 0.0482687 + 0.00423449\n",
      "[140]\tcv_agg's multi_logloss: 0.0468831 + 0.00438052\n",
      "[160]\tcv_agg's multi_logloss: 0.0466779 + 0.0045355\n",
      "[180]\tcv_agg's multi_logloss: 0.0468005 + 0.00471593\n",
      "最优结果0.046651833665874706\n",
      "总loss[0.04852498567484852, 0.029106363761690495, 0.04664073876496425, 0.0274934099341403, 0.026576197708340167, 0.026339973788190602, 0.03560644719328749, 0.04724403952185012, 0.026356844328479134, 0.0466088713028769, 0.03676851532131863, 0.04910697491473538, 0.027991616574996703, 0.037472764778355636, 0.026792865398878374, 0.029519982879886435, 0.029880590619671365, 0.037358582176927724, 0.030529320831376716, 0.046651833665874706]\n",
      "[10]\ttraining's multi_logloss: 0.908152\n",
      "[20]\ttraining's multi_logloss: 0.529228\n",
      "[30]\ttraining's multi_logloss: 0.324066\n",
      "[40]\ttraining's multi_logloss: 0.205341\n",
      "[50]\ttraining's multi_logloss: 0.13457\n",
      "[60]\ttraining's multi_logloss: 0.0915909\n",
      "[70]\ttraining's multi_logloss: 0.0649243\n",
      "[80]\ttraining's multi_logloss: 0.0481974\n",
      "[90]\ttraining's multi_logloss: 0.0373068\n",
      "[100]\ttraining's multi_logloss: 0.0299159\n",
      "[110]\ttraining's multi_logloss: 0.0245873\n",
      "[120]\ttraining's multi_logloss: 0.0208087\n",
      "[130]\ttraining's multi_logloss: 0.0180527\n",
      "[140]\ttraining's multi_logloss: 0.0159382\n",
      "[150]\ttraining's multi_logloss: 0.0142342\n",
      "[160]\ttraining's multi_logloss: 0.0128204\n",
      "[170]\ttraining's multi_logloss: 0.011659\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "pred = pd.DataFrame()\n",
    "for i in range(20):\n",
    "    print('***********第%s次抽样***********'%i)\n",
    "    x0_part = x0.copy()\n",
    "    x0_part.index = range(x0_part.shape[0])\n",
    "    x1.index = range(x1.shape[0])\n",
    "    permutation = np.random.permutation(x0_part.shape[0])\n",
    "    \n",
    "    random.seed(i+10086)\n",
    "    ratio = int(x0.shape[0]*random.uniform(1.5, 4)/10)\n",
    "\n",
    "    select = x0_part.iloc[permutation[:ratio], :]\n",
    "    \n",
    "    print('负样本抽样数量：')\n",
    "    print(select.shape)\n",
    "    \n",
    "    data_new = pd.concat([x1, select], axis = 0)\n",
    "    \n",
    "    permutation = np.random.permutation(data_new.shape[0])\n",
    "\n",
    "    data_new = data_new.iloc[permutation, :]\n",
    "    \n",
    "    label_new = data_new['label'].values\n",
    "    \n",
    "    data_new = data_new.drop(['label', 'id'], axis = 1)\n",
    "    \n",
    "    dtrain = lgb.Dataset(data_new, label=label_new)\n",
    "\n",
    "    hist = lgb.cv(params, dtrain, num_boost_round = 1000, verbose_eval=20, early_stopping_rounds=30, stratified=False)\n",
    "    \n",
    "    print('最优结果%s'%hist['multi_logloss-mean'][-1])\n",
    "    loss_list.append(hist['multi_logloss-mean'][-1])\n",
    "    print('总loss%s'%loss_list)\n",
    "    model = lgb.train(params=params, train_set=dtrain, num_boost_round=int(1.1*len(hist['multi_logloss-mean'])), verbose_eval=10,\\\n",
    "                      valid_sets=dtrain)\n",
    "    \n",
    "    pred_temp = pd.DataFrame(model.predict(test_FE))\n",
    "    \n",
    "    pred = pd.concat([pred, pred_temp], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53093, 120)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame(sub_sample.file_id)\n",
    "\n",
    "p['prob0'] = pred[0].mean(axis = 1)\n",
    "p['prob1'] = pred[1].mean(axis = 1)\n",
    "p['prob2'] = pred[2].mean(axis = 1)\n",
    "p['prob3'] = pred[3].mean(axis = 1)\n",
    "p['prob4'] = pred[4].mean(axis = 1)\n",
    "p['prob5'] = pred[5].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_id    26546.000000\n",
       "prob0          0.899965\n",
       "prob1          0.007818\n",
       "prob2          0.014223\n",
       "prob3          0.012386\n",
       "prob4          0.001676\n",
       "prob5          0.063932\n",
       "dtype: float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.to_csv('/home/libo/Security/sub/8.26_bagging_v3.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = pd.read_csv('/home/libo/Security/sub/8.25_bagging_v1_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = p.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred['prob0'] = 0.7*x1['prob0'] + 0.3*p['prob0']\n",
    "pred['prob1'] = 0.7*x1['prob1'] + 0.3*p['prob1']\n",
    "pred['prob2'] = 0.7*x1['prob2'] + 0.3*p['prob2']\n",
    "pred['prob3'] = 0.7*x1['prob3'] + 0.3*p['prob3']\n",
    "pred['prob4'] = 0.7*x1['prob4'] + 0.3*p['prob4']\n",
    "pred['prob5'] = 0.7*x1['prob5'] + 0.3*p['prob5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred.to_csv('/home/libo/Security/sub/8.26_bagging_v1_v2_v3.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>prob0</th>\n",
       "      <th>prob1</th>\n",
       "      <th>prob2</th>\n",
       "      <th>prob3</th>\n",
       "      <th>prob4</th>\n",
       "      <th>prob5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.998841</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.998663</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999697</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.999208</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.999716</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.999161</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.999500</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.997921</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.992870</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.999689</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.999716</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.999720</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.999645</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.998885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.999266</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.996816</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.001275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.999670</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.999689</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.999164</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.999685</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.999687</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.030452</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.965190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.999650</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.999118</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.999490</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.998041</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.999716</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.998950</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.999693</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53063</th>\n",
       "      <td>53063</td>\n",
       "      <td>0.999568</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53064</th>\n",
       "      <td>53064</td>\n",
       "      <td>0.999682</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53065</th>\n",
       "      <td>53065</td>\n",
       "      <td>0.999036</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53066</th>\n",
       "      <td>53066</td>\n",
       "      <td>0.998986</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53067</th>\n",
       "      <td>53067</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.994428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53068</th>\n",
       "      <td>53068</td>\n",
       "      <td>0.999703</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53069</th>\n",
       "      <td>53069</td>\n",
       "      <td>0.999381</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53070</th>\n",
       "      <td>53070</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53071</th>\n",
       "      <td>53071</td>\n",
       "      <td>0.999606</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53072</th>\n",
       "      <td>53072</td>\n",
       "      <td>0.999696</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53073</th>\n",
       "      <td>53073</td>\n",
       "      <td>0.999694</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53074</th>\n",
       "      <td>53074</td>\n",
       "      <td>0.999685</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53075</th>\n",
       "      <td>53075</td>\n",
       "      <td>0.999540</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53076</th>\n",
       "      <td>53076</td>\n",
       "      <td>0.999228</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53077</th>\n",
       "      <td>53077</td>\n",
       "      <td>0.999695</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53078</th>\n",
       "      <td>53078</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53079</th>\n",
       "      <td>53079</td>\n",
       "      <td>0.999707</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53080</th>\n",
       "      <td>53080</td>\n",
       "      <td>0.999475</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53081</th>\n",
       "      <td>53081</td>\n",
       "      <td>0.994390</td>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.003373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53082</th>\n",
       "      <td>53082</td>\n",
       "      <td>0.999027</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53083</th>\n",
       "      <td>53083</td>\n",
       "      <td>0.999629</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53084</th>\n",
       "      <td>53084</td>\n",
       "      <td>0.999715</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53085</th>\n",
       "      <td>53085</td>\n",
       "      <td>0.999659</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53086</th>\n",
       "      <td>53086</td>\n",
       "      <td>0.999618</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53087</th>\n",
       "      <td>53087</td>\n",
       "      <td>0.997611</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53088</th>\n",
       "      <td>53088</td>\n",
       "      <td>0.990308</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.003213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53089</th>\n",
       "      <td>53089</td>\n",
       "      <td>0.998926</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53090</th>\n",
       "      <td>53090</td>\n",
       "      <td>0.999702</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53091</th>\n",
       "      <td>53091</td>\n",
       "      <td>0.998849</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53092</th>\n",
       "      <td>53092</td>\n",
       "      <td>0.999717</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53093 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_id     prob0     prob1     prob2     prob3     prob4     prob5\n",
       "0            0  0.998841  0.000110  0.000219  0.000105  0.000066  0.000659\n",
       "1            1  0.998663  0.000083  0.000760  0.000077  0.000066  0.000351\n",
       "2            2  0.999697  0.000039  0.000089  0.000040  0.000027  0.000108\n",
       "3            3  0.999208  0.000073  0.000206  0.000082  0.000044  0.000388\n",
       "4            4  0.999716  0.000036  0.000078  0.000038  0.000029  0.000104\n",
       "5            5  0.999161  0.000053  0.000477  0.000054  0.000041  0.000214\n",
       "6            6  0.999500  0.000074  0.000174  0.000054  0.000037  0.000161\n",
       "7            7  0.999707  0.000043  0.000082  0.000037  0.000029  0.000102\n",
       "8            8  0.997921  0.000128  0.000861  0.000151  0.000121  0.000817\n",
       "9            9  0.992870  0.000318  0.004637  0.001034  0.000193  0.000949\n",
       "10          10  0.999689  0.000038  0.000089  0.000039  0.000028  0.000116\n",
       "11          11  0.999716  0.000038  0.000078  0.000037  0.000029  0.000102\n",
       "12          12  0.999720  0.000036  0.000078  0.000037  0.000029  0.000101\n",
       "13          13  0.999645  0.000048  0.000108  0.000044  0.000034  0.000121\n",
       "14          14  0.000321  0.000213  0.000298  0.000205  0.000078  0.998885\n",
       "15          15  0.999266  0.000092  0.000179  0.000085  0.000056  0.000322\n",
       "16          16  0.996816  0.000315  0.000597  0.000839  0.000157  0.001275\n",
       "17          17  0.999670  0.000042  0.000109  0.000043  0.000031  0.000106\n",
       "18          18  0.999689  0.000040  0.000091  0.000042  0.000031  0.000108\n",
       "19          19  0.999164  0.000069  0.000388  0.000074  0.000054  0.000252\n",
       "20          20  0.999685  0.000040  0.000089  0.000040  0.000033  0.000114\n",
       "21          21  0.999687  0.000042  0.000090  0.000041  0.000029  0.000111\n",
       "22          22  0.030452  0.000504  0.002666  0.000797  0.000391  0.965190\n",
       "23          23  0.999650  0.000050  0.000112  0.000044  0.000030  0.000114\n",
       "24          24  0.999118  0.000073  0.000229  0.000068  0.000043  0.000469\n",
       "25          25  0.999490  0.000141  0.000119  0.000053  0.000032  0.000165\n",
       "26          26  0.998041  0.000101  0.000660  0.000572  0.000079  0.000546\n",
       "27          27  0.999716  0.000036  0.000080  0.000038  0.000029  0.000103\n",
       "28          28  0.998950  0.000069  0.000670  0.000065  0.000043  0.000204\n",
       "29          29  0.999693  0.000041  0.000087  0.000042  0.000028  0.000110\n",
       "...        ...       ...       ...       ...       ...       ...       ...\n",
       "53063    53063  0.999568  0.000058  0.000106  0.000054  0.000052  0.000162\n",
       "53064    53064  0.999682  0.000039  0.000090  0.000051  0.000027  0.000111\n",
       "53065    53065  0.999036  0.000089  0.000199  0.000088  0.000060  0.000528\n",
       "53066    53066  0.998986  0.000092  0.000362  0.000099  0.000067  0.000392\n",
       "53067    53067  0.000402  0.000240  0.001157  0.003532  0.000242  0.994428\n",
       "53068    53068  0.999703  0.000038  0.000086  0.000040  0.000030  0.000103\n",
       "53069    53069  0.999381  0.000064  0.000270  0.000067  0.000042  0.000177\n",
       "53070    53070  0.999700  0.000037  0.000094  0.000038  0.000029  0.000102\n",
       "53071    53071  0.999606  0.000136  0.000084  0.000041  0.000029  0.000105\n",
       "53072    53072  0.999696  0.000041  0.000084  0.000040  0.000031  0.000109\n",
       "53073    53073  0.999694  0.000038  0.000077  0.000041  0.000028  0.000122\n",
       "53074    53074  0.999685  0.000041  0.000088  0.000040  0.000031  0.000115\n",
       "53075    53075  0.999540  0.000057  0.000152  0.000055  0.000040  0.000156\n",
       "53076    53076  0.999228  0.000064  0.000182  0.000081  0.000046  0.000399\n",
       "53077    53077  0.999695  0.000041  0.000084  0.000040  0.000031  0.000109\n",
       "53078    53078  0.999656  0.000046  0.000107  0.000044  0.000032  0.000115\n",
       "53079    53079  0.999707  0.000040  0.000081  0.000038  0.000030  0.000105\n",
       "53080    53080  0.999475  0.000049  0.000230  0.000051  0.000039  0.000157\n",
       "53081    53081  0.994390  0.000553  0.000994  0.000419  0.000271  0.003373\n",
       "53082    53082  0.999027  0.000142  0.000266  0.000121  0.000082  0.000361\n",
       "53083    53083  0.999629  0.000057  0.000095  0.000046  0.000040  0.000132\n",
       "53084    53084  0.999715  0.000036  0.000083  0.000037  0.000029  0.000101\n",
       "53085    53085  0.999659  0.000044  0.000105  0.000043  0.000028  0.000122\n",
       "53086    53086  0.999618  0.000046  0.000107  0.000047  0.000036  0.000147\n",
       "53087    53087  0.997611  0.000115  0.001128  0.000124  0.000077  0.000945\n",
       "53088    53088  0.990308  0.001672  0.003861  0.000704  0.000241  0.003213\n",
       "53089    53089  0.998926  0.000124  0.000421  0.000090  0.000084  0.000355\n",
       "53090    53090  0.999702  0.000037  0.000079  0.000037  0.000029  0.000116\n",
       "53091    53091  0.998849  0.000126  0.000438  0.000121  0.000098  0.000368\n",
       "53092    53092  0.999717  0.000039  0.000078  0.000038  0.000029  0.000100\n",
       "\n",
       "[53093 rows x 7 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### xgb bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((111545, 571), (5079, 571))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = train_FE[train_FE['label'] == 0]\n",
    "\n",
    "x1 = train_FE[train_FE['label'] != 0]\n",
    "\n",
    "x0.shape, x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'learning_rate' :  0.05, 'n_estimators':3000, 'max_depth': 3, 'num_class' : 6,  \n",
    "              'subsample':  0.85, 'colsample_bytree':  0.85,'seed':100,\n",
    "             'eta': 0.05,'alpha': 0, 'silent': 1, 'objective': 'multi:softprob', 'booster': 'gbtree',\n",
    "            'lambda': 0, 'eval_metric': \"mlogloss\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********第0次抽样***********\n",
      "负样本抽样数量：\n",
      "(37546, 571)\n",
      "[0]\ttrain-mlogloss:1.64588+5.96955e-05\ttest-mlogloss:1.64621+7.02772e-05\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "pred = pd.DataFrame()\n",
    "for i in range(20):\n",
    "    print('***********第%s次抽样***********'%i)\n",
    "    x0_part = x0.copy()\n",
    "    x0_part.index = range(x0_part.shape[0])\n",
    "    x1.index = range(x1.shape[0])\n",
    "    permutation = np.random.permutation(x0_part.shape[0])\n",
    "    \n",
    "    random.seed(10*i+2018)\n",
    "    \n",
    "    ratio = int(x0.shape[0]*random.uniform(1.5,5)/10)\n",
    "\n",
    "    select = x0_part.iloc[permutation[:ratio], :]\n",
    "    \n",
    "    print('负样本抽样数量：')\n",
    "    print(select.shape)\n",
    "    \n",
    "    data_new = pd.concat([x1, select], axis = 0)\n",
    "    \n",
    "    permutation = np.random.permutation(data_new.shape[0])\n",
    "\n",
    "    data_new = data_new.iloc[permutation, :]\n",
    "    \n",
    "    label_new = data_new['label'].values\n",
    "    \n",
    "    data_new = data_new.drop(['label', 'id'], axis = 1)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(data_new, label=label_new)\n",
    "    \n",
    "    devaluation = xgb.DMatrix(test_FE)\n",
    "    \n",
    "    hist = xgb.cv(dtrain=dtrain, early_stopping_rounds=30, stratified=False, num_boost_round=1000, params=params, \n",
    "       verbose_eval=20)    \n",
    "    print('最优结果%s'%hist['test-mlogloss-mean'].values[-1])\n",
    "    loss_list.append(hist['test-mlogloss-mean'].values[-1])\n",
    "    print('总loss%s'%loss_list)\n",
    "    \n",
    "    evallist = [(dtrain, 'train_data')]\n",
    "    \n",
    "    model = xgb.train(params=params, dtrain=dtrain, verbose_eval=20, evals=evallist, \\\n",
    "                      num_boost_round=int(1.1*(len(hist['test-mlogloss-mean']))))\n",
    "    \n",
    "    pred_temp = pd.DataFrame(model.predict(devaluation))\n",
    "    \n",
    "    pred = pd.concat([pred, pred_temp], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53093, 120)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame(sub_sample.file_id)\n",
    "\n",
    "p['prob0'] = pred[0].mean(axis = 1)\n",
    "p['prob1'] = pred[1].mean(axis = 1)\n",
    "p['prob2'] = pred[2].mean(axis = 1)\n",
    "p['prob3'] = pred[3].mean(axis = 1)\n",
    "p['prob4'] = pred[4].mean(axis = 1)\n",
    "p['prob5'] = pred[5].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_id    26545.550781\n",
       "prob0          0.900738\n",
       "prob1          0.007801\n",
       "prob2          0.014163\n",
       "prob3          0.011954\n",
       "prob4          0.001739\n",
       "prob5          0.063801\n",
       "dtype: float32"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.to_csv('/home/libo/Security/sub/8.27_xgb_bagging_v1.csv', index = None ,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
